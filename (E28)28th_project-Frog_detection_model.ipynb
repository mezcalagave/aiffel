{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frog detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32, 3), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 및 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 입력 파이프라인 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 train시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/cifar10/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer generator is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer discriminator is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Steps : 100, \t Total Gen Loss : 14.281881332397461, \t Total Dis Loss : 0.6963160634040833\n",
      "Steps : 200, \t Total Gen Loss : 14.987351417541504, \t Total Dis Loss : 0.6595878005027771\n",
      "Steps : 300, \t Total Gen Loss : 16.90044593811035, \t Total Dis Loss : 0.5454822778701782\n",
      "Steps : 400, \t Total Gen Loss : 15.452917098999023, \t Total Dis Loss : 0.4032607078552246\n",
      "Steps : 500, \t Total Gen Loss : 16.263097763061523, \t Total Dis Loss : 0.40726739168167114\n",
      "Steps : 600, \t Total Gen Loss : 19.12676429748535, \t Total Dis Loss : 0.3887118399143219\n",
      "Steps : 700, \t Total Gen Loss : 17.37770652770996, \t Total Dis Loss : 0.06872909516096115\n",
      "Steps : 800, \t Total Gen Loss : 16.667030334472656, \t Total Dis Loss : 0.09999111294746399\n",
      "Steps : 900, \t Total Gen Loss : 17.614877700805664, \t Total Dis Loss : 0.07247766107320786\n",
      "Steps : 1000, \t Total Gen Loss : 17.421205520629883, \t Total Dis Loss : 0.25633352994918823\n",
      "Steps : 1100, \t Total Gen Loss : 17.341835021972656, \t Total Dis Loss : 0.022638466209173203\n",
      "Steps : 1200, \t Total Gen Loss : 24.932247161865234, \t Total Dis Loss : 0.023395923897624016\n",
      "Steps : 1300, \t Total Gen Loss : 18.218223571777344, \t Total Dis Loss : 0.02849053591489792\n",
      "Steps : 1400, \t Total Gen Loss : 21.441082000732422, \t Total Dis Loss : 0.05653402954339981\n",
      "Steps : 1500, \t Total Gen Loss : 19.7871150970459, \t Total Dis Loss : 0.03153784200549126\n",
      "Steps : 1600, \t Total Gen Loss : 19.618690490722656, \t Total Dis Loss : 0.13560986518859863\n",
      "Steps : 1700, \t Total Gen Loss : 22.594791412353516, \t Total Dis Loss : 0.00826555397361517\n",
      "Steps : 1800, \t Total Gen Loss : 21.31999969482422, \t Total Dis Loss : 0.01584535464644432\n",
      "Steps : 1900, \t Total Gen Loss : 22.004085540771484, \t Total Dis Loss : 0.03573993593454361\n",
      "Steps : 2000, \t Total Gen Loss : 21.59442138671875, \t Total Dis Loss : 0.028593814000487328\n",
      "Steps : 2100, \t Total Gen Loss : 19.203895568847656, \t Total Dis Loss : 0.008236140944063663\n",
      "Steps : 2200, \t Total Gen Loss : 23.35087776184082, \t Total Dis Loss : 0.10797413438558578\n",
      "Steps : 2300, \t Total Gen Loss : 18.210752487182617, \t Total Dis Loss : 0.07682833075523376\n",
      "Steps : 2400, \t Total Gen Loss : 22.04530143737793, \t Total Dis Loss : 0.04215576499700546\n",
      "Steps : 2500, \t Total Gen Loss : 21.239391326904297, \t Total Dis Loss : 0.0635596290230751\n",
      "Steps : 2600, \t Total Gen Loss : 19.412405014038086, \t Total Dis Loss : 0.007065269164741039\n",
      "Steps : 2700, \t Total Gen Loss : 22.65709686279297, \t Total Dis Loss : 0.012924726121127605\n",
      "Steps : 2800, \t Total Gen Loss : 24.103471755981445, \t Total Dis Loss : 0.006403123494237661\n",
      "Steps : 2900, \t Total Gen Loss : 21.9455623626709, \t Total Dis Loss : 0.009371452033519745\n",
      "Steps : 3000, \t Total Gen Loss : 19.566972732543945, \t Total Dis Loss : 0.018503664061427116\n",
      "Steps : 3100, \t Total Gen Loss : 21.631704330444336, \t Total Dis Loss : 0.013207575306296349\n",
      "Steps : 3200, \t Total Gen Loss : 21.3361873626709, \t Total Dis Loss : 0.0054273055866360664\n",
      "Steps : 3300, \t Total Gen Loss : 23.961139678955078, \t Total Dis Loss : 0.0050742304883897305\n",
      "Steps : 3400, \t Total Gen Loss : 20.496204376220703, \t Total Dis Loss : 0.023834124207496643\n",
      "Steps : 3500, \t Total Gen Loss : 19.962539672851562, \t Total Dis Loss : 0.017937563359737396\n",
      "Steps : 3600, \t Total Gen Loss : 19.175857543945312, \t Total Dis Loss : 0.13669493794441223\n",
      "Steps : 3700, \t Total Gen Loss : 20.488548278808594, \t Total Dis Loss : 0.03834455832839012\n",
      "Steps : 3800, \t Total Gen Loss : 18.89568328857422, \t Total Dis Loss : 0.042593251913785934\n",
      "Steps : 3900, \t Total Gen Loss : 20.239805221557617, \t Total Dis Loss : 0.0043611484579741955\n",
      "Steps : 4000, \t Total Gen Loss : 23.796764373779297, \t Total Dis Loss : 0.013676784001290798\n",
      "Steps : 4100, \t Total Gen Loss : 19.580684661865234, \t Total Dis Loss : 0.37446752190589905\n",
      "Steps : 4200, \t Total Gen Loss : 21.077342987060547, \t Total Dis Loss : 0.009078672155737877\n",
      "Steps : 4300, \t Total Gen Loss : 21.904685974121094, \t Total Dis Loss : 0.029682761058211327\n",
      "Steps : 4400, \t Total Gen Loss : 20.626426696777344, \t Total Dis Loss : 0.009909439831972122\n",
      "Steps : 4500, \t Total Gen Loss : 24.56585121154785, \t Total Dis Loss : 0.006880318745970726\n",
      "Steps : 4600, \t Total Gen Loss : 20.08110237121582, \t Total Dis Loss : 0.006281716749072075\n",
      "Steps : 4700, \t Total Gen Loss : 21.251949310302734, \t Total Dis Loss : 0.002872047247365117\n",
      "Steps : 4800, \t Total Gen Loss : 22.35716438293457, \t Total Dis Loss : 0.009568242356181145\n",
      "Steps : 4900, \t Total Gen Loss : 21.940719604492188, \t Total Dis Loss : 0.005016781389713287\n",
      "Steps : 5000, \t Total Gen Loss : 22.17624282836914, \t Total Dis Loss : 0.007070271763950586\n",
      "Steps : 5100, \t Total Gen Loss : 20.454097747802734, \t Total Dis Loss : 0.003183213993906975\n",
      "Steps : 5200, \t Total Gen Loss : 21.336292266845703, \t Total Dis Loss : 0.09737168252468109\n",
      "Steps : 5300, \t Total Gen Loss : 22.066776275634766, \t Total Dis Loss : 0.005088264122605324\n",
      "Steps : 5400, \t Total Gen Loss : 21.554218292236328, \t Total Dis Loss : 0.0013707525795325637\n",
      "Steps : 5500, \t Total Gen Loss : 22.02010726928711, \t Total Dis Loss : 0.010579327121376991\n",
      "Steps : 5600, \t Total Gen Loss : 20.54414176940918, \t Total Dis Loss : 0.0027784621343016624\n",
      "Time for epoch 1 is 93.53132796287537 sec\n",
      "Steps : 5700, \t Total Gen Loss : 17.108287811279297, \t Total Dis Loss : 0.0425676554441452\n",
      "Steps : 5800, \t Total Gen Loss : 20.740270614624023, \t Total Dis Loss : 0.03563590347766876\n",
      "Steps : 5900, \t Total Gen Loss : 21.282621383666992, \t Total Dis Loss : 0.003665666561573744\n",
      "Steps : 6000, \t Total Gen Loss : 22.028480529785156, \t Total Dis Loss : 0.01793467439711094\n",
      "Steps : 6100, \t Total Gen Loss : 20.597427368164062, \t Total Dis Loss : 0.04418535530567169\n",
      "Steps : 6200, \t Total Gen Loss : 23.311725616455078, \t Total Dis Loss : 0.004667454399168491\n",
      "Steps : 6300, \t Total Gen Loss : 19.38315773010254, \t Total Dis Loss : 0.08665801584720612\n",
      "Steps : 6400, \t Total Gen Loss : 25.04326629638672, \t Total Dis Loss : 0.0037424806505441666\n",
      "Steps : 6500, \t Total Gen Loss : 21.669979095458984, \t Total Dis Loss : 0.01246938668191433\n",
      "Steps : 6600, \t Total Gen Loss : 20.570093154907227, \t Total Dis Loss : 0.0018333394546061754\n",
      "Steps : 6700, \t Total Gen Loss : 22.722267150878906, \t Total Dis Loss : 0.002267624484375119\n",
      "Steps : 6800, \t Total Gen Loss : 21.883625030517578, \t Total Dis Loss : 0.0030414110515266657\n",
      "Steps : 6900, \t Total Gen Loss : 22.8638916015625, \t Total Dis Loss : 0.004307883325964212\n",
      "Steps : 7000, \t Total Gen Loss : 22.625408172607422, \t Total Dis Loss : 0.0012944865738973022\n",
      "Steps : 7100, \t Total Gen Loss : 23.182083129882812, \t Total Dis Loss : 0.0010870648548007011\n",
      "Steps : 7200, \t Total Gen Loss : 19.44377326965332, \t Total Dis Loss : 0.005272471811622381\n",
      "Steps : 7300, \t Total Gen Loss : 22.696807861328125, \t Total Dis Loss : 0.0017225992633029819\n",
      "Steps : 7400, \t Total Gen Loss : 22.111709594726562, \t Total Dis Loss : 0.008387359790503979\n",
      "Steps : 7500, \t Total Gen Loss : 23.666473388671875, \t Total Dis Loss : 0.0021760622039437294\n",
      "Steps : 7600, \t Total Gen Loss : 21.129653930664062, \t Total Dis Loss : 0.03050318919122219\n",
      "Steps : 7700, \t Total Gen Loss : 24.416866302490234, \t Total Dis Loss : 0.003439810127019882\n",
      "Steps : 7800, \t Total Gen Loss : 20.504079818725586, \t Total Dis Loss : 0.06414838135242462\n",
      "Steps : 7900, \t Total Gen Loss : 26.682043075561523, \t Total Dis Loss : 0.009198947809636593\n",
      "Steps : 8000, \t Total Gen Loss : 19.872690200805664, \t Total Dis Loss : 0.1842450350522995\n",
      "Steps : 8100, \t Total Gen Loss : 21.49011993408203, \t Total Dis Loss : 0.004791714251041412\n",
      "Steps : 8200, \t Total Gen Loss : 23.033252716064453, \t Total Dis Loss : 0.0037256034556776285\n",
      "Steps : 8300, \t Total Gen Loss : 21.210845947265625, \t Total Dis Loss : 0.0261275302618742\n",
      "Steps : 8400, \t Total Gen Loss : 20.153926849365234, \t Total Dis Loss : 0.0037600561045110226\n",
      "Steps : 8500, \t Total Gen Loss : 23.371200561523438, \t Total Dis Loss : 0.0048491512425243855\n",
      "Steps : 8600, \t Total Gen Loss : 23.63043212890625, \t Total Dis Loss : 0.03249773755669594\n",
      "Steps : 8700, \t Total Gen Loss : 21.03474998474121, \t Total Dis Loss : 0.005872140638530254\n",
      "Steps : 8800, \t Total Gen Loss : 22.715679168701172, \t Total Dis Loss : 0.0030054657254368067\n",
      "Steps : 8900, \t Total Gen Loss : 25.163787841796875, \t Total Dis Loss : 0.005384326446801424\n",
      "Steps : 9000, \t Total Gen Loss : 20.864551544189453, \t Total Dis Loss : 0.03983578830957413\n",
      "Steps : 9100, \t Total Gen Loss : 22.19023895263672, \t Total Dis Loss : 0.017621679231524467\n",
      "Steps : 9200, \t Total Gen Loss : 21.625446319580078, \t Total Dis Loss : 0.004629306495189667\n",
      "Steps : 9300, \t Total Gen Loss : 22.08808135986328, \t Total Dis Loss : 0.01072266511619091\n",
      "Steps : 9400, \t Total Gen Loss : 21.513660430908203, \t Total Dis Loss : 0.004881170112639666\n",
      "Steps : 9500, \t Total Gen Loss : 22.524093627929688, \t Total Dis Loss : 0.046998925507068634\n",
      "Steps : 9600, \t Total Gen Loss : 21.310070037841797, \t Total Dis Loss : 0.007092681247740984\n",
      "Steps : 9700, \t Total Gen Loss : 24.00351333618164, \t Total Dis Loss : 0.004898235201835632\n",
      "Steps : 9800, \t Total Gen Loss : 21.502098083496094, \t Total Dis Loss : 0.003960765432566404\n",
      "Steps : 9900, \t Total Gen Loss : 22.539491653442383, \t Total Dis Loss : 0.004399466793984175\n",
      "Steps : 10000, \t Total Gen Loss : 20.114147186279297, \t Total Dis Loss : 0.003525710664689541\n",
      "Steps : 10100, \t Total Gen Loss : 21.32630157470703, \t Total Dis Loss : 0.04250146448612213\n",
      "Steps : 10200, \t Total Gen Loss : 23.45865249633789, \t Total Dis Loss : 0.002619110979139805\n",
      "Steps : 10300, \t Total Gen Loss : 21.57284164428711, \t Total Dis Loss : 0.0058146873489022255\n",
      "Steps : 10400, \t Total Gen Loss : 21.00511932373047, \t Total Dis Loss : 0.0015825947048142552\n",
      "Steps : 10500, \t Total Gen Loss : 24.223587036132812, \t Total Dis Loss : 0.009249179624021053\n",
      "Steps : 10600, \t Total Gen Loss : 22.087617874145508, \t Total Dis Loss : 0.0015869525959715247\n",
      "Steps : 10700, \t Total Gen Loss : 26.354969024658203, \t Total Dis Loss : 0.008079519495368004\n",
      "Steps : 10800, \t Total Gen Loss : 26.644577026367188, \t Total Dis Loss : 0.0015468457713723183\n",
      "Steps : 10900, \t Total Gen Loss : 24.15805435180664, \t Total Dis Loss : 0.004920493811368942\n",
      "Steps : 11000, \t Total Gen Loss : 24.955535888671875, \t Total Dis Loss : 0.017810093238949776\n",
      "Steps : 11100, \t Total Gen Loss : 27.45880889892578, \t Total Dis Loss : 0.0015420329291373491\n",
      "Steps : 11200, \t Total Gen Loss : 22.362503051757812, \t Total Dis Loss : 0.003892917651683092\n",
      "Time for epoch 2 is 69.62727332115173 sec\n",
      "Steps : 11300, \t Total Gen Loss : 20.948509216308594, \t Total Dis Loss : 0.006606503389775753\n",
      "Steps : 11400, \t Total Gen Loss : 21.84740447998047, \t Total Dis Loss : 0.005290363449603319\n",
      "Steps : 11500, \t Total Gen Loss : 23.838111877441406, \t Total Dis Loss : 0.0038873900193721056\n",
      "Steps : 11600, \t Total Gen Loss : 21.007957458496094, \t Total Dis Loss : 0.0016611984465271235\n",
      "Steps : 11700, \t Total Gen Loss : 25.414472579956055, \t Total Dis Loss : 0.0009051775559782982\n",
      "Steps : 11800, \t Total Gen Loss : 26.18238067626953, \t Total Dis Loss : 0.0013646205188706517\n",
      "Steps : 11900, \t Total Gen Loss : 24.576236724853516, \t Total Dis Loss : 0.0011040556710213423\n",
      "Steps : 12000, \t Total Gen Loss : 22.724472045898438, \t Total Dis Loss : 0.005238595884293318\n",
      "Steps : 12100, \t Total Gen Loss : 23.152318954467773, \t Total Dis Loss : 0.0039840866811573505\n",
      "Steps : 12200, \t Total Gen Loss : 24.624195098876953, \t Total Dis Loss : 0.0006671801675111055\n",
      "Steps : 12300, \t Total Gen Loss : 20.085424423217773, \t Total Dis Loss : 0.005279178731143475\n",
      "Steps : 12400, \t Total Gen Loss : 24.07870864868164, \t Total Dis Loss : 0.0031239583622664213\n",
      "Steps : 12500, \t Total Gen Loss : 22.50888442993164, \t Total Dis Loss : 0.0023173631634563208\n",
      "Steps : 12600, \t Total Gen Loss : 23.45356559753418, \t Total Dis Loss : 0.002457529539242387\n",
      "Steps : 12700, \t Total Gen Loss : 21.794540405273438, \t Total Dis Loss : 0.0035839290358126163\n",
      "Steps : 12800, \t Total Gen Loss : 24.87384796142578, \t Total Dis Loss : 0.05858877673745155\n",
      "Steps : 12900, \t Total Gen Loss : 23.307504653930664, \t Total Dis Loss : 0.0008976053213700652\n",
      "Steps : 13000, \t Total Gen Loss : 21.285402297973633, \t Total Dis Loss : 0.00899176113307476\n",
      "Steps : 13100, \t Total Gen Loss : 23.262771606445312, \t Total Dis Loss : 0.0019097499316558242\n",
      "Steps : 13200, \t Total Gen Loss : 22.89678382873535, \t Total Dis Loss : 0.005899060983210802\n",
      "Steps : 13300, \t Total Gen Loss : 24.145954132080078, \t Total Dis Loss : 0.01216498389840126\n",
      "Steps : 13400, \t Total Gen Loss : 25.20093536376953, \t Total Dis Loss : 0.0019064517691731453\n",
      "Steps : 13500, \t Total Gen Loss : 24.912368774414062, \t Total Dis Loss : 0.0019860665779560804\n",
      "Steps : 13600, \t Total Gen Loss : 22.913358688354492, \t Total Dis Loss : 0.0006427225307561457\n",
      "Steps : 13700, \t Total Gen Loss : 23.54397201538086, \t Total Dis Loss : 0.005034943576902151\n",
      "Steps : 13800, \t Total Gen Loss : 23.35720443725586, \t Total Dis Loss : 0.012910806573927402\n",
      "Steps : 13900, \t Total Gen Loss : 24.10480499267578, \t Total Dis Loss : 0.000987187260761857\n",
      "Steps : 14000, \t Total Gen Loss : 25.8211669921875, \t Total Dis Loss : 0.0005716358427889645\n",
      "Steps : 14100, \t Total Gen Loss : 22.847490310668945, \t Total Dis Loss : 0.0022898721508681774\n",
      "Steps : 14200, \t Total Gen Loss : 25.984468460083008, \t Total Dis Loss : 0.002231535268947482\n",
      "Steps : 14300, \t Total Gen Loss : 21.911828994750977, \t Total Dis Loss : 0.002967900363728404\n",
      "Steps : 14400, \t Total Gen Loss : 27.435802459716797, \t Total Dis Loss : 0.00034125527599826455\n",
      "Steps : 14500, \t Total Gen Loss : 23.97360610961914, \t Total Dis Loss : 0.00040658877696841955\n",
      "Steps : 14600, \t Total Gen Loss : 21.670137405395508, \t Total Dis Loss : 0.0023903483524918556\n",
      "Steps : 14700, \t Total Gen Loss : 21.081546783447266, \t Total Dis Loss : 0.005938681308180094\n",
      "Steps : 14800, \t Total Gen Loss : 26.627344131469727, \t Total Dis Loss : 0.002203155541792512\n",
      "Steps : 14900, \t Total Gen Loss : 24.54833984375, \t Total Dis Loss : 0.10963058471679688\n",
      "Steps : 15000, \t Total Gen Loss : 25.902639389038086, \t Total Dis Loss : 0.04267977923154831\n",
      "Steps : 15100, \t Total Gen Loss : 23.79638671875, \t Total Dis Loss : 0.001027675112709403\n",
      "Steps : 15200, \t Total Gen Loss : 21.36574363708496, \t Total Dis Loss : 0.006485564634203911\n",
      "Steps : 15300, \t Total Gen Loss : 23.34172821044922, \t Total Dis Loss : 0.0019397165160626173\n",
      "Steps : 15400, \t Total Gen Loss : 25.730438232421875, \t Total Dis Loss : 0.002763205673545599\n",
      "Steps : 15500, \t Total Gen Loss : 22.0591983795166, \t Total Dis Loss : 0.002364775165915489\n",
      "Steps : 15600, \t Total Gen Loss : 21.03805160522461, \t Total Dis Loss : 0.0012105791829526424\n",
      "Steps : 15700, \t Total Gen Loss : 23.166942596435547, \t Total Dis Loss : 0.0010975159239023924\n",
      "Steps : 15800, \t Total Gen Loss : 21.4578914642334, \t Total Dis Loss : 0.000593477685470134\n",
      "Steps : 15900, \t Total Gen Loss : 22.25126075744629, \t Total Dis Loss : 0.0014317259192466736\n",
      "Steps : 16000, \t Total Gen Loss : 23.92980194091797, \t Total Dis Loss : 0.0005087254103273153\n",
      "Steps : 16100, \t Total Gen Loss : 25.556766510009766, \t Total Dis Loss : 0.0004604895366355777\n",
      "Steps : 16200, \t Total Gen Loss : 25.211105346679688, \t Total Dis Loss : 0.0005452935583889484\n",
      "Steps : 16300, \t Total Gen Loss : 24.481849670410156, \t Total Dis Loss : 0.0006639173370786011\n",
      "Steps : 16400, \t Total Gen Loss : 23.9459285736084, \t Total Dis Loss : 0.001064234646037221\n",
      "Steps : 16500, \t Total Gen Loss : 24.102130889892578, \t Total Dis Loss : 0.002102005062624812\n",
      "Steps : 16600, \t Total Gen Loss : 26.445932388305664, \t Total Dis Loss : 0.0035015963949263096\n",
      "Steps : 16700, \t Total Gen Loss : 26.247800827026367, \t Total Dis Loss : 0.016210002824664116\n",
      "Steps : 16800, \t Total Gen Loss : 26.818439483642578, \t Total Dis Loss : 0.004330451134592295\n",
      "Time for epoch 3 is 69.6642165184021 sec\n",
      "Steps : 16900, \t Total Gen Loss : 24.034563064575195, \t Total Dis Loss : 0.002341404091566801\n",
      "Steps : 17000, \t Total Gen Loss : 22.330411911010742, \t Total Dis Loss : 0.0011518645333126187\n",
      "Steps : 17100, \t Total Gen Loss : 22.76331329345703, \t Total Dis Loss : 0.002365256194025278\n",
      "Steps : 17200, \t Total Gen Loss : 22.6374568939209, \t Total Dis Loss : 0.002258437452837825\n",
      "Steps : 17300, \t Total Gen Loss : 24.65780258178711, \t Total Dis Loss : 0.0005808494170196354\n",
      "Steps : 17400, \t Total Gen Loss : 23.45877456665039, \t Total Dis Loss : 0.005939282011240721\n",
      "Steps : 17500, \t Total Gen Loss : 23.333295822143555, \t Total Dis Loss : 0.0024304520338773727\n",
      "Steps : 17600, \t Total Gen Loss : 28.64626693725586, \t Total Dis Loss : 0.00015434148372150958\n",
      "Steps : 17700, \t Total Gen Loss : 27.196022033691406, \t Total Dis Loss : 0.00019759258429985493\n",
      "Steps : 17800, \t Total Gen Loss : 24.977893829345703, \t Total Dis Loss : 0.0031577690970152617\n",
      "Steps : 17900, \t Total Gen Loss : 24.102039337158203, \t Total Dis Loss : 0.00024482858134433627\n",
      "Steps : 18000, \t Total Gen Loss : 22.973453521728516, \t Total Dis Loss : 0.005436604842543602\n",
      "Steps : 18100, \t Total Gen Loss : 27.905231475830078, \t Total Dis Loss : 0.006935813929885626\n",
      "Steps : 18200, \t Total Gen Loss : 26.006187438964844, \t Total Dis Loss : 0.0003203791275154799\n",
      "Steps : 18300, \t Total Gen Loss : 23.82192611694336, \t Total Dis Loss : 0.003721594111993909\n",
      "Steps : 18400, \t Total Gen Loss : 24.104633331298828, \t Total Dis Loss : 0.0013579290825873613\n",
      "Steps : 18500, \t Total Gen Loss : 24.928434371948242, \t Total Dis Loss : 0.0037034442648291588\n",
      "Steps : 18600, \t Total Gen Loss : 20.736671447753906, \t Total Dis Loss : 0.0011636281851679087\n",
      "Steps : 18700, \t Total Gen Loss : 27.05818748474121, \t Total Dis Loss : 0.008615473285317421\n",
      "Steps : 18800, \t Total Gen Loss : 24.28767204284668, \t Total Dis Loss : 0.0002897570957429707\n",
      "Steps : 18900, \t Total Gen Loss : 26.671125411987305, \t Total Dis Loss : 0.0013041170313954353\n",
      "Steps : 19000, \t Total Gen Loss : 25.332103729248047, \t Total Dis Loss : 0.0006370837218128145\n",
      "Steps : 19100, \t Total Gen Loss : 26.07613754272461, \t Total Dis Loss : 0.0003780642873607576\n",
      "Steps : 19200, \t Total Gen Loss : 21.94512176513672, \t Total Dis Loss : 0.0010636505903676152\n",
      "Steps : 19300, \t Total Gen Loss : 30.692989349365234, \t Total Dis Loss : 0.0009489306248724461\n",
      "Steps : 19400, \t Total Gen Loss : 27.126426696777344, \t Total Dis Loss : 0.0001873148139566183\n",
      "Steps : 19500, \t Total Gen Loss : 24.25555419921875, \t Total Dis Loss : 0.0001299286086577922\n",
      "Steps : 19600, \t Total Gen Loss : 22.788387298583984, \t Total Dis Loss : 0.0009130059042945504\n",
      "Steps : 19700, \t Total Gen Loss : 24.740718841552734, \t Total Dis Loss : 0.0037531310226768255\n",
      "Steps : 19800, \t Total Gen Loss : 28.145280838012695, \t Total Dis Loss : 0.004294476471841335\n",
      "Steps : 19900, \t Total Gen Loss : 22.32390594482422, \t Total Dis Loss : 0.12533685564994812\n",
      "Steps : 20000, \t Total Gen Loss : 24.824119567871094, \t Total Dis Loss : 0.0005249598762020469\n",
      "Steps : 20100, \t Total Gen Loss : 27.725269317626953, \t Total Dis Loss : 0.0007869224064052105\n",
      "Steps : 20200, \t Total Gen Loss : 29.208118438720703, \t Total Dis Loss : 0.00012426132161635906\n",
      "Steps : 20300, \t Total Gen Loss : 19.942005157470703, \t Total Dis Loss : 0.0064940620213747025\n",
      "Steps : 20400, \t Total Gen Loss : 23.126667022705078, \t Total Dis Loss : 0.0016503430670127273\n",
      "Steps : 20500, \t Total Gen Loss : 20.731382369995117, \t Total Dis Loss : 0.0037561485078185797\n",
      "Steps : 20600, \t Total Gen Loss : 23.059017181396484, \t Total Dis Loss : 0.0033453572541475296\n",
      "Steps : 20700, \t Total Gen Loss : 24.383729934692383, \t Total Dis Loss : 0.001305802259594202\n",
      "Steps : 20800, \t Total Gen Loss : 29.807239532470703, \t Total Dis Loss : 0.00017426324484404176\n",
      "Steps : 20900, \t Total Gen Loss : 24.34362030029297, \t Total Dis Loss : 0.0007969602011144161\n",
      "Steps : 21000, \t Total Gen Loss : 24.407350540161133, \t Total Dis Loss : 0.00037279355456121266\n",
      "Steps : 21100, \t Total Gen Loss : 23.163864135742188, \t Total Dis Loss : 0.0003223636304028332\n",
      "Steps : 21200, \t Total Gen Loss : 22.13656997680664, \t Total Dis Loss : 0.0153107400983572\n",
      "Steps : 21300, \t Total Gen Loss : 25.43429946899414, \t Total Dis Loss : 0.0010836595902219415\n",
      "Steps : 21400, \t Total Gen Loss : 23.353206634521484, \t Total Dis Loss : 0.012090213596820831\n",
      "Steps : 21500, \t Total Gen Loss : 23.56511878967285, \t Total Dis Loss : 0.0007131310412660241\n",
      "Steps : 21600, \t Total Gen Loss : 21.392139434814453, \t Total Dis Loss : 0.0007654254441149533\n",
      "Steps : 21700, \t Total Gen Loss : 25.053627014160156, \t Total Dis Loss : 0.0006433554226532578\n",
      "Steps : 21800, \t Total Gen Loss : 24.489635467529297, \t Total Dis Loss : 0.00044872972648590803\n",
      "Steps : 21900, \t Total Gen Loss : 22.563533782958984, \t Total Dis Loss : 0.0006937032449059188\n",
      "Steps : 22000, \t Total Gen Loss : 27.373319625854492, \t Total Dis Loss : 0.0053971610032022\n",
      "Steps : 22100, \t Total Gen Loss : 26.31668472290039, \t Total Dis Loss : 0.0009115006541833282\n",
      "Steps : 22200, \t Total Gen Loss : 22.865446090698242, \t Total Dis Loss : 0.0002456157235428691\n",
      "Steps : 22300, \t Total Gen Loss : 29.502105712890625, \t Total Dis Loss : 0.0009196570608764887\n",
      "Steps : 22400, \t Total Gen Loss : 21.511070251464844, \t Total Dis Loss : 0.0020266780629754066\n",
      "Steps : 22500, \t Total Gen Loss : 23.45330810546875, \t Total Dis Loss : 0.0008220338495448232\n",
      "Time for epoch 4 is 69.6555814743042 sec\n",
      "Steps : 22600, \t Total Gen Loss : 24.6726131439209, \t Total Dis Loss : 0.00034361917641945183\n",
      "Steps : 22700, \t Total Gen Loss : 26.760238647460938, \t Total Dis Loss : 0.0002882572589442134\n",
      "Steps : 22800, \t Total Gen Loss : 24.873046875, \t Total Dis Loss : 0.0002754175220616162\n",
      "Steps : 22900, \t Total Gen Loss : 25.69948387145996, \t Total Dis Loss : 0.00018510488735046238\n",
      "Steps : 23000, \t Total Gen Loss : 25.121156692504883, \t Total Dis Loss : 0.005399804562330246\n",
      "Steps : 23100, \t Total Gen Loss : 21.68719482421875, \t Total Dis Loss : 0.05965319275856018\n",
      "Steps : 23200, \t Total Gen Loss : 24.545644760131836, \t Total Dis Loss : 0.004159717820584774\n",
      "Steps : 23300, \t Total Gen Loss : 25.127002716064453, \t Total Dis Loss : 0.02750013768672943\n",
      "Steps : 23400, \t Total Gen Loss : 23.33110809326172, \t Total Dis Loss : 0.010808955878019333\n",
      "Steps : 23500, \t Total Gen Loss : 21.863271713256836, \t Total Dis Loss : 0.007757507264614105\n",
      "Steps : 23600, \t Total Gen Loss : 22.494524002075195, \t Total Dis Loss : 0.0030023714061826468\n",
      "Steps : 23700, \t Total Gen Loss : 20.683767318725586, \t Total Dis Loss : 0.005340621341019869\n",
      "Steps : 23800, \t Total Gen Loss : 23.435800552368164, \t Total Dis Loss : 0.0038185566663742065\n",
      "Steps : 23900, \t Total Gen Loss : 20.32501220703125, \t Total Dis Loss : 0.007451222278177738\n",
      "Steps : 24000, \t Total Gen Loss : 22.176727294921875, \t Total Dis Loss : 0.0029545440338552\n",
      "Steps : 24100, \t Total Gen Loss : 23.85620880126953, \t Total Dis Loss : 0.0011298655299469829\n",
      "Steps : 24200, \t Total Gen Loss : 21.00641632080078, \t Total Dis Loss : 0.016776427626609802\n",
      "Steps : 24300, \t Total Gen Loss : 23.99889373779297, \t Total Dis Loss : 0.003381553804501891\n",
      "Steps : 24400, \t Total Gen Loss : 23.950111389160156, \t Total Dis Loss : 0.0023061297833919525\n",
      "Steps : 24500, \t Total Gen Loss : 21.719572067260742, \t Total Dis Loss : 0.006082930602133274\n",
      "Steps : 24600, \t Total Gen Loss : 26.959272384643555, \t Total Dis Loss : 0.017063256353139877\n",
      "Steps : 24700, \t Total Gen Loss : 22.70655059814453, \t Total Dis Loss : 0.0034264270216226578\n",
      "Steps : 24800, \t Total Gen Loss : 23.240455627441406, \t Total Dis Loss : 0.0049681635573506355\n",
      "Steps : 24900, \t Total Gen Loss : 24.08757781982422, \t Total Dis Loss : 0.0016954526072368026\n",
      "Steps : 25000, \t Total Gen Loss : 24.3165283203125, \t Total Dis Loss : 0.005305417813360691\n",
      "Steps : 25100, \t Total Gen Loss : 22.667816162109375, \t Total Dis Loss : 0.0027114353142678738\n",
      "Steps : 25200, \t Total Gen Loss : 22.798858642578125, \t Total Dis Loss : 0.0014798983465880156\n",
      "Steps : 25300, \t Total Gen Loss : 25.173423767089844, \t Total Dis Loss : 0.0018129008822143078\n",
      "Steps : 25400, \t Total Gen Loss : 23.774494171142578, \t Total Dis Loss : 0.0019882535561919212\n",
      "Steps : 25500, \t Total Gen Loss : 23.661075592041016, \t Total Dis Loss : 0.0026891406159847975\n",
      "Steps : 25600, \t Total Gen Loss : 24.119876861572266, \t Total Dis Loss : 0.0017960339318960905\n",
      "Steps : 25700, \t Total Gen Loss : 24.866405487060547, \t Total Dis Loss : 0.002818663138896227\n",
      "Steps : 25800, \t Total Gen Loss : 25.962499618530273, \t Total Dis Loss : 0.0005308581748977304\n",
      "Steps : 25900, \t Total Gen Loss : 23.53498649597168, \t Total Dis Loss : 0.0023544144351035357\n",
      "Steps : 26000, \t Total Gen Loss : 26.623065948486328, \t Total Dis Loss : 0.0018703712848946452\n",
      "Steps : 26100, \t Total Gen Loss : 28.4864444732666, \t Total Dis Loss : 0.008573914878070354\n",
      "Steps : 26200, \t Total Gen Loss : 23.94092559814453, \t Total Dis Loss : 0.008534818887710571\n",
      "Steps : 26300, \t Total Gen Loss : 26.324352264404297, \t Total Dis Loss : 0.003657903987914324\n",
      "Steps : 26400, \t Total Gen Loss : 23.87635040283203, \t Total Dis Loss : 0.0005526807508431375\n",
      "Steps : 26500, \t Total Gen Loss : 30.29719352722168, \t Total Dis Loss : 0.00499184662476182\n",
      "Steps : 26600, \t Total Gen Loss : 24.09665298461914, \t Total Dis Loss : 0.0017436310881748796\n",
      "Steps : 26700, \t Total Gen Loss : 25.40005111694336, \t Total Dis Loss : 0.002057841746136546\n",
      "Steps : 26800, \t Total Gen Loss : 24.238719940185547, \t Total Dis Loss : 0.0036131597589701414\n",
      "Steps : 26900, \t Total Gen Loss : 28.969942092895508, \t Total Dis Loss : 0.0012452269438654184\n",
      "Steps : 27000, \t Total Gen Loss : 24.648460388183594, \t Total Dis Loss : 0.0003790347254835069\n",
      "Steps : 27100, \t Total Gen Loss : 27.733497619628906, \t Total Dis Loss : 0.029412295669317245\n",
      "Steps : 27200, \t Total Gen Loss : 26.613250732421875, \t Total Dis Loss : 0.0004158627998549491\n",
      "Steps : 27300, \t Total Gen Loss : 24.316869735717773, \t Total Dis Loss : 0.0007124152034521103\n",
      "Steps : 27400, \t Total Gen Loss : 25.795915603637695, \t Total Dis Loss : 0.0018965122289955616\n",
      "Steps : 27500, \t Total Gen Loss : 25.989608764648438, \t Total Dis Loss : 0.0010321405716240406\n",
      "Steps : 27600, \t Total Gen Loss : 21.64099884033203, \t Total Dis Loss : 0.0006043646135367453\n",
      "Steps : 27700, \t Total Gen Loss : 27.820585250854492, \t Total Dis Loss : 0.00035384390503168106\n",
      "Steps : 27800, \t Total Gen Loss : 26.490215301513672, \t Total Dis Loss : 0.002075187163427472\n",
      "Steps : 27900, \t Total Gen Loss : 24.53017807006836, \t Total Dis Loss : 0.0006141527555882931\n",
      "Steps : 28000, \t Total Gen Loss : 25.208877563476562, \t Total Dis Loss : 0.005375654902309179\n",
      "Steps : 28100, \t Total Gen Loss : 27.237533569335938, \t Total Dis Loss : 0.0003362457500770688\n",
      "Time for epoch 5 is 70.1379246711731 sec\n",
      "Steps : 28200, \t Total Gen Loss : 25.747459411621094, \t Total Dis Loss : 0.0004166992148384452\n",
      "Steps : 28300, \t Total Gen Loss : 26.34031105041504, \t Total Dis Loss : 0.00043625099351629615\n",
      "Steps : 28400, \t Total Gen Loss : 24.749671936035156, \t Total Dis Loss : 0.0024468805640935898\n",
      "Steps : 28500, \t Total Gen Loss : 25.559059143066406, \t Total Dis Loss : 0.00024121264868881553\n",
      "Steps : 28600, \t Total Gen Loss : 22.76331329345703, \t Total Dis Loss : 0.0033753581810742617\n",
      "Steps : 28700, \t Total Gen Loss : 27.04867935180664, \t Total Dis Loss : 0.0014515842776745558\n",
      "Steps : 28800, \t Total Gen Loss : 24.535781860351562, \t Total Dis Loss : 0.0012056801933795214\n",
      "Steps : 28900, \t Total Gen Loss : 25.104141235351562, \t Total Dis Loss : 0.00029445617110468447\n",
      "Steps : 29000, \t Total Gen Loss : 26.759838104248047, \t Total Dis Loss : 0.0002896605874411762\n",
      "Steps : 29100, \t Total Gen Loss : 25.40536880493164, \t Total Dis Loss : 0.00029450509464368224\n",
      "Steps : 29200, \t Total Gen Loss : 25.50118637084961, \t Total Dis Loss : 0.0014076476218178868\n",
      "Steps : 29300, \t Total Gen Loss : 26.678232192993164, \t Total Dis Loss : 0.000547910516615957\n",
      "Steps : 29400, \t Total Gen Loss : 24.713109970092773, \t Total Dis Loss : 0.012121510691940784\n",
      "Steps : 29500, \t Total Gen Loss : 23.668346405029297, \t Total Dis Loss : 0.0019097044132649899\n",
      "Steps : 29600, \t Total Gen Loss : 25.683082580566406, \t Total Dis Loss : 0.000990351545624435\n",
      "Steps : 29700, \t Total Gen Loss : 25.80487060546875, \t Total Dis Loss : 0.0018992007244378328\n",
      "Steps : 29800, \t Total Gen Loss : 25.547897338867188, \t Total Dis Loss : 0.004464765079319477\n",
      "Steps : 29900, \t Total Gen Loss : 25.617122650146484, \t Total Dis Loss : 0.0003893942921422422\n",
      "Steps : 30000, \t Total Gen Loss : 26.566837310791016, \t Total Dis Loss : 0.00017327824025414884\n",
      "Steps : 30100, \t Total Gen Loss : 23.937414169311523, \t Total Dis Loss : 0.0004181361000519246\n",
      "Steps : 30200, \t Total Gen Loss : 25.388763427734375, \t Total Dis Loss : 0.0019446754595264792\n",
      "Steps : 30300, \t Total Gen Loss : 28.23041534423828, \t Total Dis Loss : 0.0003350577608216554\n",
      "Steps : 30400, \t Total Gen Loss : 23.313091278076172, \t Total Dis Loss : 0.025834234431385994\n",
      "Steps : 30500, \t Total Gen Loss : 26.587650299072266, \t Total Dis Loss : 0.00043943675700575113\n",
      "Steps : 30600, \t Total Gen Loss : 23.01952362060547, \t Total Dis Loss : 0.0028946034144610167\n",
      "Steps : 30700, \t Total Gen Loss : 20.124801635742188, \t Total Dis Loss : 0.002214164938777685\n",
      "Steps : 30800, \t Total Gen Loss : 25.994441986083984, \t Total Dis Loss : 0.0016396164428442717\n",
      "Steps : 30900, \t Total Gen Loss : 23.390701293945312, \t Total Dis Loss : 0.0031563942320644855\n",
      "Steps : 31000, \t Total Gen Loss : 23.04391860961914, \t Total Dis Loss : 0.0032256897538900375\n",
      "Steps : 31100, \t Total Gen Loss : 22.182025909423828, \t Total Dis Loss : 0.0010597760556265712\n",
      "Steps : 31200, \t Total Gen Loss : 23.674596786499023, \t Total Dis Loss : 0.001200685277581215\n",
      "Steps : 31300, \t Total Gen Loss : 23.658870697021484, \t Total Dis Loss : 0.005079573951661587\n",
      "Steps : 31400, \t Total Gen Loss : 24.126251220703125, \t Total Dis Loss : 0.000539096596185118\n",
      "Steps : 31500, \t Total Gen Loss : 23.518959045410156, \t Total Dis Loss : 0.0009385923040099442\n",
      "Steps : 31600, \t Total Gen Loss : 24.56825828552246, \t Total Dis Loss : 0.01075182668864727\n",
      "Steps : 31700, \t Total Gen Loss : 22.048208236694336, \t Total Dis Loss : 0.0005571140209212899\n",
      "Steps : 31800, \t Total Gen Loss : 24.94141387939453, \t Total Dis Loss : 0.0004116821801289916\n",
      "Steps : 31900, \t Total Gen Loss : 25.491792678833008, \t Total Dis Loss : 0.0005367109552025795\n",
      "Steps : 32000, \t Total Gen Loss : 26.297964096069336, \t Total Dis Loss : 0.0004531086015049368\n",
      "Steps : 32100, \t Total Gen Loss : 27.405698776245117, \t Total Dis Loss : 0.0002868156589102\n",
      "Steps : 32200, \t Total Gen Loss : 23.898727416992188, \t Total Dis Loss : 0.0005798973143100739\n",
      "Steps : 32300, \t Total Gen Loss : 22.01628875732422, \t Total Dis Loss : 0.0007301401346921921\n",
      "Steps : 32400, \t Total Gen Loss : 22.86437225341797, \t Total Dis Loss : 0.0020127028692513704\n",
      "Steps : 32500, \t Total Gen Loss : 26.359891891479492, \t Total Dis Loss : 0.001908025937154889\n",
      "Steps : 32600, \t Total Gen Loss : 23.745830535888672, \t Total Dis Loss : 0.00044206588063389063\n",
      "Steps : 32700, \t Total Gen Loss : 23.749845504760742, \t Total Dis Loss : 0.003811543108895421\n",
      "Steps : 32800, \t Total Gen Loss : 29.249393463134766, \t Total Dis Loss : 0.35323116183280945\n",
      "Steps : 32900, \t Total Gen Loss : 26.959447860717773, \t Total Dis Loss : 0.0007631481857970357\n",
      "Steps : 33000, \t Total Gen Loss : 24.540313720703125, \t Total Dis Loss : 0.0018314894987270236\n",
      "Steps : 33100, \t Total Gen Loss : 26.472692489624023, \t Total Dis Loss : 0.0012695032637566328\n",
      "Steps : 33200, \t Total Gen Loss : 28.597719192504883, \t Total Dis Loss : 0.00015319767408072948\n",
      "Steps : 33300, \t Total Gen Loss : 24.330642700195312, \t Total Dis Loss : 0.0008051812183111906\n",
      "Steps : 33400, \t Total Gen Loss : 25.25830078125, \t Total Dis Loss : 0.0009186538518406451\n",
      "Steps : 33500, \t Total Gen Loss : 23.090255737304688, \t Total Dis Loss : 0.002204872202128172\n",
      "Steps : 33600, \t Total Gen Loss : 21.749296188354492, \t Total Dis Loss : 0.0007410187390632927\n",
      "Steps : 33700, \t Total Gen Loss : 26.71480941772461, \t Total Dis Loss : 0.00026189815253019333\n",
      "Time for epoch 6 is 69.71008276939392 sec\n",
      "Steps : 33800, \t Total Gen Loss : 20.78545379638672, \t Total Dis Loss : 0.006743940059095621\n",
      "Steps : 33900, \t Total Gen Loss : 25.110340118408203, \t Total Dis Loss : 0.0012687630951404572\n",
      "Steps : 34000, \t Total Gen Loss : 25.78000259399414, \t Total Dis Loss : 0.0009647105471231043\n",
      "Steps : 34100, \t Total Gen Loss : 24.72109603881836, \t Total Dis Loss : 0.0003874574904330075\n",
      "Steps : 34200, \t Total Gen Loss : 25.74190330505371, \t Total Dis Loss : 0.00048706235247664154\n",
      "Steps : 34300, \t Total Gen Loss : 28.893733978271484, \t Total Dis Loss : 0.00023286329815164208\n",
      "Steps : 34400, \t Total Gen Loss : 27.910202026367188, \t Total Dis Loss : 0.0003159229236189276\n",
      "Steps : 34500, \t Total Gen Loss : 27.9007625579834, \t Total Dis Loss : 0.00020081596449017525\n",
      "Steps : 34600, \t Total Gen Loss : 26.02336311340332, \t Total Dis Loss : 0.00012080626765964553\n",
      "Steps : 34700, \t Total Gen Loss : 26.901952743530273, \t Total Dis Loss : 0.00028503438807092607\n",
      "Steps : 34800, \t Total Gen Loss : 24.36417007446289, \t Total Dis Loss : 0.00048332157894037664\n",
      "Steps : 34900, \t Total Gen Loss : 25.335453033447266, \t Total Dis Loss : 9.279033838538453e-05\n",
      "Steps : 35000, \t Total Gen Loss : 28.95014762878418, \t Total Dis Loss : 9.042375313583761e-05\n",
      "Steps : 35100, \t Total Gen Loss : 23.037071228027344, \t Total Dis Loss : 0.0003153987054247409\n",
      "Steps : 35200, \t Total Gen Loss : 24.799116134643555, \t Total Dis Loss : 0.00013469242549035698\n",
      "Steps : 35300, \t Total Gen Loss : 23.92301368713379, \t Total Dis Loss : 0.00017492618644610047\n",
      "Steps : 35400, \t Total Gen Loss : 25.93238067626953, \t Total Dis Loss : 0.00013797917927149683\n",
      "Steps : 35500, \t Total Gen Loss : 23.668132781982422, \t Total Dis Loss : 0.0001346742210444063\n",
      "Steps : 35600, \t Total Gen Loss : 24.5920467376709, \t Total Dis Loss : 0.000269055541139096\n",
      "Steps : 35700, \t Total Gen Loss : 23.31371307373047, \t Total Dis Loss : 0.00010617061343509704\n",
      "Steps : 35800, \t Total Gen Loss : 22.326780319213867, \t Total Dis Loss : 0.0001284914615098387\n",
      "Steps : 35900, \t Total Gen Loss : 25.39034652709961, \t Total Dis Loss : 0.00012475010589696467\n",
      "Steps : 36000, \t Total Gen Loss : 29.06281280517578, \t Total Dis Loss : 8.539181726519018e-05\n",
      "Steps : 36100, \t Total Gen Loss : 23.811752319335938, \t Total Dis Loss : 8.432645699940622e-05\n",
      "Steps : 36200, \t Total Gen Loss : 29.179553985595703, \t Total Dis Loss : 7.77764362283051e-05\n",
      "Steps : 36300, \t Total Gen Loss : 28.42339515686035, \t Total Dis Loss : 6.502783799078315e-05\n",
      "Steps : 36400, \t Total Gen Loss : 26.654640197753906, \t Total Dis Loss : 9.870960639091209e-05\n",
      "Steps : 36500, \t Total Gen Loss : 28.847692489624023, \t Total Dis Loss : 0.0017363873776048422\n",
      "Steps : 36600, \t Total Gen Loss : 24.42416000366211, \t Total Dis Loss : 0.0024064048193395138\n",
      "Steps : 36700, \t Total Gen Loss : 24.695518493652344, \t Total Dis Loss : 0.0027503464370965958\n",
      "Steps : 36800, \t Total Gen Loss : 24.151512145996094, \t Total Dis Loss : 8.217550930567086e-05\n",
      "Steps : 36900, \t Total Gen Loss : 26.85320281982422, \t Total Dis Loss : 6.275191117310897e-05\n",
      "Steps : 37000, \t Total Gen Loss : 28.0333251953125, \t Total Dis Loss : 6.684899562969804e-05\n",
      "Steps : 37100, \t Total Gen Loss : 23.67353057861328, \t Total Dis Loss : 0.016754621639847755\n",
      "Steps : 37200, \t Total Gen Loss : 25.92557144165039, \t Total Dis Loss : 0.00796013604849577\n",
      "Steps : 37300, \t Total Gen Loss : 22.060218811035156, \t Total Dis Loss : 0.0032248874194920063\n",
      "Steps : 37400, \t Total Gen Loss : 26.497175216674805, \t Total Dis Loss : 0.000999378738924861\n",
      "Steps : 37500, \t Total Gen Loss : 19.5958251953125, \t Total Dis Loss : 0.0015617834869772196\n",
      "Steps : 37600, \t Total Gen Loss : 21.837238311767578, \t Total Dis Loss : 0.003515389282256365\n",
      "Steps : 37700, \t Total Gen Loss : 22.617427825927734, \t Total Dis Loss : 0.0016080954810604453\n",
      "Steps : 37800, \t Total Gen Loss : 20.872953414916992, \t Total Dis Loss : 0.001951758749783039\n",
      "Steps : 37900, \t Total Gen Loss : 21.492721557617188, \t Total Dis Loss : 0.0038019961211830378\n",
      "Steps : 38000, \t Total Gen Loss : 22.46015167236328, \t Total Dis Loss : 0.05973827838897705\n",
      "Steps : 38100, \t Total Gen Loss : 25.763547897338867, \t Total Dis Loss : 0.004872014280408621\n",
      "Steps : 38200, \t Total Gen Loss : 26.29642677307129, \t Total Dis Loss : 0.00011116118548670784\n",
      "Steps : 38300, \t Total Gen Loss : 26.382022857666016, \t Total Dis Loss : 0.00041425449308007956\n",
      "Steps : 38400, \t Total Gen Loss : 25.851186752319336, \t Total Dis Loss : 0.00033636309672147036\n",
      "Steps : 38500, \t Total Gen Loss : 25.012794494628906, \t Total Dis Loss : 0.006778760347515345\n",
      "Steps : 38600, \t Total Gen Loss : 26.882904052734375, \t Total Dis Loss : 0.005368646699935198\n",
      "Steps : 38700, \t Total Gen Loss : 26.702367782592773, \t Total Dis Loss : 0.0008026193827390671\n",
      "Steps : 38800, \t Total Gen Loss : 26.48904037475586, \t Total Dis Loss : 0.01523748692125082\n",
      "Steps : 38900, \t Total Gen Loss : 24.383501052856445, \t Total Dis Loss : 0.001318500842899084\n",
      "Steps : 39000, \t Total Gen Loss : 21.50454330444336, \t Total Dis Loss : 0.0024247318506240845\n",
      "Steps : 39100, \t Total Gen Loss : 23.205657958984375, \t Total Dis Loss : 0.0006164201186038554\n",
      "Steps : 39200, \t Total Gen Loss : 26.560237884521484, \t Total Dis Loss : 0.009354321286082268\n",
      "Steps : 39300, \t Total Gen Loss : 24.588829040527344, \t Total Dis Loss : 0.0004050400457344949\n",
      "Time for epoch 7 is 69.67096829414368 sec\n",
      "Steps : 39400, \t Total Gen Loss : 25.61542510986328, \t Total Dis Loss : 0.00035487348213791847\n",
      "Steps : 39500, \t Total Gen Loss : 23.459087371826172, \t Total Dis Loss : 0.0019922342617064714\n",
      "Steps : 39600, \t Total Gen Loss : 23.744007110595703, \t Total Dis Loss : 0.0006877167033962905\n",
      "Steps : 39700, \t Total Gen Loss : 23.695798873901367, \t Total Dis Loss : 0.0018640932394191623\n",
      "Steps : 39800, \t Total Gen Loss : 26.451650619506836, \t Total Dis Loss : 0.00027738558128476143\n",
      "Steps : 39900, \t Total Gen Loss : 27.090818405151367, \t Total Dis Loss : 0.023147128522396088\n",
      "Steps : 40000, \t Total Gen Loss : 22.914310455322266, \t Total Dis Loss : 0.09554266184568405\n",
      "Steps : 40100, \t Total Gen Loss : 25.67511749267578, \t Total Dis Loss : 0.0004964555846527219\n",
      "Steps : 40200, \t Total Gen Loss : 20.732908248901367, \t Total Dis Loss : 0.0029803961515426636\n",
      "Steps : 40300, \t Total Gen Loss : 23.13486671447754, \t Total Dis Loss : 0.0005395068437792361\n",
      "Steps : 40400, \t Total Gen Loss : 25.015785217285156, \t Total Dis Loss : 0.000535267754457891\n",
      "Steps : 40500, \t Total Gen Loss : 22.687454223632812, \t Total Dis Loss : 0.0005158456042408943\n",
      "Steps : 40600, \t Total Gen Loss : 24.2520751953125, \t Total Dis Loss : 0.0002498757385183126\n",
      "Steps : 40700, \t Total Gen Loss : 24.59261703491211, \t Total Dis Loss : 0.00047978141810745\n",
      "Steps : 40800, \t Total Gen Loss : 25.46567153930664, \t Total Dis Loss : 0.0005734800361096859\n",
      "Steps : 40900, \t Total Gen Loss : 28.286861419677734, \t Total Dis Loss : 0.0007811845280230045\n",
      "Steps : 41000, \t Total Gen Loss : 23.94788360595703, \t Total Dis Loss : 0.002356634009629488\n",
      "Steps : 41100, \t Total Gen Loss : 27.29908561706543, \t Total Dis Loss : 0.0016337473643943667\n",
      "Steps : 41200, \t Total Gen Loss : 25.165802001953125, \t Total Dis Loss : 0.08680842071771622\n",
      "Steps : 41300, \t Total Gen Loss : 24.54349136352539, \t Total Dis Loss : 0.0003621999640017748\n",
      "Steps : 41400, \t Total Gen Loss : 27.427318572998047, \t Total Dis Loss : 0.001724252593703568\n",
      "Steps : 41500, \t Total Gen Loss : 29.375354766845703, \t Total Dis Loss : 0.00019569651340134442\n",
      "Steps : 41600, \t Total Gen Loss : 27.718725204467773, \t Total Dis Loss : 0.0007200147956609726\n",
      "Steps : 41700, \t Total Gen Loss : 26.37523651123047, \t Total Dis Loss : 0.0008710441179573536\n",
      "Steps : 41800, \t Total Gen Loss : 26.90158462524414, \t Total Dis Loss : 0.00034966308157891035\n",
      "Steps : 41900, \t Total Gen Loss : 27.346614837646484, \t Total Dis Loss : 0.006158321630209684\n",
      "Steps : 42000, \t Total Gen Loss : 23.72095489501953, \t Total Dis Loss : 0.0015244961250573397\n",
      "Steps : 42100, \t Total Gen Loss : 23.531980514526367, \t Total Dis Loss : 0.0026452336460351944\n",
      "Steps : 42200, \t Total Gen Loss : 24.754257202148438, \t Total Dis Loss : 0.0047706495970487595\n",
      "Steps : 42300, \t Total Gen Loss : 25.851287841796875, \t Total Dis Loss : 0.0007509447750635445\n",
      "Steps : 42400, \t Total Gen Loss : 23.843185424804688, \t Total Dis Loss : 0.0008160587167367339\n",
      "Steps : 42500, \t Total Gen Loss : 25.79420280456543, \t Total Dis Loss : 0.00029872305458411574\n",
      "Steps : 42600, \t Total Gen Loss : 22.42953872680664, \t Total Dis Loss : 0.005766601301729679\n",
      "Steps : 42700, \t Total Gen Loss : 24.34520721435547, \t Total Dis Loss : 0.0004461923963390291\n",
      "Steps : 42800, \t Total Gen Loss : 26.045228958129883, \t Total Dis Loss : 0.020936809480190277\n",
      "Steps : 42900, \t Total Gen Loss : 26.779218673706055, \t Total Dis Loss : 0.015039816498756409\n",
      "Steps : 43000, \t Total Gen Loss : 25.360836029052734, \t Total Dis Loss : 0.002111691515892744\n",
      "Steps : 43100, \t Total Gen Loss : 25.22423553466797, \t Total Dis Loss : 0.0018230830319225788\n",
      "Steps : 43200, \t Total Gen Loss : 27.954994201660156, \t Total Dis Loss : 0.011290972121059895\n",
      "Steps : 43300, \t Total Gen Loss : 24.717514038085938, \t Total Dis Loss : 0.002280098618939519\n",
      "Steps : 43400, \t Total Gen Loss : 28.65017318725586, \t Total Dis Loss : 0.005202718079090118\n",
      "Steps : 43500, \t Total Gen Loss : 26.631587982177734, \t Total Dis Loss : 0.00035211152862757444\n",
      "Steps : 43600, \t Total Gen Loss : 25.386526107788086, \t Total Dis Loss : 0.0003398680128157139\n",
      "Steps : 43700, \t Total Gen Loss : 26.857728958129883, \t Total Dis Loss : 0.00046273466432467103\n",
      "Steps : 43800, \t Total Gen Loss : 26.090946197509766, \t Total Dis Loss : 0.00020938419038429856\n",
      "Steps : 43900, \t Total Gen Loss : 25.75052261352539, \t Total Dis Loss : 0.00042028629104606807\n",
      "Steps : 44000, \t Total Gen Loss : 26.603240966796875, \t Total Dis Loss : 0.00021201642812229693\n",
      "Steps : 44100, \t Total Gen Loss : 23.526355743408203, \t Total Dis Loss : 0.00045717397006228566\n",
      "Steps : 44200, \t Total Gen Loss : 24.22010040283203, \t Total Dis Loss : 0.0026327830273658037\n",
      "Steps : 44300, \t Total Gen Loss : 24.37468910217285, \t Total Dis Loss : 0.00039875751826912165\n",
      "Steps : 44400, \t Total Gen Loss : 24.741775512695312, \t Total Dis Loss : 0.00031996413599699736\n",
      "Steps : 44500, \t Total Gen Loss : 25.874256134033203, \t Total Dis Loss : 0.00045831996249035\n",
      "Steps : 44600, \t Total Gen Loss : 28.992963790893555, \t Total Dis Loss : 0.0005000351811759174\n",
      "Steps : 44700, \t Total Gen Loss : 21.53325080871582, \t Total Dis Loss : 0.24978408217430115\n",
      "Steps : 44800, \t Total Gen Loss : 25.939697265625, \t Total Dis Loss : 0.0006083929911255836\n",
      "Steps : 44900, \t Total Gen Loss : 26.833473205566406, \t Total Dis Loss : 0.00042485984158702195\n",
      "Steps : 45000, \t Total Gen Loss : 27.48184585571289, \t Total Dis Loss : 0.0009086478967219591\n",
      "Time for epoch 8 is 69.64429998397827 sec\n",
      "Steps : 45100, \t Total Gen Loss : 26.705604553222656, \t Total Dis Loss : 0.003900977550074458\n",
      "Steps : 45200, \t Total Gen Loss : 27.056182861328125, \t Total Dis Loss : 0.00022530746355187148\n",
      "Steps : 45300, \t Total Gen Loss : 27.09442138671875, \t Total Dis Loss : 8.753144356887788e-05\n",
      "Steps : 45400, \t Total Gen Loss : 29.188457489013672, \t Total Dis Loss : 0.00015349284512922168\n",
      "Steps : 45500, \t Total Gen Loss : 26.49802017211914, \t Total Dis Loss : 0.0007088107522577047\n",
      "Steps : 45600, \t Total Gen Loss : 22.234731674194336, \t Total Dis Loss : 0.0007744742906652391\n",
      "Steps : 45700, \t Total Gen Loss : 23.235939025878906, \t Total Dis Loss : 0.0006150843109935522\n",
      "Steps : 45800, \t Total Gen Loss : 26.56254005432129, \t Total Dis Loss : 0.000235463390708901\n",
      "Steps : 45900, \t Total Gen Loss : 26.696130752563477, \t Total Dis Loss : 0.0004901594365946949\n",
      "Steps : 46000, \t Total Gen Loss : 23.529613494873047, \t Total Dis Loss : 0.00015734411135781556\n",
      "Steps : 46100, \t Total Gen Loss : 26.66647720336914, \t Total Dis Loss : 0.00020622406736947596\n",
      "Steps : 46200, \t Total Gen Loss : 25.04611587524414, \t Total Dis Loss : 0.00039821717655286193\n",
      "Steps : 46300, \t Total Gen Loss : 24.46124267578125, \t Total Dis Loss : 0.00031853836844675243\n",
      "Steps : 46400, \t Total Gen Loss : 24.21358299255371, \t Total Dis Loss : 0.0003577203897293657\n",
      "Steps : 46500, \t Total Gen Loss : 20.529399871826172, \t Total Dis Loss : 0.007418931927531958\n",
      "Steps : 46600, \t Total Gen Loss : 31.209436416625977, \t Total Dis Loss : 0.0033135279081761837\n",
      "Steps : 46700, \t Total Gen Loss : 25.53052520751953, \t Total Dis Loss : 0.0005431432509794831\n",
      "Steps : 46800, \t Total Gen Loss : 22.482402801513672, \t Total Dis Loss : 0.002087926957756281\n",
      "Steps : 46900, \t Total Gen Loss : 25.333404541015625, \t Total Dis Loss : 0.0004079965874552727\n",
      "Steps : 47000, \t Total Gen Loss : 26.470504760742188, \t Total Dis Loss : 0.0002001718239625916\n",
      "Steps : 47100, \t Total Gen Loss : 29.348651885986328, \t Total Dis Loss : 0.000540049746632576\n",
      "Steps : 47200, \t Total Gen Loss : 23.563274383544922, \t Total Dis Loss : 7.981947419466451e-05\n",
      "Steps : 47300, \t Total Gen Loss : 30.530956268310547, \t Total Dis Loss : 0.0008621160523034632\n",
      "Steps : 47400, \t Total Gen Loss : 24.900564193725586, \t Total Dis Loss : 0.001058642053976655\n",
      "Steps : 47500, \t Total Gen Loss : 27.897457122802734, \t Total Dis Loss : 0.00046123439096845686\n",
      "Steps : 47600, \t Total Gen Loss : 24.609474182128906, \t Total Dis Loss : 0.00039957999251782894\n",
      "Steps : 47700, \t Total Gen Loss : 25.93810272216797, \t Total Dis Loss : 0.00034026248613372445\n",
      "Steps : 47800, \t Total Gen Loss : 27.82265853881836, \t Total Dis Loss : 0.001328847254626453\n",
      "Steps : 47900, \t Total Gen Loss : 25.93558120727539, \t Total Dis Loss : 0.0011160018621012568\n",
      "Steps : 48000, \t Total Gen Loss : 27.931468963623047, \t Total Dis Loss : 0.02217855118215084\n",
      "Steps : 48100, \t Total Gen Loss : 26.699731826782227, \t Total Dis Loss : 0.00012166591477580369\n",
      "Steps : 48200, \t Total Gen Loss : 28.12392807006836, \t Total Dis Loss : 0.0006161608616821468\n",
      "Steps : 48300, \t Total Gen Loss : 24.611892700195312, \t Total Dis Loss : 0.0006785023142583668\n",
      "Steps : 48400, \t Total Gen Loss : 25.993236541748047, \t Total Dis Loss : 0.00026633410016074777\n",
      "Steps : 48500, \t Total Gen Loss : 25.377613067626953, \t Total Dis Loss : 8.523635915480554e-05\n",
      "Steps : 48600, \t Total Gen Loss : 24.898834228515625, \t Total Dis Loss : 0.0016315995017066598\n",
      "Steps : 48700, \t Total Gen Loss : 27.23447036743164, \t Total Dis Loss : 0.00020839119679294527\n",
      "Steps : 48800, \t Total Gen Loss : 25.233478546142578, \t Total Dis Loss : 0.0033702345099300146\n",
      "Steps : 48900, \t Total Gen Loss : 30.078289031982422, \t Total Dis Loss : 9.475869592279196e-05\n",
      "Steps : 49000, \t Total Gen Loss : 29.7004451751709, \t Total Dis Loss : 0.000776187633164227\n",
      "Steps : 49100, \t Total Gen Loss : 25.3740291595459, \t Total Dis Loss : 0.0004287764022592455\n",
      "Steps : 49200, \t Total Gen Loss : 24.747255325317383, \t Total Dis Loss : 0.002339784288778901\n",
      "Steps : 49300, \t Total Gen Loss : 23.956722259521484, \t Total Dis Loss : 0.001954195788130164\n",
      "Steps : 49400, \t Total Gen Loss : 27.70412826538086, \t Total Dis Loss : 0.000536779931280762\n",
      "Steps : 49500, \t Total Gen Loss : 24.733707427978516, \t Total Dis Loss : 0.00022544118110090494\n",
      "Steps : 49600, \t Total Gen Loss : 23.875442504882812, \t Total Dis Loss : 0.0006054661353118718\n",
      "Steps : 49700, \t Total Gen Loss : 24.520877838134766, \t Total Dis Loss : 0.00013773719547316432\n",
      "Steps : 49800, \t Total Gen Loss : 27.19660186767578, \t Total Dis Loss : 0.0002463603741489351\n",
      "Steps : 49900, \t Total Gen Loss : 24.551612854003906, \t Total Dis Loss : 0.00025004628696478903\n",
      "Steps : 50000, \t Total Gen Loss : 27.02836036682129, \t Total Dis Loss : 0.00032886175904423\n",
      "Steps : 50100, \t Total Gen Loss : 26.31583023071289, \t Total Dis Loss : 0.0001831066037993878\n",
      "Steps : 50200, \t Total Gen Loss : 26.538501739501953, \t Total Dis Loss : 0.00014750013360753655\n",
      "Steps : 50300, \t Total Gen Loss : 26.591915130615234, \t Total Dis Loss : 0.00012060935841873288\n",
      "Steps : 50400, \t Total Gen Loss : 29.179975509643555, \t Total Dis Loss : 0.0001124562113545835\n",
      "Steps : 50500, \t Total Gen Loss : 27.999174118041992, \t Total Dis Loss : 0.00023321207845583558\n",
      "Steps : 50600, \t Total Gen Loss : 29.452442169189453, \t Total Dis Loss : 0.00030874990625306964\n",
      "Time for epoch 9 is 69.64048600196838 sec\n",
      "Steps : 50700, \t Total Gen Loss : 28.05221939086914, \t Total Dis Loss : 0.0001007389000733383\n",
      "Steps : 50800, \t Total Gen Loss : 26.76829719543457, \t Total Dis Loss : 0.00030885235173627734\n",
      "Steps : 50900, \t Total Gen Loss : 26.59244155883789, \t Total Dis Loss : 0.00019898996106348932\n",
      "Steps : 51000, \t Total Gen Loss : 27.62697410583496, \t Total Dis Loss : 0.0008611925295554101\n",
      "Steps : 51100, \t Total Gen Loss : 23.520248413085938, \t Total Dis Loss : 0.013867991045117378\n",
      "Steps : 51200, \t Total Gen Loss : 29.717226028442383, \t Total Dis Loss : 0.0003896575071848929\n",
      "Steps : 51300, \t Total Gen Loss : 25.774795532226562, \t Total Dis Loss : 0.11032629013061523\n",
      "Steps : 51400, \t Total Gen Loss : 27.030824661254883, \t Total Dis Loss : 0.0013758294517174363\n",
      "Steps : 51500, \t Total Gen Loss : 30.695720672607422, \t Total Dis Loss : 0.0002610352821648121\n",
      "Steps : 51600, \t Total Gen Loss : 28.55647850036621, \t Total Dis Loss : 0.0003064493939746171\n",
      "Steps : 51700, \t Total Gen Loss : 25.567930221557617, \t Total Dis Loss : 0.001412392477504909\n",
      "Steps : 51800, \t Total Gen Loss : 29.521411895751953, \t Total Dis Loss : 0.0006302589317783713\n",
      "Steps : 51900, \t Total Gen Loss : 30.316455841064453, \t Total Dis Loss : 0.0001822948397602886\n",
      "Steps : 52000, \t Total Gen Loss : 26.981895446777344, \t Total Dis Loss : 0.0002284625661559403\n",
      "Steps : 52100, \t Total Gen Loss : 25.405101776123047, \t Total Dis Loss : 0.00031757421675138175\n",
      "Steps : 52200, \t Total Gen Loss : 26.712875366210938, \t Total Dis Loss : 0.00040317268576473\n",
      "Steps : 52300, \t Total Gen Loss : 29.19230079650879, \t Total Dis Loss : 7.332149834837765e-05\n",
      "Steps : 52400, \t Total Gen Loss : 29.695476531982422, \t Total Dis Loss : 0.0003486011119093746\n",
      "Steps : 52500, \t Total Gen Loss : 30.56157875061035, \t Total Dis Loss : 0.0014960572589188814\n",
      "Steps : 52600, \t Total Gen Loss : 29.41289520263672, \t Total Dis Loss : 0.00018345411808695644\n",
      "Steps : 52700, \t Total Gen Loss : 24.228740692138672, \t Total Dis Loss : 0.00029517474467866123\n",
      "Steps : 52800, \t Total Gen Loss : 25.968280792236328, \t Total Dis Loss : 0.00046270573511719704\n",
      "Steps : 52900, \t Total Gen Loss : 23.319355010986328, \t Total Dis Loss : 0.0003772727504838258\n",
      "Steps : 53000, \t Total Gen Loss : 26.303874969482422, \t Total Dis Loss : 0.00015476031694561243\n",
      "Steps : 53100, \t Total Gen Loss : 24.32238006591797, \t Total Dis Loss : 0.0007076355977915227\n",
      "Steps : 53200, \t Total Gen Loss : 25.10626220703125, \t Total Dis Loss : 0.0006568128010258079\n",
      "Steps : 53300, \t Total Gen Loss : 24.677104949951172, \t Total Dis Loss : 0.00031094797304831445\n",
      "Steps : 53400, \t Total Gen Loss : 25.7429141998291, \t Total Dis Loss : 0.0007835979340597987\n",
      "Steps : 53500, \t Total Gen Loss : 27.5542049407959, \t Total Dis Loss : 6.542807386722416e-05\n",
      "Steps : 53600, \t Total Gen Loss : 26.267269134521484, \t Total Dis Loss : 0.00034306070301681757\n",
      "Steps : 53700, \t Total Gen Loss : 26.45806121826172, \t Total Dis Loss : 2.733397195697762e-05\n",
      "Steps : 53800, \t Total Gen Loss : 25.93695831298828, \t Total Dis Loss : 0.00035775217111222446\n",
      "Steps : 53900, \t Total Gen Loss : 29.49444580078125, \t Total Dis Loss : 0.00013090296124573797\n",
      "Steps : 54000, \t Total Gen Loss : 26.770517349243164, \t Total Dis Loss : 0.0001390977413393557\n",
      "Steps : 54100, \t Total Gen Loss : 27.125289916992188, \t Total Dis Loss : 0.00020065729040652514\n",
      "Steps : 54200, \t Total Gen Loss : 26.96951675415039, \t Total Dis Loss : 0.00011654153058771044\n",
      "Steps : 54300, \t Total Gen Loss : 25.460432052612305, \t Total Dis Loss : 0.0002704703074414283\n",
      "Steps : 54400, \t Total Gen Loss : 26.225711822509766, \t Total Dis Loss : 7.174041820690036e-05\n",
      "Steps : 54500, \t Total Gen Loss : 27.05052375793457, \t Total Dis Loss : 5.93004806432873e-05\n",
      "Steps : 54600, \t Total Gen Loss : 24.731040954589844, \t Total Dis Loss : 0.00045556595432572067\n",
      "Steps : 54700, \t Total Gen Loss : 26.84282112121582, \t Total Dis Loss : 0.00013409569510258734\n",
      "Steps : 54800, \t Total Gen Loss : 25.070117950439453, \t Total Dis Loss : 0.0029128198511898518\n",
      "Steps : 54900, \t Total Gen Loss : 22.173355102539062, \t Total Dis Loss : 0.0007309815846383572\n",
      "Steps : 55000, \t Total Gen Loss : 25.91043472290039, \t Total Dis Loss : 0.00033005845034494996\n",
      "Steps : 55100, \t Total Gen Loss : 26.679821014404297, \t Total Dis Loss : 0.00043441136949695647\n",
      "Steps : 55200, \t Total Gen Loss : 26.601781845092773, \t Total Dis Loss : 0.0010073508601635695\n",
      "Steps : 55300, \t Total Gen Loss : 25.752784729003906, \t Total Dis Loss : 0.0041708629578351974\n",
      "Steps : 55400, \t Total Gen Loss : 23.056482315063477, \t Total Dis Loss : 0.0033639210741966963\n",
      "Steps : 55500, \t Total Gen Loss : 28.29612159729004, \t Total Dis Loss : 0.0004882485081907362\n",
      "Steps : 55600, \t Total Gen Loss : 30.223705291748047, \t Total Dis Loss : 0.00026953750057145953\n",
      "Steps : 55700, \t Total Gen Loss : 29.00922393798828, \t Total Dis Loss : 0.000525594805367291\n",
      "Steps : 55800, \t Total Gen Loss : 22.80305290222168, \t Total Dis Loss : 0.00036385050043463707\n",
      "Steps : 55900, \t Total Gen Loss : 23.796140670776367, \t Total Dis Loss : 0.022478725761175156\n",
      "Steps : 56000, \t Total Gen Loss : 26.543010711669922, \t Total Dis Loss : 0.006853536237031221\n",
      "Steps : 56100, \t Total Gen Loss : 27.811504364013672, \t Total Dis Loss : 7.236980309244245e-05\n",
      "Steps : 56200, \t Total Gen Loss : 22.532567977905273, \t Total Dis Loss : 0.003044965211302042\n",
      "Time for epoch 10 is 70.28165793418884 sec\n",
      "Steps : 56300, \t Total Gen Loss : 26.514217376708984, \t Total Dis Loss : 0.00012453895760700107\n",
      "Steps : 56400, \t Total Gen Loss : 25.360973358154297, \t Total Dis Loss : 0.0008884631679393351\n",
      "Steps : 56500, \t Total Gen Loss : 26.983190536499023, \t Total Dis Loss : 7.759595609968528e-05\n",
      "Steps : 56600, \t Total Gen Loss : 26.071491241455078, \t Total Dis Loss : 9.494269033893943e-05\n",
      "Steps : 56700, \t Total Gen Loss : 27.535377502441406, \t Total Dis Loss : 5.8760197134688497e-05\n",
      "Steps : 56800, \t Total Gen Loss : 23.99386215209961, \t Total Dis Loss : 0.00012471992522478104\n",
      "Steps : 56900, \t Total Gen Loss : 26.3869571685791, \t Total Dis Loss : 0.00012697343481704593\n",
      "Steps : 57000, \t Total Gen Loss : 27.06667709350586, \t Total Dis Loss : 0.0001465737441321835\n",
      "Steps : 57100, \t Total Gen Loss : 25.838603973388672, \t Total Dis Loss : 0.00010985445987898856\n",
      "Steps : 57200, \t Total Gen Loss : 26.723800659179688, \t Total Dis Loss : 0.0002309283008798957\n",
      "Steps : 57300, \t Total Gen Loss : 26.42951774597168, \t Total Dis Loss : 0.0001173815835500136\n",
      "Steps : 57400, \t Total Gen Loss : 25.430095672607422, \t Total Dis Loss : 5.804424290545285e-05\n",
      "Steps : 57500, \t Total Gen Loss : 25.943946838378906, \t Total Dis Loss : 5.860434612259269e-05\n",
      "Steps : 57600, \t Total Gen Loss : 22.753955841064453, \t Total Dis Loss : 0.00023838825291022658\n",
      "Steps : 57700, \t Total Gen Loss : 24.280216217041016, \t Total Dis Loss : 0.00018426307360641658\n",
      "Steps : 57800, \t Total Gen Loss : 24.7127685546875, \t Total Dis Loss : 0.00012852667714469135\n",
      "Steps : 57900, \t Total Gen Loss : 26.77776336669922, \t Total Dis Loss : 0.0004074989992659539\n",
      "Steps : 58000, \t Total Gen Loss : 28.102279663085938, \t Total Dis Loss : 9.947393846232444e-05\n",
      "Steps : 58100, \t Total Gen Loss : 25.94329071044922, \t Total Dis Loss : 5.4295607696985826e-05\n",
      "Steps : 58200, \t Total Gen Loss : 22.69816780090332, \t Total Dis Loss : 0.0030310112051665783\n",
      "Steps : 58300, \t Total Gen Loss : 21.539695739746094, \t Total Dis Loss : 0.001344735617749393\n",
      "Steps : 58400, \t Total Gen Loss : 25.48186683654785, \t Total Dis Loss : 0.00037838006392121315\n",
      "Steps : 58500, \t Total Gen Loss : 27.165721893310547, \t Total Dis Loss : 0.0006159382755868137\n",
      "Steps : 58600, \t Total Gen Loss : 24.215415954589844, \t Total Dis Loss : 5.266181688057259e-05\n",
      "Steps : 58700, \t Total Gen Loss : 23.341915130615234, \t Total Dis Loss : 0.0005388589343056083\n",
      "Steps : 58800, \t Total Gen Loss : 27.524124145507812, \t Total Dis Loss : 0.00047774886479601264\n",
      "Steps : 58900, \t Total Gen Loss : 29.152538299560547, \t Total Dis Loss : 0.0013164668343961239\n",
      "Steps : 59000, \t Total Gen Loss : 28.595783233642578, \t Total Dis Loss : 0.0005712577840313315\n",
      "Steps : 59100, \t Total Gen Loss : 26.702417373657227, \t Total Dis Loss : 0.0005756341270171106\n",
      "Steps : 59200, \t Total Gen Loss : 24.713916778564453, \t Total Dis Loss : 0.0011897971853613853\n",
      "Steps : 59300, \t Total Gen Loss : 25.333786010742188, \t Total Dis Loss : 0.0009242614032700658\n",
      "Steps : 59400, \t Total Gen Loss : 25.541595458984375, \t Total Dis Loss : 0.001446985173970461\n",
      "Steps : 59500, \t Total Gen Loss : 25.745250701904297, \t Total Dis Loss : 0.0002738636976573616\n",
      "Steps : 59600, \t Total Gen Loss : 26.367700576782227, \t Total Dis Loss : 9.544659405946732e-05\n",
      "Steps : 59700, \t Total Gen Loss : 25.482988357543945, \t Total Dis Loss : 0.00021187623497098684\n",
      "Steps : 59800, \t Total Gen Loss : 21.746746063232422, \t Total Dis Loss : 0.0004478914197534323\n",
      "Steps : 59900, \t Total Gen Loss : 22.968595504760742, \t Total Dis Loss : 0.00028164993273094296\n",
      "Steps : 60000, \t Total Gen Loss : 26.703018188476562, \t Total Dis Loss : 0.0001838558237068355\n",
      "Steps : 60100, \t Total Gen Loss : 25.525035858154297, \t Total Dis Loss : 0.0002021178079303354\n",
      "Steps : 60200, \t Total Gen Loss : 23.730449676513672, \t Total Dis Loss : 0.000291559292236343\n",
      "Steps : 60300, \t Total Gen Loss : 24.47311782836914, \t Total Dis Loss : 0.0006324945716187358\n",
      "Steps : 60400, \t Total Gen Loss : 30.685062408447266, \t Total Dis Loss : 0.0008405286935158074\n",
      "Steps : 60500, \t Total Gen Loss : 25.81777000427246, \t Total Dis Loss : 0.001221155165694654\n",
      "Steps : 60600, \t Total Gen Loss : 28.56775665283203, \t Total Dis Loss : 0.0001615539367776364\n",
      "Steps : 60700, \t Total Gen Loss : 22.05320930480957, \t Total Dis Loss : 0.0010182491969317198\n",
      "Steps : 60800, \t Total Gen Loss : 28.17184829711914, \t Total Dis Loss : 0.0004379181773401797\n",
      "Steps : 60900, \t Total Gen Loss : 26.91511344909668, \t Total Dis Loss : 0.0003342003037687391\n",
      "Steps : 61000, \t Total Gen Loss : 28.662702560424805, \t Total Dis Loss : 0.0005159528227522969\n",
      "Steps : 61100, \t Total Gen Loss : 27.905200958251953, \t Total Dis Loss : 0.000307435606373474\n",
      "Steps : 61200, \t Total Gen Loss : 25.139385223388672, \t Total Dis Loss : 7.705612370045856e-05\n",
      "Steps : 61300, \t Total Gen Loss : 27.997940063476562, \t Total Dis Loss : 5.5576994782313704e-05\n",
      "Steps : 61400, \t Total Gen Loss : 25.040721893310547, \t Total Dis Loss : 7.012846617726609e-05\n",
      "Steps : 61500, \t Total Gen Loss : 26.94295883178711, \t Total Dis Loss : 2.0800347556360066e-05\n",
      "Steps : 61600, \t Total Gen Loss : 24.740041732788086, \t Total Dis Loss : 0.0005469247698783875\n",
      "Steps : 61700, \t Total Gen Loss : 32.013092041015625, \t Total Dis Loss : 0.00011736463056877255\n",
      "Steps : 61800, \t Total Gen Loss : 23.911392211914062, \t Total Dis Loss : 0.001483116764575243\n",
      "Time for epoch 11 is 69.60915946960449 sec\n",
      "Steps : 61900, \t Total Gen Loss : 27.12371063232422, \t Total Dis Loss : 6.22716179350391e-05\n",
      "Steps : 62000, \t Total Gen Loss : 25.920438766479492, \t Total Dis Loss : 0.00031382200540974736\n",
      "Steps : 62100, \t Total Gen Loss : 25.10702133178711, \t Total Dis Loss : 0.00048124510794878006\n",
      "Steps : 62200, \t Total Gen Loss : 26.501667022705078, \t Total Dis Loss : 0.00308700418099761\n",
      "Steps : 62300, \t Total Gen Loss : 27.810361862182617, \t Total Dis Loss : 0.0012135612778365612\n",
      "Steps : 62400, \t Total Gen Loss : 27.82115936279297, \t Total Dis Loss : 8.918394451029599e-05\n",
      "Steps : 62500, \t Total Gen Loss : 26.770090103149414, \t Total Dis Loss : 0.0003246637643314898\n",
      "Steps : 62600, \t Total Gen Loss : 24.248769760131836, \t Total Dis Loss : 0.0017628951463848352\n",
      "Steps : 62700, \t Total Gen Loss : 28.57394027709961, \t Total Dis Loss : 0.0003579116309992969\n",
      "Steps : 62800, \t Total Gen Loss : 24.679344177246094, \t Total Dis Loss : 0.00018255716713611037\n",
      "Steps : 62900, \t Total Gen Loss : 25.470375061035156, \t Total Dis Loss : 0.0002158459392376244\n",
      "Steps : 63000, \t Total Gen Loss : 25.760650634765625, \t Total Dis Loss : 0.000436512753367424\n",
      "Steps : 63100, \t Total Gen Loss : 28.876399993896484, \t Total Dis Loss : 0.00024626776576042175\n",
      "Steps : 63200, \t Total Gen Loss : 29.01931381225586, \t Total Dis Loss : 0.0002073789801215753\n",
      "Steps : 63300, \t Total Gen Loss : 24.56528091430664, \t Total Dis Loss : 0.0002525282325223088\n",
      "Steps : 63400, \t Total Gen Loss : 23.53499984741211, \t Total Dis Loss : 0.0002393467293586582\n",
      "Steps : 63500, \t Total Gen Loss : 25.40102767944336, \t Total Dis Loss : 0.0001460649073123932\n",
      "Steps : 63600, \t Total Gen Loss : 24.300884246826172, \t Total Dis Loss : 0.00015540032472927123\n",
      "Steps : 63700, \t Total Gen Loss : 22.72818946838379, \t Total Dis Loss : 0.0009421446593478322\n",
      "Steps : 63800, \t Total Gen Loss : 25.24496841430664, \t Total Dis Loss : 0.0010820672614499927\n",
      "Steps : 63900, \t Total Gen Loss : 27.094215393066406, \t Total Dis Loss : 0.00046893468243069947\n",
      "Steps : 64000, \t Total Gen Loss : 24.544082641601562, \t Total Dis Loss : 0.00029920577071607113\n",
      "Steps : 64100, \t Total Gen Loss : 25.244468688964844, \t Total Dis Loss : 0.00022159129730425775\n",
      "Steps : 64200, \t Total Gen Loss : 25.22748565673828, \t Total Dis Loss : 0.00020599557319656014\n",
      "Steps : 64300, \t Total Gen Loss : 23.243982315063477, \t Total Dis Loss : 0.00032894048490561545\n",
      "Steps : 64400, \t Total Gen Loss : 24.36787223815918, \t Total Dis Loss : 0.00016068213153630495\n",
      "Steps : 64500, \t Total Gen Loss : 26.464744567871094, \t Total Dis Loss : 0.00017709756502881646\n",
      "Steps : 64600, \t Total Gen Loss : 23.30327606201172, \t Total Dis Loss : 0.00010298672714270651\n",
      "Steps : 64700, \t Total Gen Loss : 22.816974639892578, \t Total Dis Loss : 9.622899233363569e-05\n",
      "Steps : 64800, \t Total Gen Loss : 24.557289123535156, \t Total Dis Loss : 0.0003013647219631821\n",
      "Steps : 64900, \t Total Gen Loss : 26.197933197021484, \t Total Dis Loss : 0.00010462325735716149\n",
      "Steps : 65000, \t Total Gen Loss : 25.15943717956543, \t Total Dis Loss : 9.742619295138866e-05\n",
      "Steps : 65100, \t Total Gen Loss : 26.91550064086914, \t Total Dis Loss : 5.59329055249691e-05\n",
      "Steps : 65200, \t Total Gen Loss : 23.962947845458984, \t Total Dis Loss : 7.70926708355546e-05\n",
      "Steps : 65300, \t Total Gen Loss : 24.36505699157715, \t Total Dis Loss : 0.00010711878712754697\n",
      "Steps : 65400, \t Total Gen Loss : 25.08846092224121, \t Total Dis Loss : 0.000388285203371197\n",
      "Steps : 65500, \t Total Gen Loss : 24.51397132873535, \t Total Dis Loss : 0.0003718439256772399\n",
      "Steps : 65600, \t Total Gen Loss : 25.432071685791016, \t Total Dis Loss : 0.00040414914838038385\n",
      "Steps : 65700, \t Total Gen Loss : 22.117225646972656, \t Total Dis Loss : 0.0006724157137796283\n",
      "Steps : 65800, \t Total Gen Loss : 27.568889617919922, \t Total Dis Loss : 0.00021327828289940953\n",
      "Steps : 65900, \t Total Gen Loss : 26.53119659423828, \t Total Dis Loss : 0.0057295397855341434\n",
      "Steps : 66000, \t Total Gen Loss : 26.21535301208496, \t Total Dis Loss : 6.733424379490316e-05\n",
      "Steps : 66100, \t Total Gen Loss : 27.490718841552734, \t Total Dis Loss : 0.00035697678686119616\n",
      "Steps : 66200, \t Total Gen Loss : 24.230457305908203, \t Total Dis Loss : 0.00036049773916602135\n",
      "Steps : 66300, \t Total Gen Loss : 24.097225189208984, \t Total Dis Loss : 0.0005433016340248287\n",
      "Steps : 66400, \t Total Gen Loss : 22.861291885375977, \t Total Dis Loss : 0.00038061721716076136\n",
      "Steps : 66500, \t Total Gen Loss : 22.583282470703125, \t Total Dis Loss : 0.001418868312612176\n",
      "Steps : 66600, \t Total Gen Loss : 22.863258361816406, \t Total Dis Loss : 0.0006913101533427835\n",
      "Steps : 66700, \t Total Gen Loss : 22.3969783782959, \t Total Dis Loss : 0.0005796606419607997\n",
      "Steps : 66800, \t Total Gen Loss : 23.63619613647461, \t Total Dis Loss : 0.00042897500679828227\n",
      "Steps : 66900, \t Total Gen Loss : 24.16872215270996, \t Total Dis Loss : 0.0003607682592701167\n",
      "Steps : 67000, \t Total Gen Loss : 25.186307907104492, \t Total Dis Loss : 0.00021116853167768568\n",
      "Steps : 67100, \t Total Gen Loss : 22.88538360595703, \t Total Dis Loss : 0.00015219947090372443\n",
      "Steps : 67200, \t Total Gen Loss : 25.145009994506836, \t Total Dis Loss : 0.0005711889243684709\n",
      "Steps : 67300, \t Total Gen Loss : 25.594058990478516, \t Total Dis Loss : 0.00016361026791855693\n",
      "Steps : 67400, \t Total Gen Loss : 25.543899536132812, \t Total Dis Loss : 0.0005517731769941747\n",
      "Steps : 67500, \t Total Gen Loss : 25.51404571533203, \t Total Dis Loss : 0.0003592260181903839\n",
      "Time for epoch 12 is 69.58949947357178 sec\n",
      "Steps : 67600, \t Total Gen Loss : 26.430450439453125, \t Total Dis Loss : 0.005001977551728487\n",
      "Steps : 67700, \t Total Gen Loss : 28.648855209350586, \t Total Dis Loss : 5.802927626064047e-05\n",
      "Steps : 67800, \t Total Gen Loss : 26.352909088134766, \t Total Dis Loss : 0.00027679448248818517\n",
      "Steps : 67900, \t Total Gen Loss : 28.576148986816406, \t Total Dis Loss : 0.00014011147140990943\n",
      "Steps : 68000, \t Total Gen Loss : 23.551986694335938, \t Total Dis Loss : 0.00024437246611341834\n",
      "Steps : 68100, \t Total Gen Loss : 27.012357711791992, \t Total Dis Loss : 7.087356061674654e-05\n",
      "Steps : 68200, \t Total Gen Loss : 24.38558578491211, \t Total Dis Loss : 0.00020943928393535316\n",
      "Steps : 68300, \t Total Gen Loss : 24.519197463989258, \t Total Dis Loss : 0.0002859017695300281\n",
      "Steps : 68400, \t Total Gen Loss : 28.705677032470703, \t Total Dis Loss : 0.00012605941446963698\n",
      "Steps : 68500, \t Total Gen Loss : 24.87633514404297, \t Total Dis Loss : 8.038280066102743e-05\n",
      "Steps : 68600, \t Total Gen Loss : 24.938365936279297, \t Total Dis Loss : 0.0013447727542370558\n",
      "Steps : 68700, \t Total Gen Loss : 24.22936248779297, \t Total Dis Loss : 0.020810993388295174\n",
      "Steps : 68800, \t Total Gen Loss : 24.375831604003906, \t Total Dis Loss : 0.0005268529639579356\n",
      "Steps : 68900, \t Total Gen Loss : 24.27706527709961, \t Total Dis Loss : 0.0004554434563033283\n",
      "Steps : 69000, \t Total Gen Loss : 24.740440368652344, \t Total Dis Loss : 0.0002202730975113809\n",
      "Steps : 69100, \t Total Gen Loss : 26.481884002685547, \t Total Dis Loss : 0.0002454232017043978\n",
      "Steps : 69200, \t Total Gen Loss : 24.73428726196289, \t Total Dis Loss : 0.00014339439803734422\n",
      "Steps : 69300, \t Total Gen Loss : 26.285249710083008, \t Total Dis Loss : 0.00011111922503914684\n",
      "Steps : 69400, \t Total Gen Loss : 26.083778381347656, \t Total Dis Loss : 0.00013328962086234242\n",
      "Steps : 69500, \t Total Gen Loss : 24.787120819091797, \t Total Dis Loss : 4.1247800254495814e-05\n",
      "Steps : 69600, \t Total Gen Loss : 26.92377281188965, \t Total Dis Loss : 3.4525772207416594e-05\n",
      "Steps : 69700, \t Total Gen Loss : 26.746856689453125, \t Total Dis Loss : 4.945138789480552e-05\n",
      "Steps : 69800, \t Total Gen Loss : 25.804115295410156, \t Total Dis Loss : 6.347030284814537e-05\n",
      "Steps : 69900, \t Total Gen Loss : 24.318483352661133, \t Total Dis Loss : 7.492160511901602e-05\n",
      "Steps : 70000, \t Total Gen Loss : 27.46169662475586, \t Total Dis Loss : 4.533009632723406e-05\n",
      "Steps : 70100, \t Total Gen Loss : 25.51617431640625, \t Total Dis Loss : 0.00037994992453604937\n",
      "Steps : 70200, \t Total Gen Loss : 25.59983253479004, \t Total Dis Loss : 0.0001418228494003415\n",
      "Steps : 70300, \t Total Gen Loss : 24.336048126220703, \t Total Dis Loss : 0.00019951457215938717\n",
      "Steps : 70400, \t Total Gen Loss : 28.04075813293457, \t Total Dis Loss : 0.0012907360214740038\n",
      "Steps : 70500, \t Total Gen Loss : 25.68653106689453, \t Total Dis Loss : 0.02678052894771099\n",
      "Steps : 70600, \t Total Gen Loss : 24.796785354614258, \t Total Dis Loss : 0.00022475884179584682\n",
      "Steps : 70700, \t Total Gen Loss : 22.593782424926758, \t Total Dis Loss : 0.0004010511329397559\n",
      "Steps : 70800, \t Total Gen Loss : 24.21087646484375, \t Total Dis Loss : 0.0003527169465087354\n",
      "Steps : 70900, \t Total Gen Loss : 28.254552841186523, \t Total Dis Loss : 3.457042475929484e-05\n",
      "Steps : 71000, \t Total Gen Loss : 25.17901611328125, \t Total Dis Loss : 0.0009065708145499229\n",
      "Steps : 71100, \t Total Gen Loss : 26.124919891357422, \t Total Dis Loss : 0.0004201476986054331\n",
      "Steps : 71200, \t Total Gen Loss : 25.13345718383789, \t Total Dis Loss : 0.0004223475116305053\n",
      "Steps : 71300, \t Total Gen Loss : 25.222883224487305, \t Total Dis Loss : 0.0006170125561766326\n",
      "Steps : 71400, \t Total Gen Loss : 31.399158477783203, \t Total Dis Loss : 0.0014564080629497766\n",
      "Steps : 71500, \t Total Gen Loss : 29.656021118164062, \t Total Dis Loss : 0.003057118970900774\n",
      "Steps : 71600, \t Total Gen Loss : 26.059555053710938, \t Total Dis Loss : 8.15910316305235e-05\n",
      "Steps : 71700, \t Total Gen Loss : 27.279529571533203, \t Total Dis Loss : 0.00011463119881227612\n",
      "Steps : 71800, \t Total Gen Loss : 28.47283935546875, \t Total Dis Loss : 0.00010618620581226423\n",
      "Steps : 71900, \t Total Gen Loss : 26.049537658691406, \t Total Dis Loss : 0.00019163530669175088\n",
      "Steps : 72000, \t Total Gen Loss : 26.60154151916504, \t Total Dis Loss : 0.00012367399176582694\n",
      "Steps : 72100, \t Total Gen Loss : 26.630983352661133, \t Total Dis Loss : 0.00019030562543775886\n",
      "Steps : 72200, \t Total Gen Loss : 27.868988037109375, \t Total Dis Loss : 0.00011151237413287163\n",
      "Steps : 72300, \t Total Gen Loss : 26.73971176147461, \t Total Dis Loss : 8.324568625539541e-05\n",
      "Steps : 72400, \t Total Gen Loss : 27.497535705566406, \t Total Dis Loss : 5.981179128866643e-05\n",
      "Steps : 72500, \t Total Gen Loss : 27.107906341552734, \t Total Dis Loss : 7.25714344298467e-05\n",
      "Steps : 72600, \t Total Gen Loss : 28.79956817626953, \t Total Dis Loss : 9.315354691352695e-05\n",
      "Steps : 72700, \t Total Gen Loss : 26.69729232788086, \t Total Dis Loss : 6.616450991714373e-05\n",
      "Steps : 72800, \t Total Gen Loss : 26.806629180908203, \t Total Dis Loss : 8.171894296538085e-05\n",
      "Steps : 72900, \t Total Gen Loss : 26.255203247070312, \t Total Dis Loss : 0.00012985302601009607\n",
      "Steps : 73000, \t Total Gen Loss : 29.94676971435547, \t Total Dis Loss : 2.5487957827863283e-05\n",
      "Steps : 73100, \t Total Gen Loss : 29.456451416015625, \t Total Dis Loss : 8.418002107646316e-05\n",
      "Time for epoch 13 is 69.60090160369873 sec\n",
      "Steps : 73200, \t Total Gen Loss : 26.68596649169922, \t Total Dis Loss : 0.0014658846193924546\n",
      "Steps : 73300, \t Total Gen Loss : 29.06747055053711, \t Total Dis Loss : 0.005281677935272455\n",
      "Steps : 73400, \t Total Gen Loss : 28.542724609375, \t Total Dis Loss : 0.0001827920350478962\n",
      "Steps : 73500, \t Total Gen Loss : 28.625141143798828, \t Total Dis Loss : 3.591320273699239e-05\n",
      "Steps : 73600, \t Total Gen Loss : 28.304101943969727, \t Total Dis Loss : 5.427701398730278e-05\n",
      "Steps : 73700, \t Total Gen Loss : 26.472469329833984, \t Total Dis Loss : 0.0006197828915901482\n",
      "Steps : 73800, \t Total Gen Loss : 29.803621292114258, \t Total Dis Loss : 0.00010555566404946148\n",
      "Steps : 73900, \t Total Gen Loss : 28.462406158447266, \t Total Dis Loss : 0.0001228051696671173\n",
      "Steps : 74000, \t Total Gen Loss : 25.25848388671875, \t Total Dis Loss : 0.00048146722838282585\n",
      "Steps : 74100, \t Total Gen Loss : 25.475860595703125, \t Total Dis Loss : 0.0007803585031069815\n",
      "Steps : 74200, \t Total Gen Loss : 25.87502670288086, \t Total Dis Loss : 0.0016900835325941443\n",
      "Steps : 74300, \t Total Gen Loss : 26.636272430419922, \t Total Dis Loss : 0.00013262699940241873\n",
      "Steps : 74400, \t Total Gen Loss : 25.94911766052246, \t Total Dis Loss : 8.731721027288586e-05\n",
      "Steps : 74500, \t Total Gen Loss : 23.86865234375, \t Total Dis Loss : 0.0005172493983991444\n",
      "Steps : 74600, \t Total Gen Loss : 27.443805694580078, \t Total Dis Loss : 0.00012417617836035788\n",
      "Steps : 74700, \t Total Gen Loss : 23.871185302734375, \t Total Dis Loss : 0.00024813468917272985\n",
      "Steps : 74800, \t Total Gen Loss : 28.99054527282715, \t Total Dis Loss : 0.00018228466797154397\n",
      "Steps : 74900, \t Total Gen Loss : 26.79901885986328, \t Total Dis Loss : 0.00018001686839852482\n",
      "Steps : 75000, \t Total Gen Loss : 26.90005874633789, \t Total Dis Loss : 0.0002004316629609093\n",
      "Steps : 75100, \t Total Gen Loss : 26.402942657470703, \t Total Dis Loss : 8.579329733038321e-05\n",
      "Steps : 75200, \t Total Gen Loss : 27.129060745239258, \t Total Dis Loss : 0.00016121657972689718\n",
      "Steps : 75300, \t Total Gen Loss : 27.206899642944336, \t Total Dis Loss : 0.0001875427842605859\n",
      "Steps : 75400, \t Total Gen Loss : 26.635360717773438, \t Total Dis Loss : 0.0005635206471197307\n",
      "Steps : 75500, \t Total Gen Loss : 28.561954498291016, \t Total Dis Loss : 0.00027791628963313997\n",
      "Steps : 75600, \t Total Gen Loss : 24.091060638427734, \t Total Dis Loss : 0.00025367477792315185\n",
      "Steps : 75700, \t Total Gen Loss : 30.22943878173828, \t Total Dis Loss : 0.0001909697602968663\n",
      "Steps : 75800, \t Total Gen Loss : 27.039566040039062, \t Total Dis Loss : 0.00016506121028214693\n",
      "Steps : 75900, \t Total Gen Loss : 28.4710636138916, \t Total Dis Loss : 0.0003307176230009645\n",
      "Steps : 76000, \t Total Gen Loss : 29.76957130432129, \t Total Dis Loss : 7.517941412515938e-05\n",
      "Steps : 76100, \t Total Gen Loss : 27.07184600830078, \t Total Dis Loss : 0.004447800572961569\n",
      "Steps : 76200, \t Total Gen Loss : 27.180747985839844, \t Total Dis Loss : 6.597077299375087e-05\n",
      "Steps : 76300, \t Total Gen Loss : 25.31822395324707, \t Total Dis Loss : 8.735767914913595e-05\n",
      "Steps : 76400, \t Total Gen Loss : 28.74671173095703, \t Total Dis Loss : 6.708726868964732e-05\n",
      "Steps : 76500, \t Total Gen Loss : 28.725181579589844, \t Total Dis Loss : 3.5884575481759384e-05\n",
      "Steps : 76600, \t Total Gen Loss : 27.06124496459961, \t Total Dis Loss : 0.0001076746775652282\n",
      "Steps : 76700, \t Total Gen Loss : 25.10208511352539, \t Total Dis Loss : 0.006329527590423822\n",
      "Steps : 76800, \t Total Gen Loss : 25.141204833984375, \t Total Dis Loss : 0.0016541972290724516\n",
      "Steps : 76900, \t Total Gen Loss : 31.543487548828125, \t Total Dis Loss : 0.00014765633386559784\n",
      "Steps : 77000, \t Total Gen Loss : 25.530485153198242, \t Total Dis Loss : 0.0002076432283502072\n",
      "Steps : 77100, \t Total Gen Loss : 25.384906768798828, \t Total Dis Loss : 0.0004734583490062505\n",
      "Steps : 77200, \t Total Gen Loss : 25.86355209350586, \t Total Dis Loss : 0.0008976229000836611\n",
      "Steps : 77300, \t Total Gen Loss : 26.4322452545166, \t Total Dis Loss : 0.00015630437701474875\n",
      "Steps : 77400, \t Total Gen Loss : 26.294984817504883, \t Total Dis Loss : 0.0002984784368891269\n",
      "Steps : 77500, \t Total Gen Loss : 25.561946868896484, \t Total Dis Loss : 0.00014211435336619616\n",
      "Steps : 77600, \t Total Gen Loss : 28.24407196044922, \t Total Dis Loss : 0.0001529776054667309\n",
      "Steps : 77700, \t Total Gen Loss : 28.361541748046875, \t Total Dis Loss : 7.403174822684377e-05\n",
      "Steps : 77800, \t Total Gen Loss : 26.717275619506836, \t Total Dis Loss : 9.472027159063146e-05\n",
      "Steps : 77900, \t Total Gen Loss : 28.641178131103516, \t Total Dis Loss : 0.000318781501846388\n",
      "Steps : 78000, \t Total Gen Loss : 26.978107452392578, \t Total Dis Loss : 8.830016304273158e-05\n",
      "Steps : 78100, \t Total Gen Loss : 25.57487678527832, \t Total Dis Loss : 0.004638524726033211\n",
      "Steps : 78200, \t Total Gen Loss : 24.611774444580078, \t Total Dis Loss : 0.00019660033285617828\n",
      "Steps : 78300, \t Total Gen Loss : 25.089324951171875, \t Total Dis Loss : 0.00017152016516774893\n",
      "Steps : 78400, \t Total Gen Loss : 25.202129364013672, \t Total Dis Loss : 0.000406581733841449\n",
      "Steps : 78500, \t Total Gen Loss : 27.0207576751709, \t Total Dis Loss : 0.0007403245544992387\n",
      "Steps : 78600, \t Total Gen Loss : 23.661148071289062, \t Total Dis Loss : 0.0012452417286112905\n",
      "Steps : 78700, \t Total Gen Loss : 27.164886474609375, \t Total Dis Loss : 0.00027069702628068626\n",
      "Time for epoch 14 is 69.64788556098938 sec\n",
      "Steps : 78800, \t Total Gen Loss : 26.747421264648438, \t Total Dis Loss : 0.0009847052861005068\n",
      "Steps : 78900, \t Total Gen Loss : 25.66763687133789, \t Total Dis Loss : 0.000575254438444972\n",
      "Steps : 79000, \t Total Gen Loss : 23.413230895996094, \t Total Dis Loss : 0.00035715920967049897\n",
      "Steps : 79100, \t Total Gen Loss : 24.121448516845703, \t Total Dis Loss : 0.00040575978346168995\n",
      "Steps : 79200, \t Total Gen Loss : 22.264719009399414, \t Total Dis Loss : 0.0004295121179893613\n",
      "Steps : 79300, \t Total Gen Loss : 24.144874572753906, \t Total Dis Loss : 0.0005391568411141634\n",
      "Steps : 79400, \t Total Gen Loss : 24.83407974243164, \t Total Dis Loss : 0.00044408999383449554\n",
      "Steps : 79500, \t Total Gen Loss : 24.629291534423828, \t Total Dis Loss : 0.00019356480333954096\n",
      "Steps : 79600, \t Total Gen Loss : 24.323326110839844, \t Total Dis Loss : 0.0001696550752967596\n",
      "Steps : 79700, \t Total Gen Loss : 24.33994483947754, \t Total Dis Loss : 0.00044523554970510304\n",
      "Steps : 79800, \t Total Gen Loss : 21.599218368530273, \t Total Dis Loss : 0.0007263324223458767\n",
      "Steps : 79900, \t Total Gen Loss : 26.31049346923828, \t Total Dis Loss : 0.0008988443878479302\n",
      "Steps : 80000, \t Total Gen Loss : 24.033660888671875, \t Total Dis Loss : 0.0017549169715493917\n",
      "Steps : 80100, \t Total Gen Loss : 25.089723587036133, \t Total Dis Loss : 0.0003031930245924741\n",
      "Steps : 80200, \t Total Gen Loss : 23.46919059753418, \t Total Dis Loss : 0.0003626340185292065\n",
      "Steps : 80300, \t Total Gen Loss : 22.94727897644043, \t Total Dis Loss : 0.00019066107051912695\n",
      "Steps : 80400, \t Total Gen Loss : 24.452075958251953, \t Total Dis Loss : 0.0004251347854733467\n",
      "Steps : 80500, \t Total Gen Loss : 24.45758819580078, \t Total Dis Loss : 0.000274316145805642\n",
      "Steps : 80600, \t Total Gen Loss : 24.881071090698242, \t Total Dis Loss : 0.0002024057030212134\n",
      "Steps : 80700, \t Total Gen Loss : 20.729637145996094, \t Total Dis Loss : 0.013425689190626144\n",
      "Steps : 80800, \t Total Gen Loss : 24.533611297607422, \t Total Dis Loss : 0.0007843915955163538\n",
      "Steps : 80900, \t Total Gen Loss : 22.904293060302734, \t Total Dis Loss : 0.0004891162388958037\n",
      "Steps : 81000, \t Total Gen Loss : 25.514404296875, \t Total Dis Loss : 0.0007523593958467245\n",
      "Steps : 81100, \t Total Gen Loss : 24.462921142578125, \t Total Dis Loss : 0.00017018876678775996\n",
      "Steps : 81200, \t Total Gen Loss : 22.367525100708008, \t Total Dis Loss : 6.404446321539581e-05\n",
      "Steps : 81300, \t Total Gen Loss : 25.514659881591797, \t Total Dis Loss : 4.8390116717200726e-05\n",
      "Steps : 81400, \t Total Gen Loss : 25.256011962890625, \t Total Dis Loss : 0.0001630393962841481\n",
      "Steps : 81500, \t Total Gen Loss : 28.768280029296875, \t Total Dis Loss : 0.00020720253814943135\n",
      "Steps : 81600, \t Total Gen Loss : 27.46753692626953, \t Total Dis Loss : 0.00014370107965078205\n",
      "Steps : 81700, \t Total Gen Loss : 24.836349487304688, \t Total Dis Loss : 0.0027970285154879093\n",
      "Steps : 81800, \t Total Gen Loss : 27.378562927246094, \t Total Dis Loss : 0.00010799196024890989\n",
      "Steps : 81900, \t Total Gen Loss : 25.985013961791992, \t Total Dis Loss : 7.307039049919695e-05\n",
      "Steps : 82000, \t Total Gen Loss : 28.52630615234375, \t Total Dis Loss : 3.454171019257046e-05\n",
      "Steps : 82100, \t Total Gen Loss : 25.978071212768555, \t Total Dis Loss : 0.0003152964054606855\n",
      "Steps : 82200, \t Total Gen Loss : 31.38852310180664, \t Total Dis Loss : 0.0004656242672353983\n",
      "Steps : 82300, \t Total Gen Loss : 27.360626220703125, \t Total Dis Loss : 0.00016867504746187478\n",
      "Steps : 82400, \t Total Gen Loss : 28.018260955810547, \t Total Dis Loss : 0.0009248565183952451\n",
      "Steps : 82500, \t Total Gen Loss : 23.364774703979492, \t Total Dis Loss : 0.00011871611059177667\n",
      "Steps : 82600, \t Total Gen Loss : 24.55438995361328, \t Total Dis Loss : 0.0004332592070568353\n",
      "Steps : 82700, \t Total Gen Loss : 24.648706436157227, \t Total Dis Loss : 0.00022567126143258065\n",
      "Steps : 82800, \t Total Gen Loss : 25.125457763671875, \t Total Dis Loss : 6.736181967426091e-05\n",
      "Steps : 82900, \t Total Gen Loss : 26.354978561401367, \t Total Dis Loss : 7.757951971143484e-05\n",
      "Steps : 83000, \t Total Gen Loss : 26.241905212402344, \t Total Dis Loss : 9.701295493869111e-05\n",
      "Steps : 83100, \t Total Gen Loss : 24.209514617919922, \t Total Dis Loss : 0.0002909988397732377\n",
      "Steps : 83200, \t Total Gen Loss : 27.047725677490234, \t Total Dis Loss : 0.0001983650727197528\n",
      "Steps : 83300, \t Total Gen Loss : 23.718997955322266, \t Total Dis Loss : 0.00018495108815841377\n",
      "Steps : 83400, \t Total Gen Loss : 26.006454467773438, \t Total Dis Loss : 8.600394357927144e-05\n",
      "Steps : 83500, \t Total Gen Loss : 28.301300048828125, \t Total Dis Loss : 2.6316114599467255e-05\n",
      "Steps : 83600, \t Total Gen Loss : 23.556411743164062, \t Total Dis Loss : 0.00109230843372643\n",
      "Steps : 83700, \t Total Gen Loss : 25.375680923461914, \t Total Dis Loss : 0.0003231882001273334\n",
      "Steps : 83800, \t Total Gen Loss : 23.27484703063965, \t Total Dis Loss : 0.0002886370930355042\n",
      "Steps : 83900, \t Total Gen Loss : 24.552661895751953, \t Total Dis Loss : 0.00014361175999511033\n",
      "Steps : 84000, \t Total Gen Loss : 26.724136352539062, \t Total Dis Loss : 7.056145113892853e-05\n",
      "Steps : 84100, \t Total Gen Loss : 24.485958099365234, \t Total Dis Loss : 7.683943840675056e-05\n",
      "Steps : 84200, \t Total Gen Loss : 24.882898330688477, \t Total Dis Loss : 0.00028876858414150774\n",
      "Steps : 84300, \t Total Gen Loss : 26.5736141204834, \t Total Dis Loss : 0.00024122004106175154\n",
      "Time for epoch 15 is 70.54989123344421 sec\n",
      "Steps : 84400, \t Total Gen Loss : 25.913379669189453, \t Total Dis Loss : 0.00013179832603782415\n",
      "Steps : 84500, \t Total Gen Loss : 23.438453674316406, \t Total Dis Loss : 0.0003093958366662264\n",
      "Steps : 84600, \t Total Gen Loss : 24.044612884521484, \t Total Dis Loss : 0.0001334104163106531\n",
      "Steps : 84700, \t Total Gen Loss : 24.83110237121582, \t Total Dis Loss : 0.00011950894258916378\n",
      "Steps : 84800, \t Total Gen Loss : 23.82931137084961, \t Total Dis Loss : 7.591620669700205e-05\n",
      "Steps : 84900, \t Total Gen Loss : 27.799701690673828, \t Total Dis Loss : 0.0003344653523527086\n",
      "Steps : 85000, \t Total Gen Loss : 28.498931884765625, \t Total Dis Loss : 0.00015950774832163006\n",
      "Steps : 85100, \t Total Gen Loss : 27.015859603881836, \t Total Dis Loss : 0.08631031960248947\n",
      "Steps : 85200, \t Total Gen Loss : 30.300655364990234, \t Total Dis Loss : 0.001441802131012082\n",
      "Steps : 85300, \t Total Gen Loss : 28.72906494140625, \t Total Dis Loss : 0.0025998151395469904\n",
      "Steps : 85400, \t Total Gen Loss : 26.647098541259766, \t Total Dis Loss : 0.0003846633480861783\n",
      "Steps : 85500, \t Total Gen Loss : 30.14103126525879, \t Total Dis Loss : 0.00013444709475152194\n",
      "Steps : 85600, \t Total Gen Loss : 30.404888153076172, \t Total Dis Loss : 0.008538867346942425\n",
      "Steps : 85700, \t Total Gen Loss : 30.20537567138672, \t Total Dis Loss : 0.00017550824850331992\n",
      "Steps : 85800, \t Total Gen Loss : 29.414020538330078, \t Total Dis Loss : 7.734294194960967e-05\n",
      "Steps : 85900, \t Total Gen Loss : 29.97170066833496, \t Total Dis Loss : 4.5670210965909064e-05\n",
      "Steps : 86000, \t Total Gen Loss : 26.85162353515625, \t Total Dis Loss : 6.593025318579748e-05\n",
      "Steps : 86100, \t Total Gen Loss : 28.0675048828125, \t Total Dis Loss : 7.45897414162755e-05\n",
      "Steps : 86200, \t Total Gen Loss : 29.992509841918945, \t Total Dis Loss : 0.00011317795724608004\n",
      "Steps : 86300, \t Total Gen Loss : 31.18619155883789, \t Total Dis Loss : 0.00013620880781672895\n",
      "Steps : 86400, \t Total Gen Loss : 30.469810485839844, \t Total Dis Loss : 0.0001068910351023078\n",
      "Steps : 86500, \t Total Gen Loss : 30.248708724975586, \t Total Dis Loss : 9.276496712118387e-05\n",
      "Steps : 86600, \t Total Gen Loss : 30.269962310791016, \t Total Dis Loss : 0.00010745396139100194\n",
      "Steps : 86700, \t Total Gen Loss : 28.192401885986328, \t Total Dis Loss : 3.389502671780065e-05\n",
      "Steps : 86800, \t Total Gen Loss : 30.736825942993164, \t Total Dis Loss : 3.2451753213535994e-05\n",
      "Steps : 86900, \t Total Gen Loss : 26.75564956665039, \t Total Dis Loss : 4.7258101403713226e-05\n",
      "Steps : 87000, \t Total Gen Loss : 31.752182006835938, \t Total Dis Loss : 4.8090565542224795e-05\n",
      "Steps : 87100, \t Total Gen Loss : 29.021764755249023, \t Total Dis Loss : 0.00020301376935094595\n",
      "Steps : 87200, \t Total Gen Loss : 28.958675384521484, \t Total Dis Loss : 4.577344952849671e-05\n",
      "Steps : 87300, \t Total Gen Loss : 28.6535587310791, \t Total Dis Loss : 0.00010803619807120413\n",
      "Steps : 87400, \t Total Gen Loss : 30.619712829589844, \t Total Dis Loss : 0.00021741472301073372\n",
      "Steps : 87500, \t Total Gen Loss : 27.5455379486084, \t Total Dis Loss : 0.00019646569853648543\n",
      "Steps : 87600, \t Total Gen Loss : 28.035961151123047, \t Total Dis Loss : 6.362555723171681e-05\n",
      "Steps : 87700, \t Total Gen Loss : 31.859848022460938, \t Total Dis Loss : 1.3765014045930002e-05\n",
      "Steps : 87800, \t Total Gen Loss : 31.222631454467773, \t Total Dis Loss : 3.502560866763815e-05\n",
      "Steps : 87900, \t Total Gen Loss : 26.81084442138672, \t Total Dis Loss : 2.887844675569795e-05\n",
      "Steps : 88000, \t Total Gen Loss : 35.6208381652832, \t Total Dis Loss : 8.639962470624596e-05\n",
      "Steps : 88100, \t Total Gen Loss : 24.324642181396484, \t Total Dis Loss : 0.0014903475530445576\n",
      "Steps : 88200, \t Total Gen Loss : 26.035852432250977, \t Total Dis Loss : 0.0003061443567276001\n",
      "Steps : 88300, \t Total Gen Loss : 28.662952423095703, \t Total Dis Loss : 0.0002605100453365594\n",
      "Steps : 88400, \t Total Gen Loss : 24.505828857421875, \t Total Dis Loss : 0.00011417117639211938\n",
      "Steps : 88500, \t Total Gen Loss : 30.495264053344727, \t Total Dis Loss : 0.00011342680954840034\n",
      "Steps : 88600, \t Total Gen Loss : 29.460474014282227, \t Total Dis Loss : 0.0008402858511544764\n",
      "Steps : 88700, \t Total Gen Loss : 28.420093536376953, \t Total Dis Loss : 0.11937540769577026\n",
      "Steps : 88800, \t Total Gen Loss : 30.66412353515625, \t Total Dis Loss : 0.005304344929754734\n",
      "Steps : 88900, \t Total Gen Loss : 34.02593231201172, \t Total Dis Loss : 7.532933523179963e-05\n",
      "Steps : 89000, \t Total Gen Loss : 28.245756149291992, \t Total Dis Loss : 0.0029041478410363197\n",
      "Steps : 89100, \t Total Gen Loss : 30.979698181152344, \t Total Dis Loss : 0.0001678447879385203\n",
      "Steps : 89200, \t Total Gen Loss : 34.29383087158203, \t Total Dis Loss : 0.00040355048258788884\n",
      "Steps : 89300, \t Total Gen Loss : 28.044734954833984, \t Total Dis Loss : 0.00012734875781461596\n",
      "Steps : 89400, \t Total Gen Loss : 26.840347290039062, \t Total Dis Loss : 4.6350462071131915e-05\n",
      "Steps : 89500, \t Total Gen Loss : 28.963275909423828, \t Total Dis Loss : 0.0001817980082705617\n",
      "Steps : 89600, \t Total Gen Loss : 27.007883071899414, \t Total Dis Loss : 0.00015976361464709044\n",
      "Steps : 89700, \t Total Gen Loss : 25.76647186279297, \t Total Dis Loss : 8.935316145652905e-05\n",
      "Steps : 89800, \t Total Gen Loss : 23.361988067626953, \t Total Dis Loss : 0.0001245406165253371\n",
      "Steps : 89900, \t Total Gen Loss : 27.355606079101562, \t Total Dis Loss : 0.00011299354810034856\n",
      "Steps : 90000, \t Total Gen Loss : 31.156360626220703, \t Total Dis Loss : 0.002990465611219406\n",
      "Time for epoch 16 is 69.71532583236694 sec\n",
      "Steps : 90100, \t Total Gen Loss : 27.727046966552734, \t Total Dis Loss : 0.00046263402327895164\n",
      "Steps : 90200, \t Total Gen Loss : 27.709869384765625, \t Total Dis Loss : 0.00013647135347127914\n",
      "Steps : 90300, \t Total Gen Loss : 26.319869995117188, \t Total Dis Loss : 0.00010259607370244339\n",
      "Steps : 90400, \t Total Gen Loss : 29.59912872314453, \t Total Dis Loss : 7.581910176668316e-05\n",
      "Steps : 90500, \t Total Gen Loss : 28.284900665283203, \t Total Dis Loss : 0.00015257808263413608\n",
      "Steps : 90600, \t Total Gen Loss : 25.00965118408203, \t Total Dis Loss : 0.0002939504338428378\n",
      "Steps : 90700, \t Total Gen Loss : 25.909645080566406, \t Total Dis Loss : 0.0005307214451022446\n",
      "Steps : 90800, \t Total Gen Loss : 24.84107780456543, \t Total Dis Loss : 0.0005280828336253762\n",
      "Steps : 90900, \t Total Gen Loss : 27.450889587402344, \t Total Dis Loss : 6.122650665929541e-05\n",
      "Steps : 91000, \t Total Gen Loss : 26.829059600830078, \t Total Dis Loss : 0.00031380713335238397\n",
      "Steps : 91100, \t Total Gen Loss : 22.599403381347656, \t Total Dis Loss : 0.001160949352197349\n",
      "Steps : 91200, \t Total Gen Loss : 27.42274284362793, \t Total Dis Loss : 0.00019886084191966802\n",
      "Steps : 91300, \t Total Gen Loss : 25.954364776611328, \t Total Dis Loss : 0.00018584384815767407\n",
      "Steps : 91400, \t Total Gen Loss : 26.42298126220703, \t Total Dis Loss : 0.00025992191513068974\n",
      "Steps : 91500, \t Total Gen Loss : 25.93527603149414, \t Total Dis Loss : 0.0004284005844965577\n",
      "Steps : 91600, \t Total Gen Loss : 24.03192901611328, \t Total Dis Loss : 0.003731538075953722\n",
      "Steps : 91700, \t Total Gen Loss : 26.08871078491211, \t Total Dis Loss : 0.00015667089610360563\n",
      "Steps : 91800, \t Total Gen Loss : 29.02851676940918, \t Total Dis Loss : 4.5122615119908005e-05\n",
      "Steps : 91900, \t Total Gen Loss : 27.279624938964844, \t Total Dis Loss : 7.10675521986559e-05\n",
      "Steps : 92000, \t Total Gen Loss : 27.441450119018555, \t Total Dis Loss : 6.167226820252836e-05\n",
      "Steps : 92100, \t Total Gen Loss : 26.054264068603516, \t Total Dis Loss : 0.0009380215196870267\n",
      "Steps : 92200, \t Total Gen Loss : 26.321880340576172, \t Total Dis Loss : 6.54297909932211e-05\n",
      "Steps : 92300, \t Total Gen Loss : 26.50057601928711, \t Total Dis Loss : 0.0001049370039254427\n",
      "Steps : 92400, \t Total Gen Loss : 27.233383178710938, \t Total Dis Loss : 7.704406743869185e-05\n",
      "Steps : 92500, \t Total Gen Loss : 26.00616455078125, \t Total Dis Loss : 0.0002832297177519649\n",
      "Steps : 92600, \t Total Gen Loss : 29.84274673461914, \t Total Dis Loss : 0.00020189661881886423\n",
      "Steps : 92700, \t Total Gen Loss : 25.719314575195312, \t Total Dis Loss : 0.00012125205830670893\n",
      "Steps : 92800, \t Total Gen Loss : 23.97113609313965, \t Total Dis Loss : 0.0024642031639814377\n",
      "Steps : 92900, \t Total Gen Loss : 26.161602020263672, \t Total Dis Loss : 4.510566941462457e-05\n",
      "Steps : 93000, \t Total Gen Loss : 25.553136825561523, \t Total Dis Loss : 0.01412253174930811\n",
      "Steps : 93100, \t Total Gen Loss : 25.297395706176758, \t Total Dis Loss : 6.601225322810933e-05\n",
      "Steps : 93200, \t Total Gen Loss : 22.922405242919922, \t Total Dis Loss : 0.0013030844274908304\n",
      "Steps : 93300, \t Total Gen Loss : 24.8995361328125, \t Total Dis Loss : 0.000449636165285483\n",
      "Steps : 93400, \t Total Gen Loss : 24.350414276123047, \t Total Dis Loss : 0.0002541639260016382\n",
      "Steps : 93500, \t Total Gen Loss : 22.51069450378418, \t Total Dis Loss : 0.00037532596616074443\n",
      "Steps : 93600, \t Total Gen Loss : 25.115150451660156, \t Total Dis Loss : 0.00012371518823783845\n",
      "Steps : 93700, \t Total Gen Loss : 27.055072784423828, \t Total Dis Loss : 0.00019019744649995118\n",
      "Steps : 93800, \t Total Gen Loss : 26.735271453857422, \t Total Dis Loss : 0.00010311207006452605\n",
      "Steps : 93900, \t Total Gen Loss : 26.15869903564453, \t Total Dis Loss : 0.0002557018888182938\n",
      "Steps : 94000, \t Total Gen Loss : 26.306013107299805, \t Total Dis Loss : 0.0001423013600287959\n",
      "Steps : 94100, \t Total Gen Loss : 23.29165267944336, \t Total Dis Loss : 0.00037697426159866154\n",
      "Steps : 94200, \t Total Gen Loss : 25.053478240966797, \t Total Dis Loss : 0.00019676005467772484\n",
      "Steps : 94300, \t Total Gen Loss : 24.084566116333008, \t Total Dis Loss : 0.0003099293098784983\n",
      "Steps : 94400, \t Total Gen Loss : 27.336669921875, \t Total Dis Loss : 0.00012146499648224562\n",
      "Steps : 94500, \t Total Gen Loss : 25.33527946472168, \t Total Dis Loss : 0.0010142954997718334\n",
      "Steps : 94600, \t Total Gen Loss : 23.803203582763672, \t Total Dis Loss : 0.00020938765374012291\n",
      "Steps : 94700, \t Total Gen Loss : 28.270423889160156, \t Total Dis Loss : 4.57946807728149e-05\n",
      "Steps : 94800, \t Total Gen Loss : 26.539810180664062, \t Total Dis Loss : 0.00014063513663131744\n",
      "Steps : 94900, \t Total Gen Loss : 28.022808074951172, \t Total Dis Loss : 0.00012910837540403008\n",
      "Steps : 95000, \t Total Gen Loss : 26.453895568847656, \t Total Dis Loss : 0.00026105070719495416\n",
      "Steps : 95100, \t Total Gen Loss : 26.77960205078125, \t Total Dis Loss : 0.00014065094001125544\n",
      "Steps : 95200, \t Total Gen Loss : 26.339359283447266, \t Total Dis Loss : 2.6197962142759934e-05\n",
      "Steps : 95300, \t Total Gen Loss : 28.359434127807617, \t Total Dis Loss : 4.259406341589056e-05\n",
      "Steps : 95400, \t Total Gen Loss : 29.250043869018555, \t Total Dis Loss : 0.00024305617262143642\n",
      "Steps : 95500, \t Total Gen Loss : 28.380897521972656, \t Total Dis Loss : 0.00037849799264222383\n",
      "Steps : 95600, \t Total Gen Loss : 25.016143798828125, \t Total Dis Loss : 0.0001532929891254753\n",
      "Time for epoch 17 is 69.68242287635803 sec\n",
      "Steps : 95700, \t Total Gen Loss : 27.02724838256836, \t Total Dis Loss : 6.988869427004829e-05\n",
      "Steps : 95800, \t Total Gen Loss : 29.374052047729492, \t Total Dis Loss : 3.437416671658866e-05\n",
      "Steps : 95900, \t Total Gen Loss : 27.45159912109375, \t Total Dis Loss : 9.470063378103077e-05\n",
      "Steps : 96000, \t Total Gen Loss : 30.01394271850586, \t Total Dis Loss : 0.00041814736323431134\n",
      "Steps : 96100, \t Total Gen Loss : 25.675575256347656, \t Total Dis Loss : 0.0002502922434359789\n",
      "Steps : 96200, \t Total Gen Loss : 29.66748809814453, \t Total Dis Loss : 5.9860980400117114e-05\n",
      "Steps : 96300, \t Total Gen Loss : 22.97911834716797, \t Total Dis Loss : 0.004578998312354088\n",
      "Steps : 96400, \t Total Gen Loss : 27.249305725097656, \t Total Dis Loss : 8.77169004525058e-05\n",
      "Steps : 96500, \t Total Gen Loss : 31.201210021972656, \t Total Dis Loss : 4.569883458316326e-05\n",
      "Steps : 96600, \t Total Gen Loss : 31.175561904907227, \t Total Dis Loss : 4.092480230610818e-05\n",
      "Steps : 96700, \t Total Gen Loss : 30.135013580322266, \t Total Dis Loss : 5.082179995952174e-05\n",
      "Steps : 96800, \t Total Gen Loss : 32.51127243041992, \t Total Dis Loss : 1.7173977539641783e-05\n",
      "Steps : 96900, \t Total Gen Loss : 27.84868049621582, \t Total Dis Loss : 7.385651406366378e-05\n",
      "Steps : 97000, \t Total Gen Loss : 32.53059387207031, \t Total Dis Loss : 8.500317198922858e-05\n",
      "Steps : 97100, \t Total Gen Loss : 28.942176818847656, \t Total Dis Loss : 9.790990588953719e-05\n",
      "Steps : 97200, \t Total Gen Loss : 26.132705688476562, \t Total Dis Loss : 0.00014304669457487762\n",
      "Steps : 97300, \t Total Gen Loss : 30.91649627685547, \t Total Dis Loss : 7.162804831750691e-05\n",
      "Steps : 97400, \t Total Gen Loss : 26.828571319580078, \t Total Dis Loss : 6.60622026771307e-05\n",
      "Steps : 97500, \t Total Gen Loss : 27.840721130371094, \t Total Dis Loss : 5.6383687478955835e-05\n",
      "Steps : 97600, \t Total Gen Loss : 22.744794845581055, \t Total Dis Loss : 0.0019228296587243676\n",
      "Steps : 97700, \t Total Gen Loss : 27.890148162841797, \t Total Dis Loss : 3.039964758499991e-05\n",
      "Steps : 97800, \t Total Gen Loss : 27.91692352294922, \t Total Dis Loss : 0.00022783299209550023\n",
      "Steps : 97900, \t Total Gen Loss : 33.5755615234375, \t Total Dis Loss : 7.002422353252769e-05\n",
      "Steps : 98000, \t Total Gen Loss : 27.7794189453125, \t Total Dis Loss : 5.853792390553281e-05\n",
      "Steps : 98100, \t Total Gen Loss : 28.699138641357422, \t Total Dis Loss : 0.0001398738386342302\n",
      "Steps : 98200, \t Total Gen Loss : 26.723915100097656, \t Total Dis Loss : 0.00013759138528257608\n",
      "Steps : 98300, \t Total Gen Loss : 29.255573272705078, \t Total Dis Loss : 2.5887466108542867e-05\n",
      "Steps : 98400, \t Total Gen Loss : 25.71405029296875, \t Total Dis Loss : 0.00013293430674821138\n",
      "Steps : 98500, \t Total Gen Loss : 27.256528854370117, \t Total Dis Loss : 6.508549995487556e-05\n",
      "Steps : 98600, \t Total Gen Loss : 29.68518829345703, \t Total Dis Loss : 3.5826706152874976e-05\n",
      "Steps : 98700, \t Total Gen Loss : 29.550045013427734, \t Total Dis Loss : 7.138025466701947e-06\n",
      "Steps : 98800, \t Total Gen Loss : 28.480119705200195, \t Total Dis Loss : 4.0937524317996576e-05\n",
      "Steps : 98900, \t Total Gen Loss : 24.45136260986328, \t Total Dis Loss : 0.0012702117674052715\n",
      "Steps : 99000, \t Total Gen Loss : 26.999372482299805, \t Total Dis Loss : 4.725568578578532e-05\n",
      "Steps : 99100, \t Total Gen Loss : 29.07250213623047, \t Total Dis Loss : 1.9231649275752716e-05\n",
      "Steps : 99200, \t Total Gen Loss : 27.447895050048828, \t Total Dis Loss : 4.578405059874058e-05\n",
      "Steps : 99300, \t Total Gen Loss : 26.293235778808594, \t Total Dis Loss : 0.0001311822416028008\n",
      "Steps : 99400, \t Total Gen Loss : 26.536481857299805, \t Total Dis Loss : 6.184210360515863e-05\n",
      "Steps : 99500, \t Total Gen Loss : 25.439109802246094, \t Total Dis Loss : 0.00012384378351271152\n",
      "Steps : 99600, \t Total Gen Loss : 28.292865753173828, \t Total Dis Loss : 8.877117215888575e-05\n",
      "Steps : 99700, \t Total Gen Loss : 27.3731632232666, \t Total Dis Loss : 0.0004173903726041317\n",
      "Steps : 99800, \t Total Gen Loss : 27.699054718017578, \t Total Dis Loss : 5.5769407481420785e-05\n",
      "Steps : 99900, \t Total Gen Loss : 27.25403594970703, \t Total Dis Loss : 0.00020520875114016235\n",
      "Steps : 100000, \t Total Gen Loss : 25.092632293701172, \t Total Dis Loss : 6.277632928686216e-05\n",
      "Steps : 100100, \t Total Gen Loss : 26.42245101928711, \t Total Dis Loss : 3.238833596697077e-05\n",
      "Steps : 100200, \t Total Gen Loss : 28.0030460357666, \t Total Dis Loss : 0.00012695245095528662\n",
      "Steps : 100300, \t Total Gen Loss : 27.78864288330078, \t Total Dis Loss : 0.007989004254341125\n",
      "Steps : 100400, \t Total Gen Loss : 26.329858779907227, \t Total Dis Loss : 0.000529435055796057\n",
      "Steps : 100500, \t Total Gen Loss : 27.748600006103516, \t Total Dis Loss : 0.0007001433405093849\n",
      "Steps : 100600, \t Total Gen Loss : 28.94408416748047, \t Total Dis Loss : 1.96924083866179e-05\n",
      "Steps : 100700, \t Total Gen Loss : 28.858230590820312, \t Total Dis Loss : 0.00014008399739395827\n",
      "Steps : 100800, \t Total Gen Loss : 27.623056411743164, \t Total Dis Loss : 0.0007209455361589789\n",
      "Steps : 100900, \t Total Gen Loss : 27.327558517456055, \t Total Dis Loss : 0.00011234493285883218\n",
      "Steps : 101000, \t Total Gen Loss : 26.940006256103516, \t Total Dis Loss : 8.716250886209309e-05\n",
      "Steps : 101100, \t Total Gen Loss : 26.735328674316406, \t Total Dis Loss : 0.0003081793838646263\n",
      "Steps : 101200, \t Total Gen Loss : 27.959365844726562, \t Total Dis Loss : 6.656239565927535e-05\n",
      "Time for epoch 18 is 69.70378136634827 sec\n",
      "Steps : 101300, \t Total Gen Loss : 29.53683090209961, \t Total Dis Loss : 8.373437594855204e-06\n",
      "Steps : 101400, \t Total Gen Loss : 27.645248413085938, \t Total Dis Loss : 9.263474203180522e-05\n",
      "Steps : 101500, \t Total Gen Loss : 27.4405574798584, \t Total Dis Loss : 6.12139338045381e-05\n",
      "Steps : 101600, \t Total Gen Loss : 34.93122863769531, \t Total Dis Loss : 6.0326576203806326e-05\n",
      "Steps : 101700, \t Total Gen Loss : 28.129051208496094, \t Total Dis Loss : 0.00013820591266267002\n",
      "Steps : 101800, \t Total Gen Loss : 27.858041763305664, \t Total Dis Loss : 0.000396609102608636\n",
      "Steps : 101900, \t Total Gen Loss : 25.556873321533203, \t Total Dis Loss : 0.000609736773185432\n",
      "Steps : 102000, \t Total Gen Loss : 27.441007614135742, \t Total Dis Loss : 0.011446871794760227\n",
      "Steps : 102100, \t Total Gen Loss : 30.10318374633789, \t Total Dis Loss : 6.585021765204147e-05\n",
      "Steps : 102200, \t Total Gen Loss : 31.08594512939453, \t Total Dis Loss : 0.002250458113849163\n",
      "Steps : 102300, \t Total Gen Loss : 29.733179092407227, \t Total Dis Loss : 6.162696081446484e-06\n",
      "Steps : 102400, \t Total Gen Loss : 29.580411911010742, \t Total Dis Loss : 0.0003691121528390795\n",
      "Steps : 102500, \t Total Gen Loss : 31.54537582397461, \t Total Dis Loss : 2.4254644813481718e-05\n",
      "Steps : 102600, \t Total Gen Loss : 29.414478302001953, \t Total Dis Loss : 7.198262755991891e-05\n",
      "Steps : 102700, \t Total Gen Loss : 24.100282669067383, \t Total Dis Loss : 0.00033539970172569156\n",
      "Steps : 102800, \t Total Gen Loss : 26.818695068359375, \t Total Dis Loss : 0.00030624857754446566\n",
      "Steps : 102900, \t Total Gen Loss : 27.010196685791016, \t Total Dis Loss : 6.33240124443546e-05\n",
      "Steps : 103000, \t Total Gen Loss : 25.55710220336914, \t Total Dis Loss : 7.330771768465638e-05\n",
      "Steps : 103100, \t Total Gen Loss : 26.432758331298828, \t Total Dis Loss : 0.00012367015006020665\n",
      "Steps : 103200, \t Total Gen Loss : 24.7042236328125, \t Total Dis Loss : 0.00032663290039636195\n",
      "Steps : 103300, \t Total Gen Loss : 27.525358200073242, \t Total Dis Loss : 8.125857857521623e-05\n",
      "Steps : 103400, \t Total Gen Loss : 26.46783447265625, \t Total Dis Loss : 8.270043326774612e-05\n",
      "Steps : 103500, \t Total Gen Loss : 26.41061782836914, \t Total Dis Loss : 5.302342469803989e-05\n",
      "Steps : 103600, \t Total Gen Loss : 27.547935485839844, \t Total Dis Loss : 4.1764822526602075e-05\n",
      "Steps : 103700, \t Total Gen Loss : 25.921546936035156, \t Total Dis Loss : 0.0004421306075528264\n",
      "Steps : 103800, \t Total Gen Loss : 25.383272171020508, \t Total Dis Loss : 0.00020651065278798342\n",
      "Steps : 103900, \t Total Gen Loss : 27.62929916381836, \t Total Dis Loss : 4.27141239924822e-05\n",
      "Steps : 104000, \t Total Gen Loss : 25.914457321166992, \t Total Dis Loss : 0.13587108254432678\n",
      "Steps : 104100, \t Total Gen Loss : 22.71845245361328, \t Total Dis Loss : 0.00041296769632026553\n",
      "Steps : 104200, \t Total Gen Loss : 22.981151580810547, \t Total Dis Loss : 0.00029122945852577686\n",
      "Steps : 104300, \t Total Gen Loss : 22.936416625976562, \t Total Dis Loss : 0.000533676240593195\n",
      "Steps : 104400, \t Total Gen Loss : 28.463014602661133, \t Total Dis Loss : 0.0004209711914882064\n",
      "Steps : 104500, \t Total Gen Loss : 28.60008430480957, \t Total Dis Loss : 9.026262705447152e-05\n",
      "Steps : 104600, \t Total Gen Loss : 25.343578338623047, \t Total Dis Loss : 5.787623013020493e-05\n",
      "Steps : 104700, \t Total Gen Loss : 24.785804748535156, \t Total Dis Loss : 0.00013101105287205428\n",
      "Steps : 104800, \t Total Gen Loss : 29.2830867767334, \t Total Dis Loss : 0.00021071427909191698\n",
      "Steps : 104900, \t Total Gen Loss : 28.845760345458984, \t Total Dis Loss : 7.659402035642415e-05\n",
      "Steps : 105000, \t Total Gen Loss : 29.76780128479004, \t Total Dis Loss : 0.0003285215061623603\n",
      "Steps : 105100, \t Total Gen Loss : 22.327823638916016, \t Total Dis Loss : 0.0004934860626235604\n",
      "Steps : 105200, \t Total Gen Loss : 21.224618911743164, \t Total Dis Loss : 0.00037954619619995356\n",
      "Steps : 105300, \t Total Gen Loss : 24.99231719970703, \t Total Dis Loss : 0.00024502043379470706\n",
      "Steps : 105400, \t Total Gen Loss : 26.05029296875, \t Total Dis Loss : 0.00015692073793616146\n",
      "Steps : 105500, \t Total Gen Loss : 26.517946243286133, \t Total Dis Loss : 0.0001305446494370699\n",
      "Steps : 105600, \t Total Gen Loss : 28.94221305847168, \t Total Dis Loss : 0.0002053633361356333\n",
      "Steps : 105700, \t Total Gen Loss : 26.154727935791016, \t Total Dis Loss : 7.281447324203327e-05\n",
      "Steps : 105800, \t Total Gen Loss : 27.01956558227539, \t Total Dis Loss : 6.747271254425868e-05\n",
      "Steps : 105900, \t Total Gen Loss : 27.553565979003906, \t Total Dis Loss : 4.976892159902491e-05\n",
      "Steps : 106000, \t Total Gen Loss : 27.257190704345703, \t Total Dis Loss : 9.48496162891388e-05\n",
      "Steps : 106100, \t Total Gen Loss : 25.806751251220703, \t Total Dis Loss : 4.693548544310033e-05\n",
      "Steps : 106200, \t Total Gen Loss : 26.63253402709961, \t Total Dis Loss : 4.978483048034832e-05\n",
      "Steps : 106300, \t Total Gen Loss : 25.128015518188477, \t Total Dis Loss : 3.3440337574575096e-05\n",
      "Steps : 106400, \t Total Gen Loss : 28.577465057373047, \t Total Dis Loss : 2.5738760086824186e-05\n",
      "Steps : 106500, \t Total Gen Loss : 25.39665985107422, \t Total Dis Loss : 2.253434831800405e-05\n",
      "Steps : 106600, \t Total Gen Loss : 28.28353500366211, \t Total Dis Loss : 1.8673865270102397e-05\n",
      "Steps : 106700, \t Total Gen Loss : 27.606626510620117, \t Total Dis Loss : 2.047507223323919e-05\n",
      "Steps : 106800, \t Total Gen Loss : 25.592201232910156, \t Total Dis Loss : 2.6567346139927395e-05\n",
      "Time for epoch 19 is 69.71349048614502 sec\n",
      "Steps : 106900, \t Total Gen Loss : 30.511362075805664, \t Total Dis Loss : 2.1444058802444488e-05\n",
      "Steps : 107000, \t Total Gen Loss : 28.992748260498047, \t Total Dis Loss : 0.00019039759354200214\n",
      "Steps : 107100, \t Total Gen Loss : 22.259082794189453, \t Total Dis Loss : 0.0008938663522712886\n",
      "Steps : 107200, \t Total Gen Loss : 28.113849639892578, \t Total Dis Loss : 5.402386886999011e-05\n",
      "Steps : 107300, \t Total Gen Loss : 27.786365509033203, \t Total Dis Loss : 4.441753480932675e-05\n",
      "Steps : 107400, \t Total Gen Loss : 28.041194915771484, \t Total Dis Loss : 1.912242078105919e-05\n",
      "Steps : 107500, \t Total Gen Loss : 24.36398696899414, \t Total Dis Loss : 0.0005756424507126212\n",
      "Steps : 107600, \t Total Gen Loss : 21.495214462280273, \t Total Dis Loss : 0.0004992551985196769\n",
      "Steps : 107700, \t Total Gen Loss : 24.49127960205078, \t Total Dis Loss : 0.00043098139576613903\n",
      "Steps : 107800, \t Total Gen Loss : 25.952411651611328, \t Total Dis Loss : 0.00013093774032313377\n",
      "Steps : 107900, \t Total Gen Loss : 27.6658935546875, \t Total Dis Loss : 4.765015182783827e-05\n",
      "Steps : 108000, \t Total Gen Loss : 30.763530731201172, \t Total Dis Loss : 5.016680370317772e-05\n",
      "Steps : 108100, \t Total Gen Loss : 27.50238609313965, \t Total Dis Loss : 0.00023275011335499585\n",
      "Steps : 108200, \t Total Gen Loss : 27.81658935546875, \t Total Dis Loss : 1.5044164683786221e-05\n",
      "Steps : 108300, \t Total Gen Loss : 28.267871856689453, \t Total Dis Loss : 4.2901989218080416e-05\n",
      "Steps : 108400, \t Total Gen Loss : 25.248565673828125, \t Total Dis Loss : 8.654173871036619e-05\n",
      "Steps : 108500, \t Total Gen Loss : 25.497901916503906, \t Total Dis Loss : 0.00043406261829659343\n",
      "Steps : 108600, \t Total Gen Loss : 28.475189208984375, \t Total Dis Loss : 7.353650289587677e-05\n",
      "Steps : 108700, \t Total Gen Loss : 24.77139663696289, \t Total Dis Loss : 0.0005437260842882097\n",
      "Steps : 108800, \t Total Gen Loss : 27.028587341308594, \t Total Dis Loss : 6.595071317860857e-05\n",
      "Steps : 108900, \t Total Gen Loss : 26.211605072021484, \t Total Dis Loss : 0.00015516929852310568\n",
      "Steps : 109000, \t Total Gen Loss : 24.650516510009766, \t Total Dis Loss : 0.0045698946341872215\n",
      "Steps : 109100, \t Total Gen Loss : 27.072593688964844, \t Total Dis Loss : 7.613669731654227e-05\n",
      "Steps : 109200, \t Total Gen Loss : 27.169830322265625, \t Total Dis Loss : 3.574791116989218e-05\n",
      "Steps : 109300, \t Total Gen Loss : 25.492650985717773, \t Total Dis Loss : 0.00027209927793592215\n",
      "Steps : 109400, \t Total Gen Loss : 26.25741195678711, \t Total Dis Loss : 6.629538984270766e-05\n",
      "Steps : 109500, \t Total Gen Loss : 28.28382110595703, \t Total Dis Loss : 0.0002675737487152219\n",
      "Steps : 109600, \t Total Gen Loss : 28.799549102783203, \t Total Dis Loss : 2.1437348550534807e-05\n",
      "Steps : 109700, \t Total Gen Loss : 27.640167236328125, \t Total Dis Loss : 6.681753438897431e-05\n",
      "Steps : 109800, \t Total Gen Loss : 27.24604606628418, \t Total Dis Loss : 3.539317913237028e-05\n",
      "Steps : 109900, \t Total Gen Loss : 34.090240478515625, \t Total Dis Loss : 4.952348172082566e-05\n",
      "Steps : 110000, \t Total Gen Loss : 32.53549575805664, \t Total Dis Loss : 6.280941306613386e-05\n",
      "Steps : 110100, \t Total Gen Loss : 33.892822265625, \t Total Dis Loss : 0.00013701232091989368\n",
      "Steps : 110200, \t Total Gen Loss : 31.629043579101562, \t Total Dis Loss : 5.2974643040215597e-05\n",
      "Steps : 110300, \t Total Gen Loss : 28.42629051208496, \t Total Dis Loss : 0.00011860425001941621\n",
      "Steps : 110400, \t Total Gen Loss : 29.69237518310547, \t Total Dis Loss : 0.00012308011355344206\n",
      "Steps : 110500, \t Total Gen Loss : 27.602218627929688, \t Total Dis Loss : 0.0003735409118235111\n",
      "Steps : 110600, \t Total Gen Loss : 27.68743896484375, \t Total Dis Loss : 0.00010870535334106535\n",
      "Steps : 110700, \t Total Gen Loss : 29.94875717163086, \t Total Dis Loss : 2.1719088181271218e-05\n",
      "Steps : 110800, \t Total Gen Loss : 25.855342864990234, \t Total Dis Loss : 2.3544705982203595e-05\n",
      "Steps : 110900, \t Total Gen Loss : 26.669452667236328, \t Total Dis Loss : 3.002881203428842e-05\n",
      "Steps : 111000, \t Total Gen Loss : 26.41131591796875, \t Total Dis Loss : 0.00015743891708552837\n",
      "Steps : 111100, \t Total Gen Loss : 27.597427368164062, \t Total Dis Loss : 8.959310434875078e-06\n",
      "Steps : 111200, \t Total Gen Loss : 27.018184661865234, \t Total Dis Loss : 5.334526940714568e-05\n",
      "Steps : 111300, \t Total Gen Loss : 27.695669174194336, \t Total Dis Loss : 2.5826204364420846e-05\n",
      "Steps : 111400, \t Total Gen Loss : 29.603836059570312, \t Total Dis Loss : 1.625385084480513e-05\n",
      "Steps : 111500, \t Total Gen Loss : 28.223670959472656, \t Total Dis Loss : 1.904744749481324e-05\n",
      "Steps : 111600, \t Total Gen Loss : 27.37813949584961, \t Total Dis Loss : 8.708962559467182e-05\n",
      "Steps : 111700, \t Total Gen Loss : 28.66553497314453, \t Total Dis Loss : 1.408443131367676e-05\n",
      "Steps : 111800, \t Total Gen Loss : 28.07953453063965, \t Total Dis Loss : 1.6134532415890135e-05\n",
      "Steps : 111900, \t Total Gen Loss : 28.019445419311523, \t Total Dis Loss : 0.0005436113569885492\n",
      "Steps : 112000, \t Total Gen Loss : 26.8267822265625, \t Total Dis Loss : 0.00033073671511374414\n",
      "Steps : 112100, \t Total Gen Loss : 23.989166259765625, \t Total Dis Loss : 0.00010834551358129829\n",
      "Steps : 112200, \t Total Gen Loss : 28.67830467224121, \t Total Dis Loss : 5.458393934532069e-05\n",
      "Steps : 112300, \t Total Gen Loss : 24.034433364868164, \t Total Dis Loss : 4.9226269766222686e-05\n",
      "Steps : 112400, \t Total Gen Loss : 22.955738067626953, \t Total Dis Loss : 0.0024100651498883963\n",
      "Steps : 112500, \t Total Gen Loss : 27.06406021118164, \t Total Dis Loss : 0.0001916201872518286\n",
      "Time for epoch 20 is 70.44074511528015 sec\n",
      "Steps : 112600, \t Total Gen Loss : 24.520793914794922, \t Total Dis Loss : 0.0001070497528417036\n",
      "Steps : 112700, \t Total Gen Loss : 25.630815505981445, \t Total Dis Loss : 0.00014559536066371948\n",
      "Steps : 112800, \t Total Gen Loss : 24.9559326171875, \t Total Dis Loss : 0.00023521789989899844\n",
      "Steps : 112900, \t Total Gen Loss : 28.326431274414062, \t Total Dis Loss : 6.749358726665378e-05\n",
      "Steps : 113000, \t Total Gen Loss : 23.151168823242188, \t Total Dis Loss : 9.253087046090513e-05\n",
      "Steps : 113100, \t Total Gen Loss : 25.829063415527344, \t Total Dis Loss : 6.81519231875427e-05\n",
      "Steps : 113200, \t Total Gen Loss : 25.916152954101562, \t Total Dis Loss : 3.0714989406988025e-05\n",
      "Steps : 113300, \t Total Gen Loss : 24.66354751586914, \t Total Dis Loss : 9.121055336436257e-05\n",
      "Steps : 113400, \t Total Gen Loss : 22.8463191986084, \t Total Dis Loss : 0.0006724580307491124\n",
      "Steps : 113500, \t Total Gen Loss : 26.746131896972656, \t Total Dis Loss : 0.00021780419046990573\n",
      "Steps : 113600, \t Total Gen Loss : 25.227516174316406, \t Total Dis Loss : 0.00013934886374045163\n",
      "Steps : 113700, \t Total Gen Loss : 26.47999382019043, \t Total Dis Loss : 4.193143831798807e-05\n",
      "Steps : 113800, \t Total Gen Loss : 27.29354476928711, \t Total Dis Loss : 0.00012735187192447484\n",
      "Steps : 113900, \t Total Gen Loss : 27.995271682739258, \t Total Dis Loss : 0.00013250269694253802\n",
      "Steps : 114000, \t Total Gen Loss : 27.820301055908203, \t Total Dis Loss : 6.318629311863333e-05\n",
      "Steps : 114100, \t Total Gen Loss : 28.82189178466797, \t Total Dis Loss : 0.0008849598816595972\n",
      "Steps : 114200, \t Total Gen Loss : 24.557178497314453, \t Total Dis Loss : 0.0007161659887060523\n",
      "Steps : 114300, \t Total Gen Loss : 27.274700164794922, \t Total Dis Loss : 0.00012814815272577107\n",
      "Steps : 114400, \t Total Gen Loss : 26.89643096923828, \t Total Dis Loss : 7.781505701132119e-05\n",
      "Steps : 114500, \t Total Gen Loss : 23.70714569091797, \t Total Dis Loss : 0.0002742648939602077\n",
      "Steps : 114600, \t Total Gen Loss : 28.788421630859375, \t Total Dis Loss : 5.726516246795654e-05\n",
      "Steps : 114700, \t Total Gen Loss : 26.45193099975586, \t Total Dis Loss : 6.869908975204453e-05\n",
      "Steps : 114800, \t Total Gen Loss : 25.774276733398438, \t Total Dis Loss : 4.464043740881607e-05\n",
      "Steps : 114900, \t Total Gen Loss : 28.05301284790039, \t Total Dis Loss : 0.00012557790614664555\n",
      "Steps : 115000, \t Total Gen Loss : 27.286409378051758, \t Total Dis Loss : 3.384657975402661e-05\n",
      "Steps : 115100, \t Total Gen Loss : 28.370800018310547, \t Total Dis Loss : 9.111221879720688e-05\n",
      "Steps : 115200, \t Total Gen Loss : 28.21236801147461, \t Total Dis Loss : 2.4447328542009927e-05\n",
      "Steps : 115300, \t Total Gen Loss : 25.931232452392578, \t Total Dis Loss : 1.4424894288822543e-05\n",
      "Steps : 115400, \t Total Gen Loss : 27.06218719482422, \t Total Dis Loss : 0.0003343285352457315\n",
      "Steps : 115500, \t Total Gen Loss : 28.958974838256836, \t Total Dis Loss : 0.00030275151948444545\n",
      "Steps : 115600, \t Total Gen Loss : 29.08917999267578, \t Total Dis Loss : 0.0002563440939411521\n",
      "Steps : 115700, \t Total Gen Loss : 28.231374740600586, \t Total Dis Loss : 0.0002346720575587824\n",
      "Steps : 115800, \t Total Gen Loss : 27.32623291015625, \t Total Dis Loss : 3.095157444477081e-05\n",
      "Steps : 115900, \t Total Gen Loss : 27.471994400024414, \t Total Dis Loss : 3.165427187923342e-05\n",
      "Steps : 116000, \t Total Gen Loss : 29.783889770507812, \t Total Dis Loss : 6.817012035753578e-05\n",
      "Steps : 116100, \t Total Gen Loss : 27.575172424316406, \t Total Dis Loss : 2.552036130509805e-05\n",
      "Steps : 116200, \t Total Gen Loss : 24.274951934814453, \t Total Dis Loss : 0.00021168215607758611\n",
      "Steps : 116300, \t Total Gen Loss : 22.56210708618164, \t Total Dis Loss : 8.795188477961347e-05\n",
      "Steps : 116400, \t Total Gen Loss : 26.016477584838867, \t Total Dis Loss : 7.835952419554815e-05\n",
      "Steps : 116500, \t Total Gen Loss : 25.054092407226562, \t Total Dis Loss : 4.8623849579598755e-05\n",
      "Steps : 116600, \t Total Gen Loss : 24.976491928100586, \t Total Dis Loss : 2.1228846890153363e-05\n",
      "Steps : 116700, \t Total Gen Loss : 28.12799072265625, \t Total Dis Loss : 2.3272021280718036e-05\n",
      "Steps : 116800, \t Total Gen Loss : 24.121002197265625, \t Total Dis Loss : 8.082551357802004e-05\n",
      "Steps : 116900, \t Total Gen Loss : 27.01803207397461, \t Total Dis Loss : 4.829527097172104e-05\n",
      "Steps : 117000, \t Total Gen Loss : 28.178354263305664, \t Total Dis Loss : 5.476372461998835e-05\n",
      "Steps : 117100, \t Total Gen Loss : 25.812511444091797, \t Total Dis Loss : 7.113892206689343e-05\n",
      "Steps : 117200, \t Total Gen Loss : 26.22054672241211, \t Total Dis Loss : 3.232067683711648e-05\n",
      "Steps : 117300, \t Total Gen Loss : 27.101287841796875, \t Total Dis Loss : 0.0002103975130012259\n",
      "Steps : 117400, \t Total Gen Loss : 25.60794448852539, \t Total Dis Loss : 0.00011958428513025865\n",
      "Steps : 117500, \t Total Gen Loss : 24.325214385986328, \t Total Dis Loss : 5.258902456262149e-05\n",
      "Steps : 117600, \t Total Gen Loss : 23.372413635253906, \t Total Dis Loss : 5.2031038649147376e-05\n",
      "Steps : 117700, \t Total Gen Loss : 27.149555206298828, \t Total Dis Loss : 4.0780323615763336e-05\n",
      "Steps : 117800, \t Total Gen Loss : 25.249286651611328, \t Total Dis Loss : 7.067582191666588e-05\n",
      "Steps : 117900, \t Total Gen Loss : 27.168766021728516, \t Total Dis Loss : 5.741737186326645e-05\n",
      "Steps : 118000, \t Total Gen Loss : 25.159320831298828, \t Total Dis Loss : 4.067940608365461e-05\n",
      "Steps : 118100, \t Total Gen Loss : 30.22630500793457, \t Total Dis Loss : 3.350817860336974e-05\n",
      "Time for epoch 21 is 69.72037148475647 sec\n",
      "Steps : 118200, \t Total Gen Loss : 26.91172218322754, \t Total Dis Loss : 3.3720301871653646e-05\n",
      "Steps : 118300, \t Total Gen Loss : 25.88224220275879, \t Total Dis Loss : 3.2472355087520555e-05\n",
      "Steps : 118400, \t Total Gen Loss : 26.599233627319336, \t Total Dis Loss : 3.330892650410533e-05\n",
      "Steps : 118500, \t Total Gen Loss : 25.521408081054688, \t Total Dis Loss : 7.185338472481817e-05\n",
      "Steps : 118600, \t Total Gen Loss : 25.56032943725586, \t Total Dis Loss : 2.759917151706759e-05\n",
      "Steps : 118700, \t Total Gen Loss : 25.210166931152344, \t Total Dis Loss : 2.9622824513353407e-05\n",
      "Steps : 118800, \t Total Gen Loss : 26.01885223388672, \t Total Dis Loss : 2.997496267198585e-05\n",
      "Steps : 118900, \t Total Gen Loss : 26.092721939086914, \t Total Dis Loss : 2.598514038254507e-05\n",
      "Steps : 119000, \t Total Gen Loss : 26.09123420715332, \t Total Dis Loss : 6.218963244464248e-05\n",
      "Steps : 119100, \t Total Gen Loss : 28.041973114013672, \t Total Dis Loss : 0.0004870711709372699\n",
      "Steps : 119200, \t Total Gen Loss : 25.6832332611084, \t Total Dis Loss : 0.00031135929748415947\n",
      "Steps : 119300, \t Total Gen Loss : 24.457679748535156, \t Total Dis Loss : 5.580563811236061e-05\n",
      "Steps : 119400, \t Total Gen Loss : 26.528865814208984, \t Total Dis Loss : 0.0005122597212903202\n",
      "Steps : 119500, \t Total Gen Loss : 30.171545028686523, \t Total Dis Loss : 6.395504897227511e-05\n",
      "Steps : 119600, \t Total Gen Loss : 23.880756378173828, \t Total Dis Loss : 0.0011586929904296994\n",
      "Steps : 119700, \t Total Gen Loss : 28.098474502563477, \t Total Dis Loss : 2.8158594432170503e-05\n",
      "Steps : 119800, \t Total Gen Loss : 25.033971786499023, \t Total Dis Loss : 0.00034467133809812367\n",
      "Steps : 119900, \t Total Gen Loss : 22.975051879882812, \t Total Dis Loss : 0.0009532782714813948\n",
      "Steps : 120000, \t Total Gen Loss : 27.013151168823242, \t Total Dis Loss : 8.754391456022859e-05\n",
      "Steps : 120100, \t Total Gen Loss : 28.965850830078125, \t Total Dis Loss : 1.0515561370993964e-05\n",
      "Steps : 120200, \t Total Gen Loss : 25.556819915771484, \t Total Dis Loss : 1.957227141247131e-05\n",
      "Steps : 120300, \t Total Gen Loss : 29.110910415649414, \t Total Dis Loss : 3.717881190823391e-05\n",
      "Steps : 120400, \t Total Gen Loss : 32.74905776977539, \t Total Dis Loss : 5.705562671209918e-06\n",
      "Steps : 120500, \t Total Gen Loss : 29.564481735229492, \t Total Dis Loss : 5.907097147428431e-06\n",
      "Steps : 120600, \t Total Gen Loss : 28.70828628540039, \t Total Dis Loss : 0.0005054267821833491\n",
      "Steps : 120700, \t Total Gen Loss : 26.592004776000977, \t Total Dis Loss : 1.3369804037210997e-05\n",
      "Steps : 120800, \t Total Gen Loss : 25.529403686523438, \t Total Dis Loss : 0.00011937320232391357\n",
      "Steps : 120900, \t Total Gen Loss : 31.508804321289062, \t Total Dis Loss : 0.0001089817305910401\n",
      "Steps : 121000, \t Total Gen Loss : 27.311527252197266, \t Total Dis Loss : 1.0167127584281843e-05\n",
      "Steps : 121100, \t Total Gen Loss : 29.301807403564453, \t Total Dis Loss : 7.015642495389329e-06\n",
      "Steps : 121200, \t Total Gen Loss : 26.134231567382812, \t Total Dis Loss : 2.0698626030934975e-05\n",
      "Steps : 121300, \t Total Gen Loss : 29.412899017333984, \t Total Dis Loss : 5.451837932923809e-05\n",
      "Steps : 121400, \t Total Gen Loss : 27.1640567779541, \t Total Dis Loss : 0.00018507728236727417\n",
      "Steps : 121500, \t Total Gen Loss : 25.053503036499023, \t Total Dis Loss : 0.000577548227738589\n",
      "Steps : 121600, \t Total Gen Loss : 25.423532485961914, \t Total Dis Loss : 0.0005160096916370094\n",
      "Steps : 121700, \t Total Gen Loss : 31.469240188598633, \t Total Dis Loss : 6.318425585050136e-05\n",
      "Steps : 121800, \t Total Gen Loss : 28.069904327392578, \t Total Dis Loss : 0.0005174579564481974\n",
      "Steps : 121900, \t Total Gen Loss : 27.884977340698242, \t Total Dis Loss : 0.00013842825137544423\n",
      "Steps : 122000, \t Total Gen Loss : 26.188579559326172, \t Total Dis Loss : 0.0013813638361170888\n",
      "Steps : 122100, \t Total Gen Loss : 28.940689086914062, \t Total Dis Loss : 0.00018936186097562313\n",
      "Steps : 122200, \t Total Gen Loss : 25.180587768554688, \t Total Dis Loss : 0.00016836781287565827\n",
      "Steps : 122300, \t Total Gen Loss : 30.65380096435547, \t Total Dis Loss : 2.169672370655462e-05\n",
      "Steps : 122400, \t Total Gen Loss : 25.0150203704834, \t Total Dis Loss : 0.0004375074349809438\n",
      "Steps : 122500, \t Total Gen Loss : 29.63226890563965, \t Total Dis Loss : 0.00023134135699365288\n",
      "Steps : 122600, \t Total Gen Loss : 27.189916610717773, \t Total Dis Loss : 6.91099776304327e-05\n",
      "Steps : 122700, \t Total Gen Loss : 26.501920700073242, \t Total Dis Loss : 0.00047956628259271383\n",
      "Steps : 122800, \t Total Gen Loss : 27.864883422851562, \t Total Dis Loss : 0.00012672969023697078\n",
      "Steps : 122900, \t Total Gen Loss : 27.905242919921875, \t Total Dis Loss : 5.52656601939816e-05\n",
      "Steps : 123000, \t Total Gen Loss : 31.23177719116211, \t Total Dis Loss : 2.0580755517585203e-05\n",
      "Steps : 123100, \t Total Gen Loss : 25.946884155273438, \t Total Dis Loss : 0.0005003336118534207\n",
      "Steps : 123200, \t Total Gen Loss : 26.1921329498291, \t Total Dis Loss : 8.781715587247163e-05\n",
      "Steps : 123300, \t Total Gen Loss : 28.215665817260742, \t Total Dis Loss : 0.00012360475375317037\n",
      "Steps : 123400, \t Total Gen Loss : 29.16107749938965, \t Total Dis Loss : 7.547664426965639e-05\n",
      "Steps : 123500, \t Total Gen Loss : 27.105403900146484, \t Total Dis Loss : 0.00010400051542092115\n",
      "Steps : 123600, \t Total Gen Loss : 25.831743240356445, \t Total Dis Loss : 5.130298450239934e-05\n",
      "Steps : 123700, \t Total Gen Loss : 27.097156524658203, \t Total Dis Loss : 4.677899778471328e-05\n",
      "Time for epoch 22 is 72.40197038650513 sec\n",
      "Steps : 123800, \t Total Gen Loss : 26.0777587890625, \t Total Dis Loss : 3.8756581488996744e-05\n",
      "Steps : 123900, \t Total Gen Loss : 28.860353469848633, \t Total Dis Loss : 1.4153747542877682e-05\n",
      "Steps : 124000, \t Total Gen Loss : 29.25680160522461, \t Total Dis Loss : 8.236126450356096e-05\n",
      "Steps : 124100, \t Total Gen Loss : 29.632219314575195, \t Total Dis Loss : 2.970249261124991e-05\n",
      "Steps : 124200, \t Total Gen Loss : 29.28595542907715, \t Total Dis Loss : 0.00011768316471716389\n",
      "Steps : 124300, \t Total Gen Loss : 27.48369026184082, \t Total Dis Loss : 7.463958172593266e-05\n",
      "Steps : 124400, \t Total Gen Loss : 33.106834411621094, \t Total Dis Loss : 1.861186683527194e-05\n",
      "Steps : 124500, \t Total Gen Loss : 29.620193481445312, \t Total Dis Loss : 7.84191615821328e-06\n",
      "Steps : 124600, \t Total Gen Loss : 34.01917266845703, \t Total Dis Loss : 3.4178370697190985e-05\n",
      "Steps : 124700, \t Total Gen Loss : 29.97762107849121, \t Total Dis Loss : 1.0558292160567362e-05\n",
      "Steps : 124800, \t Total Gen Loss : 26.995119094848633, \t Total Dis Loss : 3.1634688639314845e-05\n",
      "Steps : 124900, \t Total Gen Loss : 30.044307708740234, \t Total Dis Loss : 3.765626024687663e-05\n",
      "Steps : 125000, \t Total Gen Loss : 25.370372772216797, \t Total Dis Loss : 1.945091389643494e-05\n",
      "Steps : 125100, \t Total Gen Loss : 26.80620765686035, \t Total Dis Loss : 2.7343119654688053e-05\n",
      "Steps : 125200, \t Total Gen Loss : 27.359825134277344, \t Total Dis Loss : 1.3946629223937634e-05\n",
      "Steps : 125300, \t Total Gen Loss : 27.144519805908203, \t Total Dis Loss : 0.0005019698874093592\n",
      "Steps : 125400, \t Total Gen Loss : 27.468528747558594, \t Total Dis Loss : 0.00016380423039663583\n",
      "Steps : 125500, \t Total Gen Loss : 28.56728744506836, \t Total Dis Loss : 5.870825771125965e-05\n",
      "Steps : 125600, \t Total Gen Loss : 27.335248947143555, \t Total Dis Loss : 7.164559792727232e-05\n",
      "Steps : 125700, \t Total Gen Loss : 26.525482177734375, \t Total Dis Loss : 5.6111915910150856e-05\n",
      "Steps : 125800, \t Total Gen Loss : 22.49576187133789, \t Total Dis Loss : 0.011147238314151764\n",
      "Steps : 125900, \t Total Gen Loss : 26.240497589111328, \t Total Dis Loss : 3.222648228984326e-05\n",
      "Steps : 126000, \t Total Gen Loss : 26.897010803222656, \t Total Dis Loss : 0.000542783469427377\n",
      "Steps : 126100, \t Total Gen Loss : 25.194622039794922, \t Total Dis Loss : 0.000132654735352844\n",
      "Steps : 126200, \t Total Gen Loss : 26.128459930419922, \t Total Dis Loss : 7.228253525681794e-05\n",
      "Steps : 126300, \t Total Gen Loss : 25.258136749267578, \t Total Dis Loss : 0.00020505311840679497\n",
      "Steps : 126400, \t Total Gen Loss : 22.502044677734375, \t Total Dis Loss : 0.0005525753949768841\n",
      "Steps : 126500, \t Total Gen Loss : 24.807449340820312, \t Total Dis Loss : 0.000590008101426065\n",
      "Steps : 126600, \t Total Gen Loss : 25.265888214111328, \t Total Dis Loss : 0.00012457987759262323\n",
      "Steps : 126700, \t Total Gen Loss : 28.805984497070312, \t Total Dis Loss : 2.9838796763215214e-05\n",
      "Steps : 126800, \t Total Gen Loss : 24.617523193359375, \t Total Dis Loss : 0.00031359304557554424\n",
      "Steps : 126900, \t Total Gen Loss : 29.58797264099121, \t Total Dis Loss : 0.00025775827816687524\n",
      "Steps : 127000, \t Total Gen Loss : 24.74187660217285, \t Total Dis Loss : 0.0002006727590924129\n",
      "Steps : 127100, \t Total Gen Loss : 27.668785095214844, \t Total Dis Loss : 8.329932461492717e-05\n",
      "Steps : 127200, \t Total Gen Loss : 28.104211807250977, \t Total Dis Loss : 0.0003532137197908014\n",
      "Steps : 127300, \t Total Gen Loss : 27.881633758544922, \t Total Dis Loss : 2.902609958255198e-05\n",
      "Steps : 127400, \t Total Gen Loss : 30.771726608276367, \t Total Dis Loss : 7.910448766779155e-05\n",
      "Steps : 127500, \t Total Gen Loss : 27.206531524658203, \t Total Dis Loss : 0.00022668365272693336\n",
      "Steps : 127600, \t Total Gen Loss : 25.57041358947754, \t Total Dis Loss : 0.00027787458384409547\n",
      "Steps : 127700, \t Total Gen Loss : 26.578651428222656, \t Total Dis Loss : 0.00016033656720537692\n",
      "Steps : 127800, \t Total Gen Loss : 23.93053436279297, \t Total Dis Loss : 0.0008919353131204844\n",
      "Steps : 127900, \t Total Gen Loss : 24.206396102905273, \t Total Dis Loss : 0.0003381107235327363\n",
      "Steps : 128000, \t Total Gen Loss : 26.126861572265625, \t Total Dis Loss : 0.0001701834553387016\n",
      "Steps : 128100, \t Total Gen Loss : 26.482269287109375, \t Total Dis Loss : 9.021028381539509e-05\n",
      "Steps : 128200, \t Total Gen Loss : 26.506221771240234, \t Total Dis Loss : 4.634022479876876e-05\n",
      "Steps : 128300, \t Total Gen Loss : 25.916112899780273, \t Total Dis Loss : 0.0002078037359751761\n",
      "Steps : 128400, \t Total Gen Loss : 23.109664916992188, \t Total Dis Loss : 0.0005557801923714578\n",
      "Steps : 128500, \t Total Gen Loss : 24.217052459716797, \t Total Dis Loss : 9.493422840023413e-05\n",
      "Steps : 128600, \t Total Gen Loss : 27.486839294433594, \t Total Dis Loss : 0.00012022784358123317\n",
      "Steps : 128700, \t Total Gen Loss : 26.700565338134766, \t Total Dis Loss : 7.809190719854087e-05\n",
      "Steps : 128800, \t Total Gen Loss : 27.61389923095703, \t Total Dis Loss : 3.923119948012754e-05\n",
      "Steps : 128900, \t Total Gen Loss : 29.251373291015625, \t Total Dis Loss : 4.5986660552443936e-05\n",
      "Steps : 129000, \t Total Gen Loss : 25.99590301513672, \t Total Dis Loss : 0.0004539837536867708\n",
      "Steps : 129100, \t Total Gen Loss : 24.5931339263916, \t Total Dis Loss : 0.00012948470248375088\n",
      "Steps : 129200, \t Total Gen Loss : 26.216140747070312, \t Total Dis Loss : 5.0793900300050154e-05\n",
      "Steps : 129300, \t Total Gen Loss : 22.753572463989258, \t Total Dis Loss : 0.0005034460336901248\n",
      "Time for epoch 23 is 75.58536529541016 sec\n",
      "Steps : 129400, \t Total Gen Loss : 26.161537170410156, \t Total Dis Loss : 8.530692139174789e-05\n",
      "Steps : 129500, \t Total Gen Loss : 26.911487579345703, \t Total Dis Loss : 8.846708078635857e-05\n",
      "Steps : 129600, \t Total Gen Loss : 26.898117065429688, \t Total Dis Loss : 3.637115878518671e-05\n",
      "Steps : 129700, \t Total Gen Loss : 24.144184112548828, \t Total Dis Loss : 5.367708581616171e-05\n",
      "Steps : 129800, \t Total Gen Loss : 26.64573860168457, \t Total Dis Loss : 2.8754699087585323e-05\n",
      "Steps : 129900, \t Total Gen Loss : 27.07257843017578, \t Total Dis Loss : 1.4535404261550866e-05\n",
      "Steps : 130000, \t Total Gen Loss : 27.668703079223633, \t Total Dis Loss : 1.4241389180824626e-05\n",
      "Steps : 130100, \t Total Gen Loss : 27.4119873046875, \t Total Dis Loss : 1.7389009371981956e-05\n",
      "Steps : 130200, \t Total Gen Loss : 27.860855102539062, \t Total Dis Loss : 1.6177182260435075e-05\n",
      "Steps : 130300, \t Total Gen Loss : 21.334152221679688, \t Total Dis Loss : 0.0010438431054353714\n",
      "Steps : 130400, \t Total Gen Loss : 23.03089714050293, \t Total Dis Loss : 0.00022902566706761718\n",
      "Steps : 130500, \t Total Gen Loss : 24.234798431396484, \t Total Dis Loss : 6.229340215213597e-05\n",
      "Steps : 130600, \t Total Gen Loss : 24.68557357788086, \t Total Dis Loss : 0.00014680637104902416\n",
      "Steps : 130700, \t Total Gen Loss : 27.715099334716797, \t Total Dis Loss : 6.966512592043728e-05\n",
      "Steps : 130800, \t Total Gen Loss : 25.40557098388672, \t Total Dis Loss : 3.549280881998129e-05\n",
      "Steps : 130900, \t Total Gen Loss : 26.656497955322266, \t Total Dis Loss : 2.671356560313143e-05\n",
      "Steps : 131000, \t Total Gen Loss : 25.89352798461914, \t Total Dis Loss : 4.679690027842298e-05\n",
      "Steps : 131100, \t Total Gen Loss : 26.924694061279297, \t Total Dis Loss : 0.00013202604895923287\n",
      "Steps : 131200, \t Total Gen Loss : 27.871173858642578, \t Total Dis Loss : 1.5407742466777563e-05\n",
      "Steps : 131300, \t Total Gen Loss : 24.419654846191406, \t Total Dis Loss : 3.075666245422326e-05\n",
      "Steps : 131400, \t Total Gen Loss : 25.239187240600586, \t Total Dis Loss : 9.170403791358694e-05\n",
      "Steps : 131500, \t Total Gen Loss : 24.850318908691406, \t Total Dis Loss : 6.495865090982988e-05\n",
      "Steps : 131600, \t Total Gen Loss : 26.2508602142334, \t Total Dis Loss : 0.00011751185229513794\n",
      "Steps : 131700, \t Total Gen Loss : 27.58917999267578, \t Total Dis Loss : 1.5498426364501938e-05\n",
      "Steps : 131800, \t Total Gen Loss : 25.03118133544922, \t Total Dis Loss : 0.005308507941663265\n",
      "Steps : 131900, \t Total Gen Loss : 26.409759521484375, \t Total Dis Loss : 0.00013405726349446923\n",
      "Steps : 132000, \t Total Gen Loss : 30.743144989013672, \t Total Dis Loss : 2.7376245270716026e-05\n",
      "Steps : 132100, \t Total Gen Loss : 27.5200252532959, \t Total Dis Loss : 3.250739973736927e-05\n",
      "Steps : 132200, \t Total Gen Loss : 28.33629608154297, \t Total Dis Loss : 0.0030415826477110386\n",
      "Steps : 132300, \t Total Gen Loss : 25.997756958007812, \t Total Dis Loss : 0.00014260373427532613\n",
      "Steps : 132400, \t Total Gen Loss : 25.86993980407715, \t Total Dis Loss : 0.00032945757266134024\n",
      "Steps : 132500, \t Total Gen Loss : 31.632314682006836, \t Total Dis Loss : 3.163119254168123e-05\n",
      "Steps : 132600, \t Total Gen Loss : 28.87221908569336, \t Total Dis Loss : 0.0001253923837793991\n",
      "Steps : 132700, \t Total Gen Loss : 26.107492446899414, \t Total Dis Loss : 0.00129652488976717\n",
      "Steps : 132800, \t Total Gen Loss : 29.33976936340332, \t Total Dis Loss : 2.1443405785248615e-05\n",
      "Steps : 132900, \t Total Gen Loss : 30.62673568725586, \t Total Dis Loss : 0.0001476057805120945\n",
      "Steps : 133000, \t Total Gen Loss : 29.197559356689453, \t Total Dis Loss : 0.0007878353935666382\n",
      "Steps : 133100, \t Total Gen Loss : 31.716432571411133, \t Total Dis Loss : 3.624477903940715e-05\n",
      "Steps : 133200, \t Total Gen Loss : 31.121936798095703, \t Total Dis Loss : 2.3562548449262977e-05\n",
      "Steps : 133300, \t Total Gen Loss : 31.112083435058594, \t Total Dis Loss : 7.257785910042003e-05\n",
      "Steps : 133400, \t Total Gen Loss : 32.5449104309082, \t Total Dis Loss : 5.8349029131932184e-05\n",
      "Steps : 133500, \t Total Gen Loss : 33.67928695678711, \t Total Dis Loss : 5.1759358029812574e-05\n",
      "Steps : 133600, \t Total Gen Loss : 27.880138397216797, \t Total Dis Loss : 0.00010742019367171451\n",
      "Steps : 133700, \t Total Gen Loss : 27.912471771240234, \t Total Dis Loss : 5.306050297804177e-05\n",
      "Steps : 133800, \t Total Gen Loss : 31.466020584106445, \t Total Dis Loss : 7.172109326347709e-05\n",
      "Steps : 133900, \t Total Gen Loss : 29.974613189697266, \t Total Dis Loss : 0.0003350291808601469\n",
      "Steps : 134000, \t Total Gen Loss : 28.603458404541016, \t Total Dis Loss : 4.9967715312959626e-05\n",
      "Steps : 134100, \t Total Gen Loss : 30.70696449279785, \t Total Dis Loss : 2.906503141275607e-05\n",
      "Steps : 134200, \t Total Gen Loss : 24.555805206298828, \t Total Dis Loss : 0.0035646127071231604\n",
      "Steps : 134300, \t Total Gen Loss : 29.804523468017578, \t Total Dis Loss : 0.0002286427334183827\n",
      "Steps : 134400, \t Total Gen Loss : 29.390121459960938, \t Total Dis Loss : 6.957067671464756e-05\n",
      "Steps : 134500, \t Total Gen Loss : 30.58844757080078, \t Total Dis Loss : 8.436582720605657e-05\n",
      "Steps : 134600, \t Total Gen Loss : 30.113683700561523, \t Total Dis Loss : 0.0001180298495455645\n",
      "Steps : 134700, \t Total Gen Loss : 30.125734329223633, \t Total Dis Loss : 0.0001222117425641045\n",
      "Steps : 134800, \t Total Gen Loss : 30.988826751708984, \t Total Dis Loss : 0.00014028878649696708\n",
      "Steps : 134900, \t Total Gen Loss : 28.559484481811523, \t Total Dis Loss : 7.863659266149625e-05\n",
      "Steps : 135000, \t Total Gen Loss : 30.89869499206543, \t Total Dis Loss : 2.39248856814811e-05\n",
      "Time for epoch 24 is 73.0174913406372 sec\n",
      "Steps : 135100, \t Total Gen Loss : 29.354351043701172, \t Total Dis Loss : 5.614493056782521e-05\n",
      "Steps : 135200, \t Total Gen Loss : 30.610694885253906, \t Total Dis Loss : 1.1737251043086872e-05\n",
      "Steps : 135300, \t Total Gen Loss : 30.229074478149414, \t Total Dis Loss : 1.078807508747559e-05\n",
      "Steps : 135400, \t Total Gen Loss : 27.754756927490234, \t Total Dis Loss : 6.505366036435589e-05\n",
      "Steps : 135500, \t Total Gen Loss : 28.770233154296875, \t Total Dis Loss : 0.00014030304737389088\n",
      "Steps : 135600, \t Total Gen Loss : 26.595829010009766, \t Total Dis Loss : 6.272218888625503e-05\n",
      "Steps : 135700, \t Total Gen Loss : 26.808616638183594, \t Total Dis Loss : 0.00012231098662596196\n",
      "Steps : 135800, \t Total Gen Loss : 26.164907455444336, \t Total Dis Loss : 0.00026290156529285014\n",
      "Steps : 135900, \t Total Gen Loss : 27.097612380981445, \t Total Dis Loss : 2.2679718313156627e-05\n",
      "Steps : 136000, \t Total Gen Loss : 29.27786636352539, \t Total Dis Loss : 2.6847155822906643e-05\n",
      "Steps : 136100, \t Total Gen Loss : 24.328989028930664, \t Total Dis Loss : 0.00022145385446492583\n",
      "Steps : 136200, \t Total Gen Loss : 26.704288482666016, \t Total Dis Loss : 0.0011985746677964926\n",
      "Steps : 136300, \t Total Gen Loss : 32.172428131103516, \t Total Dis Loss : 5.608805440715514e-05\n",
      "Steps : 136400, \t Total Gen Loss : 26.973251342773438, \t Total Dis Loss : 0.00022930459817871451\n",
      "Steps : 136500, \t Total Gen Loss : 25.523784637451172, \t Total Dis Loss : 5.315606904332526e-05\n",
      "Steps : 136600, \t Total Gen Loss : 27.712614059448242, \t Total Dis Loss : 3.551323970896192e-05\n",
      "Steps : 136700, \t Total Gen Loss : 26.11178207397461, \t Total Dis Loss : 4.4167827581986785e-05\n",
      "Steps : 136800, \t Total Gen Loss : 27.224607467651367, \t Total Dis Loss : 4.804313721251674e-05\n",
      "Steps : 136900, \t Total Gen Loss : 29.97252655029297, \t Total Dis Loss : 4.1438626794843e-05\n",
      "Steps : 137000, \t Total Gen Loss : 24.588241577148438, \t Total Dis Loss : 0.0014877888606861234\n",
      "Steps : 137100, \t Total Gen Loss : 25.901382446289062, \t Total Dis Loss : 0.0006650981376878917\n",
      "Steps : 137200, \t Total Gen Loss : 26.50820541381836, \t Total Dis Loss : 0.00022822257596999407\n",
      "Steps : 137300, \t Total Gen Loss : 26.727115631103516, \t Total Dis Loss : 0.00012079031876055524\n",
      "Steps : 137400, \t Total Gen Loss : 26.620975494384766, \t Total Dis Loss : 0.0010233491193503141\n",
      "Steps : 137500, \t Total Gen Loss : 29.484451293945312, \t Total Dis Loss : 8.761318167671561e-05\n",
      "Steps : 137600, \t Total Gen Loss : 31.083675384521484, \t Total Dis Loss : 0.0002618309226818383\n",
      "Steps : 137700, \t Total Gen Loss : 35.528778076171875, \t Total Dis Loss : 5.0627128075575456e-05\n",
      "Steps : 137800, \t Total Gen Loss : 28.5888671875, \t Total Dis Loss : 0.00016862532356753945\n",
      "Steps : 137900, \t Total Gen Loss : 25.577598571777344, \t Total Dis Loss : 8.132014045258984e-05\n",
      "Steps : 138000, \t Total Gen Loss : 27.559932708740234, \t Total Dis Loss : 0.5083340406417847\n",
      "Steps : 138100, \t Total Gen Loss : 29.224782943725586, \t Total Dis Loss : 0.0002545728348195553\n",
      "Steps : 138200, \t Total Gen Loss : 27.868881225585938, \t Total Dis Loss : 2.2983187591307797e-05\n",
      "Steps : 138300, \t Total Gen Loss : 23.930728912353516, \t Total Dis Loss : 0.0003527252411004156\n",
      "Steps : 138400, \t Total Gen Loss : 26.07843780517578, \t Total Dis Loss : 0.0006248787976801395\n",
      "Steps : 138500, \t Total Gen Loss : 27.60356903076172, \t Total Dis Loss : 6.801480776630342e-05\n",
      "Steps : 138600, \t Total Gen Loss : 24.28264617919922, \t Total Dis Loss : 9.552523260936141e-05\n",
      "Steps : 138700, \t Total Gen Loss : 27.032209396362305, \t Total Dis Loss : 0.00019209281890653074\n",
      "Steps : 138800, \t Total Gen Loss : 24.54904556274414, \t Total Dis Loss : 3.439878491917625e-05\n",
      "Steps : 138900, \t Total Gen Loss : 27.30184555053711, \t Total Dis Loss : 4.236403765389696e-05\n",
      "Steps : 139000, \t Total Gen Loss : 27.930273056030273, \t Total Dis Loss : 3.549427856341936e-05\n",
      "Steps : 139100, \t Total Gen Loss : 26.13581085205078, \t Total Dis Loss : 5.861973113496788e-05\n",
      "Steps : 139200, \t Total Gen Loss : 24.662582397460938, \t Total Dis Loss : 0.00017236382700502872\n",
      "Steps : 139300, \t Total Gen Loss : 25.99994468688965, \t Total Dis Loss : 8.134893869282678e-05\n",
      "Steps : 139400, \t Total Gen Loss : 29.048500061035156, \t Total Dis Loss : 1.0899047993007116e-05\n",
      "Steps : 139500, \t Total Gen Loss : 25.445281982421875, \t Total Dis Loss : 4.2035993828903884e-05\n",
      "Steps : 139600, \t Total Gen Loss : 27.446876525878906, \t Total Dis Loss : 1.833044552768115e-05\n",
      "Steps : 139700, \t Total Gen Loss : 29.8385009765625, \t Total Dis Loss : 9.305775165557861e-06\n",
      "Steps : 139800, \t Total Gen Loss : 26.83440399169922, \t Total Dis Loss : 1.046411125571467e-05\n",
      "Steps : 139900, \t Total Gen Loss : 27.192951202392578, \t Total Dis Loss : 3.116905645583756e-05\n",
      "Steps : 140000, \t Total Gen Loss : 27.027297973632812, \t Total Dis Loss : 1.871777385531459e-05\n",
      "Steps : 140100, \t Total Gen Loss : 26.901742935180664, \t Total Dis Loss : 6.156232848297805e-05\n",
      "Steps : 140200, \t Total Gen Loss : 25.647727966308594, \t Total Dis Loss : 2.7283756935503334e-05\n",
      "Steps : 140300, \t Total Gen Loss : 29.95297622680664, \t Total Dis Loss : 4.538063512882218e-05\n",
      "Steps : 140400, \t Total Gen Loss : 28.161319732666016, \t Total Dis Loss : 2.769072489172686e-05\n",
      "Steps : 140500, \t Total Gen Loss : 29.750003814697266, \t Total Dis Loss : 9.203392437484581e-06\n",
      "Steps : 140600, \t Total Gen Loss : 28.51056480407715, \t Total Dis Loss : 1.710027936496772e-05\n",
      "Time for epoch 25 is 75.58887100219727 sec\n",
      "Steps : 140700, \t Total Gen Loss : 26.756732940673828, \t Total Dis Loss : 2.669779132702388e-05\n",
      "Steps : 140800, \t Total Gen Loss : 26.565263748168945, \t Total Dis Loss : 5.318268449627794e-05\n",
      "Steps : 140900, \t Total Gen Loss : 27.643665313720703, \t Total Dis Loss : 4.0139118937077e-05\n",
      "Steps : 141000, \t Total Gen Loss : 26.07147216796875, \t Total Dis Loss : 3.0424585929722525e-05\n",
      "Steps : 141100, \t Total Gen Loss : 25.06626319885254, \t Total Dis Loss : 4.6716166252736e-05\n",
      "Steps : 141200, \t Total Gen Loss : 27.704681396484375, \t Total Dis Loss : 3.5398512409301475e-05\n",
      "Steps : 141300, \t Total Gen Loss : 25.29936981201172, \t Total Dis Loss : 4.472411819733679e-05\n",
      "Steps : 141400, \t Total Gen Loss : 27.678855895996094, \t Total Dis Loss : 6.841178401373327e-05\n",
      "Steps : 141500, \t Total Gen Loss : 25.199981689453125, \t Total Dis Loss : 1.9447705199127086e-05\n",
      "Steps : 141600, \t Total Gen Loss : 27.47303009033203, \t Total Dis Loss : 1.413494555890793e-05\n",
      "Steps : 141700, \t Total Gen Loss : 28.020259857177734, \t Total Dis Loss : 2.3679109290242195e-05\n",
      "Steps : 141800, \t Total Gen Loss : 27.977455139160156, \t Total Dis Loss : 2.212406434409786e-05\n",
      "Steps : 141900, \t Total Gen Loss : 28.867431640625, \t Total Dis Loss : 3.307613224023953e-05\n",
      "Steps : 142000, \t Total Gen Loss : 27.68281364440918, \t Total Dis Loss : 6.214817403815687e-06\n",
      "Steps : 142100, \t Total Gen Loss : 28.3942813873291, \t Total Dis Loss : 4.702402293332852e-06\n",
      "Steps : 142200, \t Total Gen Loss : 25.484024047851562, \t Total Dis Loss : 4.864494258072227e-05\n",
      "Steps : 142300, \t Total Gen Loss : 29.884387969970703, \t Total Dis Loss : 2.321142892469652e-05\n",
      "Steps : 142400, \t Total Gen Loss : 26.901390075683594, \t Total Dis Loss : 1.736669582896866e-05\n",
      "Steps : 142500, \t Total Gen Loss : 27.93665313720703, \t Total Dis Loss : 1.8437514881952666e-05\n",
      "Steps : 142600, \t Total Gen Loss : 27.764972686767578, \t Total Dis Loss : 1.4333043509395793e-05\n",
      "Steps : 142700, \t Total Gen Loss : 26.485851287841797, \t Total Dis Loss : 2.002236942644231e-05\n",
      "Steps : 142800, \t Total Gen Loss : 27.904020309448242, \t Total Dis Loss : 6.820208363933489e-05\n",
      "Steps : 142900, \t Total Gen Loss : 27.568119049072266, \t Total Dis Loss : 3.5642151487991214e-05\n",
      "Steps : 143000, \t Total Gen Loss : 26.17597770690918, \t Total Dis Loss : 0.00013824013876728714\n",
      "Steps : 143100, \t Total Gen Loss : 26.68758773803711, \t Total Dis Loss : 9.954823326552287e-05\n",
      "Steps : 143200, \t Total Gen Loss : 30.137157440185547, \t Total Dis Loss : 4.578407697408693e-06\n",
      "Steps : 143300, \t Total Gen Loss : 27.727928161621094, \t Total Dis Loss : 0.0011282078921794891\n",
      "Steps : 143400, \t Total Gen Loss : 31.3306884765625, \t Total Dis Loss : 1.2218812116771005e-05\n",
      "Steps : 143500, \t Total Gen Loss : 30.003021240234375, \t Total Dis Loss : 1.0423440471640788e-05\n",
      "Steps : 143600, \t Total Gen Loss : 30.019683837890625, \t Total Dis Loss : 2.9916895073256455e-05\n",
      "Steps : 143700, \t Total Gen Loss : 27.143983840942383, \t Total Dis Loss : 6.323115667328238e-05\n",
      "Steps : 143800, \t Total Gen Loss : 35.19828796386719, \t Total Dis Loss : 8.571733633289114e-05\n",
      "Steps : 143900, \t Total Gen Loss : 28.154338836669922, \t Total Dis Loss : 0.0002718325995374471\n",
      "Steps : 144000, \t Total Gen Loss : 30.173830032348633, \t Total Dis Loss : 8.084902947302908e-05\n",
      "Steps : 144100, \t Total Gen Loss : 25.708444595336914, \t Total Dis Loss : 0.00029738404555246234\n",
      "Steps : 144200, \t Total Gen Loss : 28.74132537841797, \t Total Dis Loss : 9.707077697385103e-05\n",
      "Steps : 144300, \t Total Gen Loss : 26.227985382080078, \t Total Dis Loss : 0.00013162565301172435\n",
      "Steps : 144400, \t Total Gen Loss : 26.99915885925293, \t Total Dis Loss : 0.00012013084051432088\n",
      "Steps : 144500, \t Total Gen Loss : 26.250818252563477, \t Total Dis Loss : 1.3527438568416983e-05\n",
      "Steps : 144600, \t Total Gen Loss : 25.406005859375, \t Total Dis Loss : 0.00012084336776752025\n",
      "Steps : 144700, \t Total Gen Loss : 29.477962493896484, \t Total Dis Loss : 2.5975939934141934e-05\n",
      "Steps : 144800, \t Total Gen Loss : 27.295021057128906, \t Total Dis Loss : 9.659818897489458e-05\n",
      "Steps : 144900, \t Total Gen Loss : 23.612796783447266, \t Total Dis Loss : 0.00014257286966312677\n",
      "Steps : 145000, \t Total Gen Loss : 29.855493545532227, \t Total Dis Loss : 5.56181330466643e-05\n",
      "Steps : 145100, \t Total Gen Loss : 26.933902740478516, \t Total Dis Loss : 2.7595506253419444e-05\n",
      "Steps : 145200, \t Total Gen Loss : 28.583316802978516, \t Total Dis Loss : 2.077677345369011e-05\n",
      "Steps : 145300, \t Total Gen Loss : 28.28786277770996, \t Total Dis Loss : 1.9176441128365695e-05\n",
      "Steps : 145400, \t Total Gen Loss : 26.79669189453125, \t Total Dis Loss : 1.9677299860632047e-05\n",
      "Steps : 145500, \t Total Gen Loss : 27.455089569091797, \t Total Dis Loss : 2.7831949410028756e-05\n",
      "Steps : 145600, \t Total Gen Loss : 27.740318298339844, \t Total Dis Loss : 2.1630574337905273e-05\n",
      "Steps : 145700, \t Total Gen Loss : 30.484128952026367, \t Total Dis Loss : 2.6955516659654677e-05\n",
      "Steps : 145800, \t Total Gen Loss : 27.838516235351562, \t Total Dis Loss : 7.019199074420612e-06\n",
      "Steps : 145900, \t Total Gen Loss : 27.59780502319336, \t Total Dis Loss : 2.0613548258552328e-05\n",
      "Steps : 146000, \t Total Gen Loss : 27.681896209716797, \t Total Dis Loss : 2.3498081645811908e-05\n",
      "Steps : 146100, \t Total Gen Loss : 36.483272552490234, \t Total Dis Loss : 0.00020460854284465313\n",
      "Steps : 146200, \t Total Gen Loss : 30.966392517089844, \t Total Dis Loss : 4.9806676543084905e-05\n",
      "Time for epoch 26 is 82.1041169166565 sec\n",
      "Steps : 146300, \t Total Gen Loss : 29.66982650756836, \t Total Dis Loss : 3.5279252188047394e-05\n",
      "Steps : 146400, \t Total Gen Loss : 28.019304275512695, \t Total Dis Loss : 0.00017358004697598517\n",
      "Steps : 146500, \t Total Gen Loss : 25.601444244384766, \t Total Dis Loss : 6.195965397637337e-05\n",
      "Steps : 146600, \t Total Gen Loss : 26.555259704589844, \t Total Dis Loss : 6.395249511115253e-05\n",
      "Steps : 146700, \t Total Gen Loss : 24.241512298583984, \t Total Dis Loss : 0.00021998728334438056\n",
      "Steps : 146800, \t Total Gen Loss : 25.450746536254883, \t Total Dis Loss : 0.00027270440477877855\n",
      "Steps : 146900, \t Total Gen Loss : 24.3309268951416, \t Total Dis Loss : 0.0001485906250309199\n",
      "Steps : 147000, \t Total Gen Loss : 25.96700668334961, \t Total Dis Loss : 8.297662861878052e-05\n",
      "Steps : 147100, \t Total Gen Loss : 25.862388610839844, \t Total Dis Loss : 5.9263700677547604e-05\n",
      "Steps : 147200, \t Total Gen Loss : 25.293777465820312, \t Total Dis Loss : 0.00048598088324069977\n",
      "Steps : 147300, \t Total Gen Loss : 23.452125549316406, \t Total Dis Loss : 0.0003883619501721114\n",
      "Steps : 147400, \t Total Gen Loss : 26.346637725830078, \t Total Dis Loss : 0.0002205985365435481\n",
      "Steps : 147500, \t Total Gen Loss : 25.463455200195312, \t Total Dis Loss : 0.00012669882562477142\n",
      "Steps : 147600, \t Total Gen Loss : 30.134292602539062, \t Total Dis Loss : 0.00015638476179447025\n",
      "Steps : 147700, \t Total Gen Loss : 26.463886260986328, \t Total Dis Loss : 8.026395516935736e-05\n",
      "Steps : 147800, \t Total Gen Loss : 25.12428092956543, \t Total Dis Loss : 0.00011033713235519826\n",
      "Steps : 147900, \t Total Gen Loss : 25.07640838623047, \t Total Dis Loss : 8.88613285496831e-05\n",
      "Steps : 148000, \t Total Gen Loss : 26.539987564086914, \t Total Dis Loss : 6.226114055607468e-05\n",
      "Steps : 148100, \t Total Gen Loss : 27.411273956298828, \t Total Dis Loss : 4.8292749852407724e-05\n",
      "Steps : 148200, \t Total Gen Loss : 26.442913055419922, \t Total Dis Loss : 4.248699406161904e-05\n",
      "Steps : 148300, \t Total Gen Loss : 29.24348258972168, \t Total Dis Loss : 3.5413235309533775e-05\n",
      "Steps : 148400, \t Total Gen Loss : 25.78605079650879, \t Total Dis Loss : 0.00011968216131208465\n",
      "Steps : 148500, \t Total Gen Loss : 26.84774398803711, \t Total Dis Loss : 3.0045297535252757e-05\n",
      "Steps : 148600, \t Total Gen Loss : 26.278352737426758, \t Total Dis Loss : 2.5089761038543656e-05\n",
      "Steps : 148700, \t Total Gen Loss : 25.740793228149414, \t Total Dis Loss : 2.5951496354537085e-05\n",
      "Steps : 148800, \t Total Gen Loss : 26.605518341064453, \t Total Dis Loss : 4.240014823153615e-05\n",
      "Steps : 148900, \t Total Gen Loss : 24.543996810913086, \t Total Dis Loss : 3.216040204279125e-05\n",
      "Steps : 149000, \t Total Gen Loss : 25.34505844116211, \t Total Dis Loss : 9.241888619726524e-05\n",
      "Steps : 149100, \t Total Gen Loss : 26.870887756347656, \t Total Dis Loss : 0.0001233041111845523\n",
      "Steps : 149200, \t Total Gen Loss : 27.834522247314453, \t Total Dis Loss : 2.5967225155909546e-05\n",
      "Steps : 149300, \t Total Gen Loss : 26.6149959564209, \t Total Dis Loss : 2.1965171981719323e-05\n",
      "Steps : 149400, \t Total Gen Loss : 30.547300338745117, \t Total Dis Loss : 2.1423849830171093e-05\n",
      "Steps : 149500, \t Total Gen Loss : 26.182636260986328, \t Total Dis Loss : 2.366888838878367e-05\n",
      "Steps : 149600, \t Total Gen Loss : 25.504886627197266, \t Total Dis Loss : 1.8102753529092297e-05\n",
      "Steps : 149700, \t Total Gen Loss : 29.33267593383789, \t Total Dis Loss : 5.254448115010746e-05\n",
      "Steps : 149800, \t Total Gen Loss : 28.29298210144043, \t Total Dis Loss : 2.886293441406451e-05\n",
      "Steps : 149900, \t Total Gen Loss : 25.8272705078125, \t Total Dis Loss : 1.7357413526042365e-05\n",
      "Steps : 150000, \t Total Gen Loss : 25.797060012817383, \t Total Dis Loss : 1.5740139133413322e-05\n",
      "Steps : 150100, \t Total Gen Loss : 24.40438461303711, \t Total Dis Loss : 4.244131923769601e-05\n",
      "Steps : 150200, \t Total Gen Loss : 25.795442581176758, \t Total Dis Loss : 4.5578919525723904e-05\n",
      "Steps : 150300, \t Total Gen Loss : 25.064964294433594, \t Total Dis Loss : 1.7948763343156315e-05\n",
      "Steps : 150400, \t Total Gen Loss : 25.66147232055664, \t Total Dis Loss : 6.406290776794776e-05\n",
      "Steps : 150500, \t Total Gen Loss : 27.67647933959961, \t Total Dis Loss : 2.7510273866937496e-05\n",
      "Steps : 150600, \t Total Gen Loss : 28.28188705444336, \t Total Dis Loss : 1.5340725440182723e-05\n",
      "Steps : 150700, \t Total Gen Loss : 27.069835662841797, \t Total Dis Loss : 2.2106502001406625e-05\n",
      "Steps : 150800, \t Total Gen Loss : 26.201047897338867, \t Total Dis Loss : 1.2563383279484697e-05\n",
      "Steps : 150900, \t Total Gen Loss : 28.325923919677734, \t Total Dis Loss : 8.85687222762499e-06\n",
      "Steps : 151000, \t Total Gen Loss : 27.234983444213867, \t Total Dis Loss : 1.0840823961189017e-05\n",
      "Steps : 151100, \t Total Gen Loss : 27.885150909423828, \t Total Dis Loss : 5.912645519856596e-06\n",
      "Steps : 151200, \t Total Gen Loss : 27.273387908935547, \t Total Dis Loss : 8.939260624174494e-06\n",
      "Steps : 151300, \t Total Gen Loss : 25.601476669311523, \t Total Dis Loss : 1.4708401067764498e-05\n",
      "Steps : 151400, \t Total Gen Loss : 27.89122772216797, \t Total Dis Loss : 2.3850941943237558e-05\n",
      "Steps : 151500, \t Total Gen Loss : 29.137771606445312, \t Total Dis Loss : 1.228770906891441e-05\n",
      "Steps : 151600, \t Total Gen Loss : 28.292984008789062, \t Total Dis Loss : 6.342589040286839e-05\n",
      "Steps : 151700, \t Total Gen Loss : 30.10529899597168, \t Total Dis Loss : 1.3060514902463183e-05\n",
      "Steps : 151800, \t Total Gen Loss : 28.86001205444336, \t Total Dis Loss : 3.573971116566099e-05\n",
      "Time for epoch 27 is 80.37326264381409 sec\n",
      "Steps : 151900, \t Total Gen Loss : 28.553897857666016, \t Total Dis Loss : 6.272327937040245e-06\n",
      "Steps : 152000, \t Total Gen Loss : 29.202499389648438, \t Total Dis Loss : 1.275494560104562e-05\n",
      "Steps : 152100, \t Total Gen Loss : 29.554277420043945, \t Total Dis Loss : 4.574334525386803e-06\n",
      "Steps : 152200, \t Total Gen Loss : 29.13345718383789, \t Total Dis Loss : 3.1548706829198636e-06\n",
      "Steps : 152300, \t Total Gen Loss : 29.643051147460938, \t Total Dis Loss : 3.3872829590109177e-06\n",
      "Steps : 152400, \t Total Gen Loss : 29.162599563598633, \t Total Dis Loss : 4.324657311371993e-06\n",
      "Steps : 152500, \t Total Gen Loss : 32.458988189697266, \t Total Dis Loss : 5.2559853429556824e-06\n",
      "Steps : 152600, \t Total Gen Loss : 26.402637481689453, \t Total Dis Loss : 4.320755579101387e-06\n",
      "Steps : 152700, \t Total Gen Loss : 29.418275833129883, \t Total Dis Loss : 9.002062142826617e-06\n",
      "Steps : 152800, \t Total Gen Loss : 31.024883270263672, \t Total Dis Loss : 3.3291062209173106e-06\n",
      "Steps : 152900, \t Total Gen Loss : 29.207435607910156, \t Total Dis Loss : 2.587877816040418e-06\n",
      "Steps : 153000, \t Total Gen Loss : 27.914955139160156, \t Total Dis Loss : 3.1651700282964157e-06\n",
      "Steps : 153100, \t Total Gen Loss : 26.804296493530273, \t Total Dis Loss : 8.27675248729065e-05\n",
      "Steps : 153200, \t Total Gen Loss : 28.92799186706543, \t Total Dis Loss : 7.674049811612349e-06\n",
      "Steps : 153300, \t Total Gen Loss : 27.90701675415039, \t Total Dis Loss : 2.786102186291828e-06\n",
      "Steps : 153400, \t Total Gen Loss : 29.28261947631836, \t Total Dis Loss : 4.141468707530294e-06\n",
      "Steps : 153500, \t Total Gen Loss : 24.80629539489746, \t Total Dis Loss : 0.0043454538099467754\n",
      "Steps : 153600, \t Total Gen Loss : 24.479244232177734, \t Total Dis Loss : 0.00018445953901391476\n",
      "Steps : 153700, \t Total Gen Loss : 24.264602661132812, \t Total Dis Loss : 4.4363205233821645e-05\n",
      "Steps : 153800, \t Total Gen Loss : 28.152679443359375, \t Total Dis Loss : 4.3493480916367844e-05\n",
      "Steps : 153900, \t Total Gen Loss : 25.261737823486328, \t Total Dis Loss : 0.00017374425078742206\n",
      "Steps : 154000, \t Total Gen Loss : 25.116989135742188, \t Total Dis Loss : 0.00027085619512945414\n",
      "Steps : 154100, \t Total Gen Loss : 22.700740814208984, \t Total Dis Loss : 0.0027855292428284883\n",
      "Steps : 154200, \t Total Gen Loss : 24.864891052246094, \t Total Dis Loss : 0.0005565294995903969\n",
      "Steps : 154300, \t Total Gen Loss : 26.130680084228516, \t Total Dis Loss : 3.101598849752918e-05\n",
      "Steps : 154400, \t Total Gen Loss : 26.162796020507812, \t Total Dis Loss : 0.00022792219533585012\n",
      "Steps : 154500, \t Total Gen Loss : 24.286903381347656, \t Total Dis Loss : 0.00011449207522673532\n",
      "Steps : 154600, \t Total Gen Loss : 29.33490562438965, \t Total Dis Loss : 0.00022006395738571882\n",
      "Steps : 154700, \t Total Gen Loss : 28.321287155151367, \t Total Dis Loss : 0.00021839357214048505\n",
      "Steps : 154800, \t Total Gen Loss : 24.16616439819336, \t Total Dis Loss : 0.00032781323534436524\n",
      "Steps : 154900, \t Total Gen Loss : 27.480464935302734, \t Total Dis Loss : 0.0001057219342328608\n",
      "Steps : 155000, \t Total Gen Loss : 28.74242401123047, \t Total Dis Loss : 6.370073242578655e-05\n",
      "Steps : 155100, \t Total Gen Loss : 28.09499740600586, \t Total Dis Loss : 9.852342191152275e-05\n",
      "Steps : 155200, \t Total Gen Loss : 29.35342788696289, \t Total Dis Loss : 7.033369911368936e-05\n",
      "Steps : 155300, \t Total Gen Loss : 26.629470825195312, \t Total Dis Loss : 4.837253436562605e-05\n",
      "Steps : 155400, \t Total Gen Loss : 29.05100440979004, \t Total Dis Loss : 3.8864382077008486e-05\n",
      "Steps : 155500, \t Total Gen Loss : 28.321453094482422, \t Total Dis Loss : 2.543937625887338e-05\n",
      "Steps : 155600, \t Total Gen Loss : 25.642024993896484, \t Total Dis Loss : 0.00016650964971631765\n",
      "Steps : 155700, \t Total Gen Loss : 24.997737884521484, \t Total Dis Loss : 0.00010214518988505006\n",
      "Steps : 155800, \t Total Gen Loss : 25.124135971069336, \t Total Dis Loss : 2.5002458642120473e-05\n",
      "Steps : 155900, \t Total Gen Loss : 26.363685607910156, \t Total Dis Loss : 5.045548459747806e-05\n",
      "Steps : 156000, \t Total Gen Loss : 23.884052276611328, \t Total Dis Loss : 0.00011383514356566593\n",
      "Steps : 156100, \t Total Gen Loss : 24.665693283081055, \t Total Dis Loss : 0.0004237017419654876\n",
      "Steps : 156200, \t Total Gen Loss : 28.400110244750977, \t Total Dis Loss : 3.2501739042345434e-05\n",
      "Steps : 156300, \t Total Gen Loss : 25.286083221435547, \t Total Dis Loss : 4.574110062094405e-05\n",
      "Steps : 156400, \t Total Gen Loss : 26.518003463745117, \t Total Dis Loss : 3.1969444535207e-05\n",
      "Steps : 156500, \t Total Gen Loss : 25.345420837402344, \t Total Dis Loss : 6.29151618340984e-05\n",
      "Steps : 156600, \t Total Gen Loss : 25.082237243652344, \t Total Dis Loss : 0.00015215124585665762\n",
      "Steps : 156700, \t Total Gen Loss : 25.616003036499023, \t Total Dis Loss : 7.822812767699361e-05\n",
      "Steps : 156800, \t Total Gen Loss : 26.096511840820312, \t Total Dis Loss : 9.013706585392356e-05\n",
      "Steps : 156900, \t Total Gen Loss : 25.764137268066406, \t Total Dis Loss : 0.0003237962373532355\n",
      "Steps : 157000, \t Total Gen Loss : 23.01941680908203, \t Total Dis Loss : 0.0003892032545991242\n",
      "Steps : 157100, \t Total Gen Loss : 25.152265548706055, \t Total Dis Loss : 3.983186979894526e-05\n",
      "Steps : 157200, \t Total Gen Loss : 23.779300689697266, \t Total Dis Loss : 0.0009865034371614456\n",
      "Steps : 157300, \t Total Gen Loss : 24.717208862304688, \t Total Dis Loss : 0.00021112954709678888\n",
      "Steps : 157400, \t Total Gen Loss : 27.760147094726562, \t Total Dis Loss : 4.6649843170598615e-06\n",
      "Steps : 157500, \t Total Gen Loss : 26.980487823486328, \t Total Dis Loss : 4.66601995867677e-05\n",
      "Time for epoch 28 is 80.08644270896912 sec\n",
      "Steps : 157600, \t Total Gen Loss : 26.12029266357422, \t Total Dis Loss : 0.0005941179697401822\n",
      "Steps : 157700, \t Total Gen Loss : 28.771928787231445, \t Total Dis Loss : 0.0003604224184527993\n",
      "Steps : 157800, \t Total Gen Loss : 29.356300354003906, \t Total Dis Loss : 4.793693369720131e-05\n",
      "Steps : 157900, \t Total Gen Loss : 26.022083282470703, \t Total Dis Loss : 4.926194378640503e-05\n",
      "Steps : 158000, \t Total Gen Loss : 28.77507781982422, \t Total Dis Loss : 1.442442589905113e-05\n",
      "Steps : 158100, \t Total Gen Loss : 26.935035705566406, \t Total Dis Loss : 1.2234232599439565e-05\n",
      "Steps : 158200, \t Total Gen Loss : 27.7998046875, \t Total Dis Loss : 4.2675041186157614e-05\n",
      "Steps : 158300, \t Total Gen Loss : 24.726600646972656, \t Total Dis Loss : 0.0003673313476610929\n",
      "Steps : 158400, \t Total Gen Loss : 27.447975158691406, \t Total Dis Loss : 0.0001446286478312686\n",
      "Steps : 158500, \t Total Gen Loss : 26.751453399658203, \t Total Dis Loss : 4.5170225348556414e-05\n",
      "Steps : 158600, \t Total Gen Loss : 34.5713996887207, \t Total Dis Loss : 9.966496463675867e-07\n",
      "Steps : 158700, \t Total Gen Loss : 32.070499420166016, \t Total Dis Loss : 6.709612534905318e-06\n",
      "Steps : 158800, \t Total Gen Loss : 32.46717071533203, \t Total Dis Loss : 1.2486415471357759e-05\n",
      "Steps : 158900, \t Total Gen Loss : 33.603729248046875, \t Total Dis Loss : 4.684695795731386e-06\n",
      "Steps : 159000, \t Total Gen Loss : 26.396047592163086, \t Total Dis Loss : 4.600474130711518e-05\n",
      "Steps : 159100, \t Total Gen Loss : 28.75174331665039, \t Total Dis Loss : 9.87193561741151e-06\n",
      "Steps : 159200, \t Total Gen Loss : 29.233081817626953, \t Total Dis Loss : 6.255770585994469e-06\n",
      "Steps : 159300, \t Total Gen Loss : 26.504348754882812, \t Total Dis Loss : 0.00010045242379419506\n",
      "Steps : 159400, \t Total Gen Loss : 27.18120574951172, \t Total Dis Loss : 3.30961229337845e-05\n",
      "Steps : 159500, \t Total Gen Loss : 28.684940338134766, \t Total Dis Loss : 0.00029191808425821364\n",
      "Steps : 159600, \t Total Gen Loss : 31.030689239501953, \t Total Dis Loss : 4.49831313744653e-05\n",
      "Steps : 159700, \t Total Gen Loss : 34.00825119018555, \t Total Dis Loss : 0.00030813299235887825\n",
      "Steps : 159800, \t Total Gen Loss : 32.80424880981445, \t Total Dis Loss : 0.00015828361210878938\n",
      "Steps : 159900, \t Total Gen Loss : 31.057918548583984, \t Total Dis Loss : 0.0001052795778377913\n",
      "Steps : 160000, \t Total Gen Loss : 33.46531677246094, \t Total Dis Loss : 0.00023538173991255462\n",
      "Steps : 160100, \t Total Gen Loss : 35.16521453857422, \t Total Dis Loss : 4.919559432892129e-05\n",
      "Steps : 160200, \t Total Gen Loss : 34.14861297607422, \t Total Dis Loss : 0.0004923423402942717\n",
      "Steps : 160300, \t Total Gen Loss : 35.38561248779297, \t Total Dis Loss : 0.00011775464372476563\n",
      "Steps : 160400, \t Total Gen Loss : 29.644521713256836, \t Total Dis Loss : 0.0004995819763280451\n",
      "Steps : 160500, \t Total Gen Loss : 26.696697235107422, \t Total Dis Loss : 0.003756576916202903\n",
      "Steps : 160600, \t Total Gen Loss : 27.74814224243164, \t Total Dis Loss : 5.8840643760049716e-05\n",
      "Steps : 160700, \t Total Gen Loss : 26.681373596191406, \t Total Dis Loss : 7.222012936836109e-05\n",
      "Steps : 160800, \t Total Gen Loss : 27.843769073486328, \t Total Dis Loss : 6.171719724079594e-05\n",
      "Steps : 160900, \t Total Gen Loss : 28.88412094116211, \t Total Dis Loss : 0.0001037723632180132\n",
      "Steps : 161000, \t Total Gen Loss : 30.440399169921875, \t Total Dis Loss : 9.746472642291337e-05\n",
      "Steps : 161100, \t Total Gen Loss : 30.185670852661133, \t Total Dis Loss : 4.5990353100933135e-05\n",
      "Steps : 161200, \t Total Gen Loss : 27.279102325439453, \t Total Dis Loss : 0.0003006523475050926\n",
      "Steps : 161300, \t Total Gen Loss : 26.322696685791016, \t Total Dis Loss : 0.0001790209353202954\n",
      "Steps : 161400, \t Total Gen Loss : 28.508089065551758, \t Total Dis Loss : 0.0011372780427336693\n",
      "Steps : 161500, \t Total Gen Loss : 28.127180099487305, \t Total Dis Loss : 0.00017362376092933118\n",
      "Steps : 161600, \t Total Gen Loss : 29.73038673400879, \t Total Dis Loss : 3.788989852182567e-05\n",
      "Steps : 161700, \t Total Gen Loss : 28.720861434936523, \t Total Dis Loss : 4.919336788589135e-05\n",
      "Steps : 161800, \t Total Gen Loss : 28.347108840942383, \t Total Dis Loss : 8.248077210737392e-05\n",
      "Steps : 161900, \t Total Gen Loss : 29.53905487060547, \t Total Dis Loss : 7.35532958060503e-05\n",
      "Steps : 162000, \t Total Gen Loss : 28.45652961730957, \t Total Dis Loss : 3.0417202651733533e-05\n",
      "Steps : 162100, \t Total Gen Loss : 30.961835861206055, \t Total Dis Loss : 7.67667661421001e-05\n",
      "Steps : 162200, \t Total Gen Loss : 28.91320037841797, \t Total Dis Loss : 7.168509910115972e-05\n",
      "Steps : 162300, \t Total Gen Loss : 32.705657958984375, \t Total Dis Loss : 0.0001329188671661541\n",
      "Steps : 162400, \t Total Gen Loss : 27.915767669677734, \t Total Dis Loss : 3.78592376364395e-05\n",
      "Steps : 162500, \t Total Gen Loss : 29.979021072387695, \t Total Dis Loss : 2.1846590243512765e-05\n",
      "Steps : 162600, \t Total Gen Loss : 32.62758255004883, \t Total Dis Loss : 3.5536460927687585e-05\n",
      "Steps : 162700, \t Total Gen Loss : 29.86088752746582, \t Total Dis Loss : 5.087393583380617e-05\n",
      "Steps : 162800, \t Total Gen Loss : 32.099281311035156, \t Total Dis Loss : 1.342294126516208e-05\n",
      "Steps : 162900, \t Total Gen Loss : 27.305362701416016, \t Total Dis Loss : 0.00029340246692299843\n",
      "Steps : 163000, \t Total Gen Loss : 30.171314239501953, \t Total Dis Loss : 0.00031460030004382133\n",
      "Steps : 163100, \t Total Gen Loss : 26.779125213623047, \t Total Dis Loss : 0.00012953605619259179\n",
      "Time for epoch 29 is 77.22390556335449 sec\n",
      "Steps : 163200, \t Total Gen Loss : 29.674222946166992, \t Total Dis Loss : 3.182552245561965e-05\n",
      "Steps : 163300, \t Total Gen Loss : 24.996471405029297, \t Total Dis Loss : 9.420518472325057e-05\n",
      "Steps : 163400, \t Total Gen Loss : 27.014034271240234, \t Total Dis Loss : 7.002230267971754e-05\n",
      "Steps : 163500, \t Total Gen Loss : 29.14895248413086, \t Total Dis Loss : 1.909886850626208e-05\n",
      "Steps : 163600, \t Total Gen Loss : 30.094743728637695, \t Total Dis Loss : 1.0523206583457068e-05\n",
      "Steps : 163700, \t Total Gen Loss : 30.05499267578125, \t Total Dis Loss : 0.00011772438301704824\n",
      "Steps : 163800, \t Total Gen Loss : 24.52337074279785, \t Total Dis Loss : 3.564580765669234e-05\n",
      "Steps : 163900, \t Total Gen Loss : 28.951152801513672, \t Total Dis Loss : 4.2282434151275083e-05\n",
      "Steps : 164000, \t Total Gen Loss : 29.595809936523438, \t Total Dis Loss : 4.085273030796088e-05\n",
      "Steps : 164100, \t Total Gen Loss : 28.89743995666504, \t Total Dis Loss : 0.00010875182488234714\n",
      "Steps : 164200, \t Total Gen Loss : 28.458248138427734, \t Total Dis Loss : 0.0009306564461439848\n",
      "Steps : 164300, \t Total Gen Loss : 28.24386215209961, \t Total Dis Loss : 0.0017075496725738049\n",
      "Steps : 164400, \t Total Gen Loss : 29.34183120727539, \t Total Dis Loss : 1.304628858633805e-05\n",
      "Steps : 164500, \t Total Gen Loss : 26.063800811767578, \t Total Dis Loss : 0.0016984910471364856\n",
      "Steps : 164600, \t Total Gen Loss : 26.223215103149414, \t Total Dis Loss : 1.828050153562799e-05\n",
      "Steps : 164700, \t Total Gen Loss : 30.0695858001709, \t Total Dis Loss : 3.0814571800874546e-05\n",
      "Steps : 164800, \t Total Gen Loss : 32.01701354980469, \t Total Dis Loss : 1.295100537390681e-05\n",
      "Steps : 164900, \t Total Gen Loss : 34.15169143676758, \t Total Dis Loss : 5.9804311604239047e-05\n",
      "Steps : 165000, \t Total Gen Loss : 28.35948944091797, \t Total Dis Loss : 2.938863508461509e-05\n",
      "Steps : 165100, \t Total Gen Loss : 30.423051834106445, \t Total Dis Loss : 2.077254612231627e-05\n",
      "Steps : 165200, \t Total Gen Loss : 31.408267974853516, \t Total Dis Loss : 2.0092655176995322e-05\n",
      "Steps : 165300, \t Total Gen Loss : 26.863525390625, \t Total Dis Loss : 0.000518952845595777\n",
      "Steps : 165400, \t Total Gen Loss : 25.91991424560547, \t Total Dis Loss : 6.857278640381992e-05\n",
      "Steps : 165500, \t Total Gen Loss : 29.72022247314453, \t Total Dis Loss : 2.5842167815426365e-05\n",
      "Steps : 165600, \t Total Gen Loss : 28.80031967163086, \t Total Dis Loss : 2.2901858756085858e-05\n",
      "Steps : 165700, \t Total Gen Loss : 27.88772964477539, \t Total Dis Loss : 1.4409166396944784e-05\n",
      "Steps : 165800, \t Total Gen Loss : 31.421186447143555, \t Total Dis Loss : 2.817940003296826e-05\n",
      "Steps : 165900, \t Total Gen Loss : 27.930076599121094, \t Total Dis Loss : 0.00010984077380271628\n",
      "Steps : 166000, \t Total Gen Loss : 26.41870880126953, \t Total Dis Loss : 8.617037383373827e-05\n",
      "Steps : 166100, \t Total Gen Loss : 26.78462028503418, \t Total Dis Loss : 2.575695725681726e-05\n",
      "Steps : 166200, \t Total Gen Loss : 27.040904998779297, \t Total Dis Loss : 0.0004628016904462129\n",
      "Steps : 166300, \t Total Gen Loss : 26.882179260253906, \t Total Dis Loss : 0.00012867503392044455\n",
      "Steps : 166400, \t Total Gen Loss : 27.9897403717041, \t Total Dis Loss : 7.39880561013706e-05\n",
      "Steps : 166500, \t Total Gen Loss : 29.73149871826172, \t Total Dis Loss : 2.2379997972166166e-05\n",
      "Steps : 166600, \t Total Gen Loss : 28.33381462097168, \t Total Dis Loss : 6.879689317429438e-05\n",
      "Steps : 166700, \t Total Gen Loss : 28.80869483947754, \t Total Dis Loss : 4.253277074894868e-05\n",
      "Steps : 166800, \t Total Gen Loss : 26.55954933166504, \t Total Dis Loss : 2.2984217139310203e-05\n",
      "Steps : 166900, \t Total Gen Loss : 29.667781829833984, \t Total Dis Loss : 3.900014780811034e-05\n",
      "Steps : 167000, \t Total Gen Loss : 30.785552978515625, \t Total Dis Loss : 0.0008445756393484771\n",
      "Steps : 167100, \t Total Gen Loss : 28.98965072631836, \t Total Dis Loss : 3.269847002229653e-05\n",
      "Steps : 167200, \t Total Gen Loss : 27.326095581054688, \t Total Dis Loss : 0.00010197128722211346\n",
      "Steps : 167300, \t Total Gen Loss : 26.52505111694336, \t Total Dis Loss : 3.2556290534557775e-05\n",
      "Steps : 167400, \t Total Gen Loss : 29.594024658203125, \t Total Dis Loss : 3.741450927918777e-05\n",
      "Steps : 167500, \t Total Gen Loss : 27.361804962158203, \t Total Dis Loss : 4.677427205024287e-05\n",
      "Steps : 167600, \t Total Gen Loss : 27.351232528686523, \t Total Dis Loss : 2.7821490220958367e-05\n",
      "Steps : 167700, \t Total Gen Loss : 28.655900955200195, \t Total Dis Loss : 2.976966425194405e-05\n",
      "Steps : 167800, \t Total Gen Loss : 26.671222686767578, \t Total Dis Loss : 2.9570432161563076e-05\n",
      "Steps : 167900, \t Total Gen Loss : 29.64682388305664, \t Total Dis Loss : 2.0169773051748052e-05\n",
      "Steps : 168000, \t Total Gen Loss : 30.436311721801758, \t Total Dis Loss : 1.5424657249241136e-05\n",
      "Steps : 168100, \t Total Gen Loss : 28.056554794311523, \t Total Dis Loss : 1.4630322766606696e-05\n",
      "Steps : 168200, \t Total Gen Loss : 25.400291442871094, \t Total Dis Loss : 1.2871376384282485e-05\n",
      "Steps : 168300, \t Total Gen Loss : 27.72067642211914, \t Total Dis Loss : 3.077321161981672e-05\n",
      "Steps : 168400, \t Total Gen Loss : 27.386978149414062, \t Total Dis Loss : 3.430958531680517e-05\n",
      "Steps : 168500, \t Total Gen Loss : 28.95748519897461, \t Total Dis Loss : 1.103587510442594e-05\n",
      "Steps : 168600, \t Total Gen Loss : 25.291414260864258, \t Total Dis Loss : 1.599637107574381e-05\n",
      "Steps : 168700, \t Total Gen Loss : 25.908702850341797, \t Total Dis Loss : 8.095421435427852e-06\n",
      "Time for epoch 30 is 79.38470244407654 sec\n",
      "Steps : 168800, \t Total Gen Loss : 27.087459564208984, \t Total Dis Loss : 1.169925781141501e-05\n",
      "Steps : 168900, \t Total Gen Loss : 26.19866943359375, \t Total Dis Loss : 1.2478566532081459e-05\n",
      "Steps : 169000, \t Total Gen Loss : 31.573074340820312, \t Total Dis Loss : 1.0000969268730842e-05\n",
      "Steps : 169100, \t Total Gen Loss : 28.46961212158203, \t Total Dis Loss : 9.401008355780505e-06\n",
      "Steps : 169200, \t Total Gen Loss : 23.556926727294922, \t Total Dis Loss : 0.00011458673543529585\n",
      "Steps : 169300, \t Total Gen Loss : 26.64727783203125, \t Total Dis Loss : 2.3625743779120967e-05\n",
      "Steps : 169400, \t Total Gen Loss : 27.64153289794922, \t Total Dis Loss : 4.305914990254678e-05\n",
      "Steps : 169500, \t Total Gen Loss : 26.682727813720703, \t Total Dis Loss : 7.947665835672524e-06\n",
      "Steps : 169600, \t Total Gen Loss : 28.44057846069336, \t Total Dis Loss : 1.1476176041469444e-05\n",
      "Steps : 169700, \t Total Gen Loss : 30.808443069458008, \t Total Dis Loss : 7.474233825632837e-06\n",
      "Steps : 169800, \t Total Gen Loss : 28.491790771484375, \t Total Dis Loss : 8.469874956062995e-06\n",
      "Steps : 169900, \t Total Gen Loss : 28.691200256347656, \t Total Dis Loss : 4.150182121520629e-06\n",
      "Steps : 170000, \t Total Gen Loss : 30.234413146972656, \t Total Dis Loss : 1.723690002108924e-05\n",
      "Steps : 170100, \t Total Gen Loss : 27.814226150512695, \t Total Dis Loss : 8.878118933353107e-06\n",
      "Steps : 170200, \t Total Gen Loss : 28.43850326538086, \t Total Dis Loss : 6.433147063944489e-05\n",
      "Steps : 170300, \t Total Gen Loss : 27.764850616455078, \t Total Dis Loss : 6.313067387964111e-06\n",
      "Steps : 170400, \t Total Gen Loss : 29.32086944580078, \t Total Dis Loss : 1.7926106011145748e-05\n",
      "Steps : 170500, \t Total Gen Loss : 30.69066619873047, \t Total Dis Loss : 1.9653545678011142e-05\n",
      "Steps : 170600, \t Total Gen Loss : 27.298486709594727, \t Total Dis Loss : 2.550146382418461e-05\n",
      "Steps : 170700, \t Total Gen Loss : 29.610721588134766, \t Total Dis Loss : 3.920208655472379e-06\n",
      "Steps : 170800, \t Total Gen Loss : 30.7830810546875, \t Total Dis Loss : 1.0269540325680282e-05\n",
      "Steps : 170900, \t Total Gen Loss : 29.95842742919922, \t Total Dis Loss : 8.779740710451733e-06\n",
      "Steps : 171000, \t Total Gen Loss : 28.135974884033203, \t Total Dis Loss : 1.1007001376128756e-05\n",
      "Steps : 171100, \t Total Gen Loss : 27.154266357421875, \t Total Dis Loss : 0.00016167096327990294\n",
      "Steps : 171200, \t Total Gen Loss : 28.40339469909668, \t Total Dis Loss : 0.00023391701688524336\n",
      "Steps : 171300, \t Total Gen Loss : 31.0506591796875, \t Total Dis Loss : 0.0002848186995834112\n",
      "Steps : 171400, \t Total Gen Loss : 31.621370315551758, \t Total Dis Loss : 6.172885332489386e-05\n",
      "Steps : 171500, \t Total Gen Loss : 32.459449768066406, \t Total Dis Loss : 5.384193173085805e-06\n",
      "Steps : 171600, \t Total Gen Loss : 30.538894653320312, \t Total Dis Loss : 6.40603102510795e-05\n",
      "Steps : 171700, \t Total Gen Loss : 29.65869140625, \t Total Dis Loss : 9.981585026253015e-05\n",
      "Steps : 171800, \t Total Gen Loss : 32.516685485839844, \t Total Dis Loss : 0.00011041147809009999\n",
      "Steps : 171900, \t Total Gen Loss : 31.313568115234375, \t Total Dis Loss : 7.128187280613929e-05\n",
      "Steps : 172000, \t Total Gen Loss : 31.520822525024414, \t Total Dis Loss : 2.3993865397642367e-05\n",
      "Steps : 172100, \t Total Gen Loss : 30.897838592529297, \t Total Dis Loss : 0.0004250815836712718\n",
      "Steps : 172200, \t Total Gen Loss : 31.38307762145996, \t Total Dis Loss : 3.9718284824630246e-05\n",
      "Steps : 172300, \t Total Gen Loss : 29.883092880249023, \t Total Dis Loss : 3.264216138632037e-05\n",
      "Steps : 172400, \t Total Gen Loss : 32.20668029785156, \t Total Dis Loss : 2.2194120901986025e-05\n",
      "Steps : 172500, \t Total Gen Loss : 34.8775634765625, \t Total Dis Loss : 0.0011218760628253222\n",
      "Steps : 172600, \t Total Gen Loss : 31.403676986694336, \t Total Dis Loss : 0.040675751864910126\n",
      "Steps : 172700, \t Total Gen Loss : 32.053016662597656, \t Total Dis Loss : 8.190576045308262e-05\n",
      "Steps : 172800, \t Total Gen Loss : 30.488977432250977, \t Total Dis Loss : 9.891823719954118e-05\n",
      "Steps : 172900, \t Total Gen Loss : 30.202531814575195, \t Total Dis Loss : 4.545722185866907e-05\n",
      "Steps : 173000, \t Total Gen Loss : 31.141557693481445, \t Total Dis Loss : 4.569344309857115e-05\n",
      "Steps : 173100, \t Total Gen Loss : 34.50547790527344, \t Total Dis Loss : 2.7794218112831004e-05\n",
      "Steps : 173200, \t Total Gen Loss : 30.538240432739258, \t Total Dis Loss : 9.18273872230202e-06\n",
      "Steps : 173300, \t Total Gen Loss : 31.585216522216797, \t Total Dis Loss : 3.433392703300342e-05\n",
      "Steps : 173400, \t Total Gen Loss : 31.569456100463867, \t Total Dis Loss : 2.810067053360399e-05\n",
      "Steps : 173500, \t Total Gen Loss : 31.15459442138672, \t Total Dis Loss : 0.0032522957772016525\n",
      "Steps : 173600, \t Total Gen Loss : 29.449199676513672, \t Total Dis Loss : 0.00017795240273699164\n",
      "Steps : 173700, \t Total Gen Loss : 32.88208770751953, \t Total Dis Loss : 4.814599014935084e-05\n",
      "Steps : 173800, \t Total Gen Loss : 28.453563690185547, \t Total Dis Loss : 0.00010908605327131227\n",
      "Steps : 173900, \t Total Gen Loss : 31.87301254272461, \t Total Dis Loss : 7.381624163826928e-05\n",
      "Steps : 174000, \t Total Gen Loss : 28.679595947265625, \t Total Dis Loss : 1.715969483484514e-05\n",
      "Steps : 174100, \t Total Gen Loss : 25.362655639648438, \t Total Dis Loss : 9.094943379750475e-05\n",
      "Steps : 174200, \t Total Gen Loss : 28.678531646728516, \t Total Dis Loss : 3.980055043939501e-05\n",
      "Steps : 174300, \t Total Gen Loss : 31.066932678222656, \t Total Dis Loss : 1.3814047633786686e-05\n",
      "Time for epoch 31 is 77.58775925636292 sec\n",
      "Steps : 174400, \t Total Gen Loss : 29.985736846923828, \t Total Dis Loss : 4.185880970908329e-05\n",
      "Steps : 174500, \t Total Gen Loss : 26.891258239746094, \t Total Dis Loss : 3.738547457032837e-05\n",
      "Steps : 174600, \t Total Gen Loss : 28.248741149902344, \t Total Dis Loss : 1.1145109965582378e-05\n",
      "Steps : 174700, \t Total Gen Loss : 27.578601837158203, \t Total Dis Loss : 0.0002574106038082391\n",
      "Steps : 174800, \t Total Gen Loss : 30.61539649963379, \t Total Dis Loss : 0.00024642315111123025\n",
      "Steps : 174900, \t Total Gen Loss : 25.75267219543457, \t Total Dis Loss : 0.000515163061209023\n",
      "Steps : 175000, \t Total Gen Loss : 29.892555236816406, \t Total Dis Loss : 3.720830864040181e-05\n",
      "Steps : 175100, \t Total Gen Loss : 29.39525032043457, \t Total Dis Loss : 5.6380281421297695e-06\n",
      "Steps : 175200, \t Total Gen Loss : 31.0552978515625, \t Total Dis Loss : 2.5928484319592826e-05\n",
      "Steps : 175300, \t Total Gen Loss : 25.389083862304688, \t Total Dis Loss : 0.00010101949010277167\n",
      "Steps : 175400, \t Total Gen Loss : 27.766857147216797, \t Total Dis Loss : 3.649450809461996e-05\n",
      "Steps : 175500, \t Total Gen Loss : 27.648733139038086, \t Total Dis Loss : 0.0004223730938974768\n",
      "Steps : 175600, \t Total Gen Loss : 21.90810203552246, \t Total Dis Loss : 0.040231551975011826\n",
      "Steps : 175700, \t Total Gen Loss : 27.478866577148438, \t Total Dis Loss : 6.057144855731167e-05\n",
      "Steps : 175800, \t Total Gen Loss : 28.83972930908203, \t Total Dis Loss : 1.8692686353460886e-05\n",
      "Steps : 175900, \t Total Gen Loss : 28.915760040283203, \t Total Dis Loss : 1.580906246090308e-05\n",
      "Steps : 176000, \t Total Gen Loss : 28.042316436767578, \t Total Dis Loss : 1.9456430891295895e-05\n",
      "Steps : 176100, \t Total Gen Loss : 28.51411247253418, \t Total Dis Loss : 5.0621689297258854e-05\n",
      "Steps : 176200, \t Total Gen Loss : 29.048519134521484, \t Total Dis Loss : 1.1430749509599991e-05\n",
      "Steps : 176300, \t Total Gen Loss : 28.080995559692383, \t Total Dis Loss : 5.8261262893211097e-05\n",
      "Steps : 176400, \t Total Gen Loss : 27.526697158813477, \t Total Dis Loss : 0.00012365743168629706\n",
      "Steps : 176500, \t Total Gen Loss : 25.33437728881836, \t Total Dis Loss : 0.00014165675383992493\n",
      "Steps : 176600, \t Total Gen Loss : 28.102813720703125, \t Total Dis Loss : 2.5123888917732984e-05\n",
      "Steps : 176700, \t Total Gen Loss : 29.582611083984375, \t Total Dis Loss : 0.00012112392869312316\n",
      "Steps : 176800, \t Total Gen Loss : 28.44468879699707, \t Total Dis Loss : 6.45280524622649e-05\n",
      "Steps : 176900, \t Total Gen Loss : 28.159025192260742, \t Total Dis Loss : 6.54777031741105e-05\n",
      "Steps : 177000, \t Total Gen Loss : 28.256439208984375, \t Total Dis Loss : 0.0018753308104351163\n",
      "Steps : 177100, \t Total Gen Loss : 30.497426986694336, \t Total Dis Loss : 7.798323349561542e-05\n",
      "Steps : 177200, \t Total Gen Loss : 26.97406768798828, \t Total Dis Loss : 1.8175956938648596e-05\n",
      "Steps : 177300, \t Total Gen Loss : 30.467876434326172, \t Total Dis Loss : 1.3932025467511266e-05\n",
      "Steps : 177400, \t Total Gen Loss : 28.338119506835938, \t Total Dis Loss : 3.83307269657962e-05\n",
      "Steps : 177500, \t Total Gen Loss : 27.144100189208984, \t Total Dis Loss : 3.6881356209050864e-05\n",
      "Steps : 177600, \t Total Gen Loss : 27.050273895263672, \t Total Dis Loss : 1.623038770048879e-05\n",
      "Steps : 177700, \t Total Gen Loss : 23.77794647216797, \t Total Dis Loss : 0.0005087509634904563\n",
      "Steps : 177800, \t Total Gen Loss : 27.363828659057617, \t Total Dis Loss : 0.0012108416995033622\n",
      "Steps : 177900, \t Total Gen Loss : 25.411174774169922, \t Total Dis Loss : 0.0003155978047288954\n",
      "Steps : 178000, \t Total Gen Loss : 24.58977508544922, \t Total Dis Loss : 0.0003360688569955528\n",
      "Steps : 178100, \t Total Gen Loss : 27.35184097290039, \t Total Dis Loss : 0.00011521551641635597\n",
      "Steps : 178200, \t Total Gen Loss : 25.175228118896484, \t Total Dis Loss : 0.001015820656903088\n",
      "Steps : 178300, \t Total Gen Loss : 22.585594177246094, \t Total Dis Loss : 0.0003481188614387065\n",
      "Steps : 178400, \t Total Gen Loss : 27.474159240722656, \t Total Dis Loss : 0.00013228424359112978\n",
      "Steps : 178500, \t Total Gen Loss : 25.61806869506836, \t Total Dis Loss : 0.00019798496214207262\n",
      "Steps : 178600, \t Total Gen Loss : 26.0277156829834, \t Total Dis Loss : 7.540485239587724e-05\n",
      "Steps : 178700, \t Total Gen Loss : 25.243572235107422, \t Total Dis Loss : 0.00010263674630550668\n",
      "Steps : 178800, \t Total Gen Loss : 26.32490348815918, \t Total Dis Loss : 0.00011162068403791636\n",
      "Steps : 178900, \t Total Gen Loss : 22.612756729125977, \t Total Dis Loss : 0.0012589626712724566\n",
      "Steps : 179000, \t Total Gen Loss : 24.557838439941406, \t Total Dis Loss : 0.0002067809400614351\n",
      "Steps : 179100, \t Total Gen Loss : 22.996906280517578, \t Total Dis Loss : 0.0008096689125522971\n",
      "Steps : 179200, \t Total Gen Loss : 25.115217208862305, \t Total Dis Loss : 0.00010913062578765675\n",
      "Steps : 179300, \t Total Gen Loss : 27.930105209350586, \t Total Dis Loss : 6.565438525285572e-05\n",
      "Steps : 179400, \t Total Gen Loss : 29.21854591369629, \t Total Dis Loss : 4.001150227850303e-05\n",
      "Steps : 179500, \t Total Gen Loss : 27.490507125854492, \t Total Dis Loss : 3.210221620975062e-05\n",
      "Steps : 179600, \t Total Gen Loss : 25.96635627746582, \t Total Dis Loss : 4.77237299492117e-05\n",
      "Steps : 179700, \t Total Gen Loss : 28.261180877685547, \t Total Dis Loss : 3.2372568966820836e-05\n",
      "Steps : 179800, \t Total Gen Loss : 31.15192413330078, \t Total Dis Loss : 2.8652606488321908e-05\n",
      "Steps : 179900, \t Total Gen Loss : 28.702686309814453, \t Total Dis Loss : 3.430009383009747e-05\n",
      "Steps : 180000, \t Total Gen Loss : 27.57866668701172, \t Total Dis Loss : 2.165362639061641e-05\n",
      "Time for epoch 32 is 79.26268935203552 sec\n",
      "Steps : 180100, \t Total Gen Loss : 26.73448944091797, \t Total Dis Loss : 1.6140535080921836e-05\n",
      "Steps : 180200, \t Total Gen Loss : 26.452112197875977, \t Total Dis Loss : 1.674655868555419e-05\n",
      "Steps : 180300, \t Total Gen Loss : 25.992916107177734, \t Total Dis Loss : 3.537828160915524e-05\n",
      "Steps : 180400, \t Total Gen Loss : 26.571338653564453, \t Total Dis Loss : 0.0006248543504625559\n",
      "Steps : 180500, \t Total Gen Loss : 26.331100463867188, \t Total Dis Loss : 1.86361630767351e-05\n",
      "Steps : 180600, \t Total Gen Loss : 26.295419692993164, \t Total Dis Loss : 1.5383939171442762e-05\n",
      "Steps : 180700, \t Total Gen Loss : 26.67932891845703, \t Total Dis Loss : 1.6327567209373228e-05\n",
      "Steps : 180800, \t Total Gen Loss : 25.382665634155273, \t Total Dis Loss : 0.0001427094975952059\n",
      "Steps : 180900, \t Total Gen Loss : 27.564292907714844, \t Total Dis Loss : 6.404914074664703e-06\n",
      "Steps : 181000, \t Total Gen Loss : 26.227718353271484, \t Total Dis Loss : 0.0003176280588377267\n",
      "Steps : 181100, \t Total Gen Loss : 25.520652770996094, \t Total Dis Loss : 0.00035960416425950825\n",
      "Steps : 181200, \t Total Gen Loss : 32.44157791137695, \t Total Dis Loss : 0.0003507546498440206\n",
      "Steps : 181300, \t Total Gen Loss : 31.19631004333496, \t Total Dis Loss : 0.0003286785213276744\n",
      "Steps : 181400, \t Total Gen Loss : 35.0569953918457, \t Total Dis Loss : 5.787933696410619e-05\n",
      "Steps : 181500, \t Total Gen Loss : 31.355302810668945, \t Total Dis Loss : 2.1336745703592896e-05\n",
      "Steps : 181600, \t Total Gen Loss : 30.8167781829834, \t Total Dis Loss : 1.5029319911263883e-05\n",
      "Steps : 181700, \t Total Gen Loss : 30.820838928222656, \t Total Dis Loss : 1.739306389936246e-05\n",
      "Steps : 181800, \t Total Gen Loss : 35.16762924194336, \t Total Dis Loss : 9.543074156681541e-06\n",
      "Steps : 181900, \t Total Gen Loss : 29.011642456054688, \t Total Dis Loss : 0.00014276866568252444\n",
      "Steps : 182000, \t Total Gen Loss : 31.98706817626953, \t Total Dis Loss : 2.810600199154578e-05\n",
      "Steps : 182100, \t Total Gen Loss : 27.138870239257812, \t Total Dis Loss : 0.0001672207290539518\n",
      "Steps : 182200, \t Total Gen Loss : 29.514118194580078, \t Total Dis Loss : 7.586686115246266e-05\n",
      "Steps : 182300, \t Total Gen Loss : 30.571020126342773, \t Total Dis Loss : 0.0001698140986263752\n",
      "Steps : 182400, \t Total Gen Loss : 24.453353881835938, \t Total Dis Loss : 0.0011345120146870613\n",
      "Steps : 182500, \t Total Gen Loss : 26.278182983398438, \t Total Dis Loss : 0.00033083578455261886\n",
      "Steps : 182600, \t Total Gen Loss : 27.208786010742188, \t Total Dis Loss : 0.0001844952639658004\n",
      "Steps : 182700, \t Total Gen Loss : 26.414241790771484, \t Total Dis Loss : 0.00013766570191364735\n",
      "Steps : 182800, \t Total Gen Loss : 25.854440689086914, \t Total Dis Loss : 0.25970324873924255\n",
      "Steps : 182900, \t Total Gen Loss : 29.614559173583984, \t Total Dis Loss : 2.3379152480629273e-05\n",
      "Steps : 183000, \t Total Gen Loss : 26.219341278076172, \t Total Dis Loss : 4.787178477272391e-05\n",
      "Steps : 183100, \t Total Gen Loss : 24.875974655151367, \t Total Dis Loss : 6.230630970094353e-05\n",
      "Steps : 183200, \t Total Gen Loss : 25.791118621826172, \t Total Dis Loss : 3.30877628584858e-05\n",
      "Steps : 183300, \t Total Gen Loss : 27.67556381225586, \t Total Dis Loss : 8.323771908180788e-05\n",
      "Steps : 183400, \t Total Gen Loss : 29.697246551513672, \t Total Dis Loss : 8.23484078864567e-05\n",
      "Steps : 183500, \t Total Gen Loss : 28.070470809936523, \t Total Dis Loss : 0.00023140001576393843\n",
      "Steps : 183600, \t Total Gen Loss : 27.8149471282959, \t Total Dis Loss : 4.834210631088354e-05\n",
      "Steps : 183700, \t Total Gen Loss : 29.097576141357422, \t Total Dis Loss : 4.255215390003286e-05\n",
      "Steps : 183800, \t Total Gen Loss : 27.509241104125977, \t Total Dis Loss : 4.763168908539228e-05\n",
      "Steps : 183900, \t Total Gen Loss : 29.896636962890625, \t Total Dis Loss : 2.3745666112517938e-05\n",
      "Steps : 184000, \t Total Gen Loss : 28.038928985595703, \t Total Dis Loss : 2.6366968086222187e-05\n",
      "Steps : 184100, \t Total Gen Loss : 29.338642120361328, \t Total Dis Loss : 3.367151293787174e-05\n",
      "Steps : 184200, \t Total Gen Loss : 28.478055953979492, \t Total Dis Loss : 5.2302599215181544e-05\n",
      "Steps : 184300, \t Total Gen Loss : 29.02467155456543, \t Total Dis Loss : 2.6610729037201963e-05\n",
      "Steps : 184400, \t Total Gen Loss : 26.25235939025879, \t Total Dis Loss : 6.433487578760833e-05\n",
      "Steps : 184500, \t Total Gen Loss : 24.61750030517578, \t Total Dis Loss : 0.00020103147835470736\n",
      "Steps : 184600, \t Total Gen Loss : 23.86526107788086, \t Total Dis Loss : 0.00010103071690537035\n",
      "Steps : 184700, \t Total Gen Loss : 25.999483108520508, \t Total Dis Loss : 0.000292494980385527\n",
      "Steps : 184800, \t Total Gen Loss : 24.42746353149414, \t Total Dis Loss : 0.00022624293342232704\n",
      "Steps : 184900, \t Total Gen Loss : 23.413484573364258, \t Total Dis Loss : 0.00013237203529570252\n",
      "Steps : 185000, \t Total Gen Loss : 28.369836807250977, \t Total Dis Loss : 7.872673450037837e-05\n",
      "Steps : 185100, \t Total Gen Loss : 24.450653076171875, \t Total Dis Loss : 0.00017252142424695194\n",
      "Steps : 185200, \t Total Gen Loss : 26.762758255004883, \t Total Dis Loss : 0.0001059845308191143\n",
      "Steps : 185300, \t Total Gen Loss : 26.703155517578125, \t Total Dis Loss : 7.770566298859194e-05\n",
      "Steps : 185400, \t Total Gen Loss : 24.172561645507812, \t Total Dis Loss : 8.881206304067746e-05\n",
      "Steps : 185500, \t Total Gen Loss : 27.8181095123291, \t Total Dis Loss : 0.00013215042417868972\n",
      "Steps : 185600, \t Total Gen Loss : 26.74407196044922, \t Total Dis Loss : 0.00014183996245265007\n",
      "Time for epoch 33 is 78.32731461524963 sec\n",
      "Steps : 185700, \t Total Gen Loss : 30.711843490600586, \t Total Dis Loss : 8.714305295143276e-06\n",
      "Steps : 185800, \t Total Gen Loss : 30.280672073364258, \t Total Dis Loss : 6.690973532386124e-05\n",
      "Steps : 185900, \t Total Gen Loss : 28.656721115112305, \t Total Dis Loss : 7.164564885897562e-05\n",
      "Steps : 186000, \t Total Gen Loss : 29.29989242553711, \t Total Dis Loss : 2.780253089440521e-05\n",
      "Steps : 186100, \t Total Gen Loss : 28.259082794189453, \t Total Dis Loss : 0.00011010899470420554\n",
      "Steps : 186200, \t Total Gen Loss : 24.98802947998047, \t Total Dis Loss : 0.000395167269743979\n",
      "Steps : 186300, \t Total Gen Loss : 27.276172637939453, \t Total Dis Loss : 5.093187064630911e-05\n",
      "Steps : 186400, \t Total Gen Loss : 27.516056060791016, \t Total Dis Loss : 6.383829895639792e-05\n",
      "Steps : 186500, \t Total Gen Loss : 29.90265655517578, \t Total Dis Loss : 8.370463183382526e-05\n",
      "Steps : 186600, \t Total Gen Loss : 24.75380516052246, \t Total Dis Loss : 0.0011219795560464263\n",
      "Steps : 186700, \t Total Gen Loss : 22.749801635742188, \t Total Dis Loss : 0.00018074995023198426\n",
      "Steps : 186800, \t Total Gen Loss : 26.632831573486328, \t Total Dis Loss : 0.00014383882808033377\n",
      "Steps : 186900, \t Total Gen Loss : 25.34865379333496, \t Total Dis Loss : 0.0001712909433990717\n",
      "Steps : 187000, \t Total Gen Loss : 24.92936897277832, \t Total Dis Loss : 0.00045751448487862945\n",
      "Steps : 187100, \t Total Gen Loss : 29.22977638244629, \t Total Dis Loss : 0.00030444469302892685\n",
      "Steps : 187200, \t Total Gen Loss : 29.113636016845703, \t Total Dis Loss : 0.00011248531518504024\n",
      "Steps : 187300, \t Total Gen Loss : 28.068361282348633, \t Total Dis Loss : 0.0002055536606349051\n",
      "Steps : 187400, \t Total Gen Loss : 23.67402458190918, \t Total Dis Loss : 0.00024509590002708137\n",
      "Steps : 187500, \t Total Gen Loss : 26.908733367919922, \t Total Dis Loss : 0.0001914449385367334\n",
      "Steps : 187600, \t Total Gen Loss : 24.964824676513672, \t Total Dis Loss : 0.000424661353463307\n",
      "Steps : 187700, \t Total Gen Loss : 30.812149047851562, \t Total Dis Loss : 4.860723129240796e-05\n",
      "Steps : 187800, \t Total Gen Loss : 26.65638542175293, \t Total Dis Loss : 0.0001491657894803211\n",
      "Steps : 187900, \t Total Gen Loss : 26.241867065429688, \t Total Dis Loss : 3.342143463669345e-05\n",
      "Steps : 188000, \t Total Gen Loss : 26.688247680664062, \t Total Dis Loss : 0.0003845352912321687\n",
      "Steps : 188100, \t Total Gen Loss : 23.424270629882812, \t Total Dis Loss : 9.665383549872786e-05\n",
      "Steps : 188200, \t Total Gen Loss : 24.157373428344727, \t Total Dis Loss : 8.616331615485251e-05\n",
      "Steps : 188300, \t Total Gen Loss : 24.840682983398438, \t Total Dis Loss : 0.0003107251541223377\n",
      "Steps : 188400, \t Total Gen Loss : 28.350772857666016, \t Total Dis Loss : 5.655349013977684e-05\n",
      "Steps : 188500, \t Total Gen Loss : 27.514610290527344, \t Total Dis Loss : 0.0004913354059681296\n",
      "Steps : 188600, \t Total Gen Loss : 30.349912643432617, \t Total Dis Loss : 2.4310360458912328e-05\n",
      "Steps : 188700, \t Total Gen Loss : 29.81705665588379, \t Total Dis Loss : 5.821416198159568e-05\n",
      "Steps : 188800, \t Total Gen Loss : 25.94359588623047, \t Total Dis Loss : 9.763691195985302e-05\n",
      "Steps : 188900, \t Total Gen Loss : 26.114849090576172, \t Total Dis Loss : 9.717533248476684e-05\n",
      "Steps : 189000, \t Total Gen Loss : 25.470577239990234, \t Total Dis Loss : 0.12070897966623306\n",
      "Steps : 189100, \t Total Gen Loss : 27.223491668701172, \t Total Dis Loss : 7.003052451182157e-05\n",
      "Steps : 189200, \t Total Gen Loss : 23.612499237060547, \t Total Dis Loss : 0.0003954888670705259\n",
      "Steps : 189300, \t Total Gen Loss : 23.5062313079834, \t Total Dis Loss : 0.0002618999860715121\n",
      "Steps : 189400, \t Total Gen Loss : 25.902057647705078, \t Total Dis Loss : 5.7277844462078065e-05\n",
      "Steps : 189500, \t Total Gen Loss : 24.48063850402832, \t Total Dis Loss : 8.876477659214288e-05\n",
      "Steps : 189600, \t Total Gen Loss : 28.433582305908203, \t Total Dis Loss : 4.2706764361355454e-05\n",
      "Steps : 189700, \t Total Gen Loss : 29.575550079345703, \t Total Dis Loss : 0.0008442949620075524\n",
      "Steps : 189800, \t Total Gen Loss : 28.246057510375977, \t Total Dis Loss : 0.00030873037758283317\n",
      "Steps : 189900, \t Total Gen Loss : 25.53609848022461, \t Total Dis Loss : 0.000921808066777885\n",
      "Steps : 190000, \t Total Gen Loss : 23.492576599121094, \t Total Dis Loss : 0.0015793762868270278\n",
      "Steps : 190100, \t Total Gen Loss : 20.41551971435547, \t Total Dis Loss : 0.17835907638072968\n",
      "Steps : 190200, \t Total Gen Loss : 26.015422821044922, \t Total Dis Loss : 0.00040037595317699015\n",
      "Steps : 190300, \t Total Gen Loss : 24.99504852294922, \t Total Dis Loss : 5.024315032642335e-05\n",
      "Steps : 190400, \t Total Gen Loss : 29.48341941833496, \t Total Dis Loss : 5.5232194426935166e-05\n",
      "Steps : 190500, \t Total Gen Loss : 25.205711364746094, \t Total Dis Loss : 0.0001570207969052717\n",
      "Steps : 190600, \t Total Gen Loss : 27.796070098876953, \t Total Dis Loss : 0.00013044466322753578\n",
      "Steps : 190700, \t Total Gen Loss : 25.970237731933594, \t Total Dis Loss : 0.0007466992828994989\n",
      "Steps : 190800, \t Total Gen Loss : 26.940155029296875, \t Total Dis Loss : 0.00027113090618513525\n",
      "Steps : 190900, \t Total Gen Loss : 23.068696975708008, \t Total Dis Loss : 0.0001291809749091044\n",
      "Steps : 191000, \t Total Gen Loss : 26.452980041503906, \t Total Dis Loss : 7.809132512193173e-05\n",
      "Steps : 191100, \t Total Gen Loss : 23.340164184570312, \t Total Dis Loss : 0.00011008290312020108\n",
      "Steps : 191200, \t Total Gen Loss : 25.25696563720703, \t Total Dis Loss : 4.85462587676011e-05\n",
      "Time for epoch 34 is 75.51782941818237 sec\n",
      "Steps : 191300, \t Total Gen Loss : 23.68696403503418, \t Total Dis Loss : 3.8195699744392186e-05\n",
      "Steps : 191400, \t Total Gen Loss : 28.845394134521484, \t Total Dis Loss : 7.417644519591704e-05\n",
      "Steps : 191500, \t Total Gen Loss : 26.976646423339844, \t Total Dis Loss : 6.899746222188696e-05\n",
      "Steps : 191600, \t Total Gen Loss : 26.911365509033203, \t Total Dis Loss : 7.908952102297917e-05\n",
      "Steps : 191700, \t Total Gen Loss : 27.498699188232422, \t Total Dis Loss : 7.507427653763443e-05\n",
      "Steps : 191800, \t Total Gen Loss : 26.88787269592285, \t Total Dis Loss : 2.6072411856148392e-05\n",
      "Steps : 191900, \t Total Gen Loss : 25.436058044433594, \t Total Dis Loss : 5.424441405921243e-05\n",
      "Steps : 192000, \t Total Gen Loss : 25.642839431762695, \t Total Dis Loss : 4.613404962583445e-05\n",
      "Steps : 192100, \t Total Gen Loss : 25.078636169433594, \t Total Dis Loss : 3.065973942284472e-05\n",
      "Steps : 192200, \t Total Gen Loss : 28.03945541381836, \t Total Dis Loss : 4.17726332671009e-05\n",
      "Steps : 192300, \t Total Gen Loss : 26.53946304321289, \t Total Dis Loss : 2.740053787420038e-05\n",
      "Steps : 192400, \t Total Gen Loss : 26.971736907958984, \t Total Dis Loss : 2.6217790946247987e-05\n",
      "Steps : 192500, \t Total Gen Loss : 30.414514541625977, \t Total Dis Loss : 1.150799471361097e-05\n",
      "Steps : 192600, \t Total Gen Loss : 28.568387985229492, \t Total Dis Loss : 2.0099878383916803e-05\n",
      "Steps : 192700, \t Total Gen Loss : 26.483285903930664, \t Total Dis Loss : 1.602846350579057e-05\n",
      "Steps : 192800, \t Total Gen Loss : 24.786182403564453, \t Total Dis Loss : 1.596719812368974e-05\n",
      "Steps : 192900, \t Total Gen Loss : 27.492507934570312, \t Total Dis Loss : 1.9721330318134278e-05\n",
      "Steps : 193000, \t Total Gen Loss : 28.259918212890625, \t Total Dis Loss : 1.8137747247237712e-05\n",
      "Steps : 193100, \t Total Gen Loss : 25.901691436767578, \t Total Dis Loss : 1.261243232875131e-05\n",
      "Steps : 193200, \t Total Gen Loss : 26.416990280151367, \t Total Dis Loss : 1.5055396033858415e-05\n",
      "Steps : 193300, \t Total Gen Loss : 27.523143768310547, \t Total Dis Loss : 0.0011324819643050432\n",
      "Steps : 193400, \t Total Gen Loss : 20.191944122314453, \t Total Dis Loss : 0.007814090698957443\n",
      "Steps : 193500, \t Total Gen Loss : 30.154239654541016, \t Total Dis Loss : 7.034564623609185e-05\n",
      "Steps : 193600, \t Total Gen Loss : 27.83041000366211, \t Total Dis Loss : 5.958908150205389e-05\n",
      "Steps : 193700, \t Total Gen Loss : 30.72616195678711, \t Total Dis Loss : 3.5691716675501084e-06\n",
      "Steps : 193800, \t Total Gen Loss : 30.343164443969727, \t Total Dis Loss : 9.674156444816617e-07\n",
      "Steps : 193900, \t Total Gen Loss : 26.339921951293945, \t Total Dis Loss : 2.4345303245354444e-05\n",
      "Steps : 194000, \t Total Gen Loss : 30.74831771850586, \t Total Dis Loss : 1.103569502447499e-05\n",
      "Steps : 194100, \t Total Gen Loss : 27.86474609375, \t Total Dis Loss : 0.00042046638554893434\n",
      "Steps : 194200, \t Total Gen Loss : 29.881183624267578, \t Total Dis Loss : 3.707483483594842e-05\n",
      "Steps : 194300, \t Total Gen Loss : 30.326974868774414, \t Total Dis Loss : 2.4285924155265093e-05\n",
      "Steps : 194400, \t Total Gen Loss : 24.78478240966797, \t Total Dis Loss : 0.00612665107473731\n",
      "Steps : 194500, \t Total Gen Loss : 28.578718185424805, \t Total Dis Loss : 0.0004566836287267506\n",
      "Steps : 194600, \t Total Gen Loss : 31.053550720214844, \t Total Dis Loss : 0.00014045104035176337\n",
      "Steps : 194700, \t Total Gen Loss : 22.04380989074707, \t Total Dis Loss : 0.00010410459071863443\n",
      "Steps : 194800, \t Total Gen Loss : 26.46184539794922, \t Total Dis Loss : 0.00020746073278132826\n",
      "Steps : 194900, \t Total Gen Loss : 28.507896423339844, \t Total Dis Loss : 3.526645741658285e-05\n",
      "Steps : 195000, \t Total Gen Loss : 27.565399169921875, \t Total Dis Loss : 6.233473686734214e-05\n",
      "Steps : 195100, \t Total Gen Loss : 29.403629302978516, \t Total Dis Loss : 6.262859824346378e-05\n",
      "Steps : 195200, \t Total Gen Loss : 29.411378860473633, \t Total Dis Loss : 4.659447949961759e-05\n",
      "Steps : 195300, \t Total Gen Loss : 26.20349884033203, \t Total Dis Loss : 6.85747800162062e-05\n",
      "Steps : 195400, \t Total Gen Loss : 31.654327392578125, \t Total Dis Loss : 9.835966193350032e-05\n",
      "Steps : 195500, \t Total Gen Loss : 28.868453979492188, \t Total Dis Loss : 4.76864333904814e-05\n",
      "Steps : 195600, \t Total Gen Loss : 27.933490753173828, \t Total Dis Loss : 1.8713488316279836e-05\n",
      "Steps : 195700, \t Total Gen Loss : 28.231830596923828, \t Total Dis Loss : 1.8576334696263075e-05\n",
      "Steps : 195800, \t Total Gen Loss : 27.649398803710938, \t Total Dis Loss : 2.4753366233198904e-05\n",
      "Steps : 195900, \t Total Gen Loss : 26.956735610961914, \t Total Dis Loss : 3.3216616429854184e-05\n",
      "Steps : 196000, \t Total Gen Loss : 28.760150909423828, \t Total Dis Loss : 1.9164564946549945e-05\n",
      "Steps : 196100, \t Total Gen Loss : 27.239364624023438, \t Total Dis Loss : 4.1340466850670055e-05\n",
      "Steps : 196200, \t Total Gen Loss : 28.2989501953125, \t Total Dis Loss : 3.083396950387396e-05\n",
      "Steps : 196300, \t Total Gen Loss : 26.742441177368164, \t Total Dis Loss : 0.000284857873339206\n",
      "Steps : 196400, \t Total Gen Loss : 25.639511108398438, \t Total Dis Loss : 0.00013219643733464181\n",
      "Steps : 196500, \t Total Gen Loss : 29.8883113861084, \t Total Dis Loss : 9.901988960336894e-05\n",
      "Steps : 196600, \t Total Gen Loss : 25.013416290283203, \t Total Dis Loss : 4.960620208294131e-05\n",
      "Steps : 196700, \t Total Gen Loss : 27.27492904663086, \t Total Dis Loss : 7.532486506534042e-06\n",
      "Steps : 196800, \t Total Gen Loss : 24.72702980041504, \t Total Dis Loss : 0.0006962682236917317\n",
      "Time for epoch 35 is 74.74031519889832 sec\n",
      "Steps : 196900, \t Total Gen Loss : 30.289499282836914, \t Total Dis Loss : 0.0003828518674708903\n",
      "Steps : 197000, \t Total Gen Loss : 31.21076774597168, \t Total Dis Loss : 7.477322651538998e-05\n",
      "Steps : 197100, \t Total Gen Loss : 22.045658111572266, \t Total Dis Loss : 0.0005070168408565223\n",
      "Steps : 197200, \t Total Gen Loss : 28.695310592651367, \t Total Dis Loss : 0.0005315692978911102\n",
      "Steps : 197300, \t Total Gen Loss : 25.764802932739258, \t Total Dis Loss : 0.0005217220168560743\n",
      "Steps : 197400, \t Total Gen Loss : 26.380216598510742, \t Total Dis Loss : 0.0002762479125522077\n",
      "Steps : 197500, \t Total Gen Loss : 23.719547271728516, \t Total Dis Loss : 0.0013698178809136152\n",
      "Steps : 197600, \t Total Gen Loss : 23.338638305664062, \t Total Dis Loss : 0.00010619304521242157\n",
      "Steps : 197700, \t Total Gen Loss : 25.54606056213379, \t Total Dis Loss : 4.789407830685377e-05\n",
      "Steps : 197800, \t Total Gen Loss : 25.84600257873535, \t Total Dis Loss : 0.0013670164626091719\n",
      "Steps : 197900, \t Total Gen Loss : 26.421415328979492, \t Total Dis Loss : 0.00012386251182761043\n",
      "Steps : 198000, \t Total Gen Loss : 31.697879791259766, \t Total Dis Loss : 3.273914262535982e-05\n",
      "Steps : 198100, \t Total Gen Loss : 29.535350799560547, \t Total Dis Loss : 8.895030987332575e-06\n",
      "Steps : 198200, \t Total Gen Loss : 32.16657257080078, \t Total Dis Loss : 5.2488969231490046e-05\n",
      "Steps : 198300, \t Total Gen Loss : 29.167762756347656, \t Total Dis Loss : 0.00013075352762825787\n",
      "Steps : 198400, \t Total Gen Loss : 29.347944259643555, \t Total Dis Loss : 7.228648610180244e-05\n",
      "Steps : 198500, \t Total Gen Loss : 27.999109268188477, \t Total Dis Loss : 5.203955515753478e-05\n",
      "Steps : 198600, \t Total Gen Loss : 31.345922470092773, \t Total Dis Loss : 3.4886706998804584e-05\n",
      "Steps : 198700, \t Total Gen Loss : 32.239837646484375, \t Total Dis Loss : 6.348495662678033e-05\n",
      "Steps : 198800, \t Total Gen Loss : 29.615018844604492, \t Total Dis Loss : 9.55915038503008e-06\n",
      "Steps : 198900, \t Total Gen Loss : 30.803211212158203, \t Total Dis Loss : 1.61918669618899e-05\n",
      "Steps : 199000, \t Total Gen Loss : 29.586179733276367, \t Total Dis Loss : 2.5719316909089684e-05\n",
      "Steps : 199100, \t Total Gen Loss : 31.889713287353516, \t Total Dis Loss : 1.9871993572451174e-05\n",
      "Steps : 199200, \t Total Gen Loss : 27.397891998291016, \t Total Dis Loss : 5.939836773904972e-05\n",
      "Steps : 199300, \t Total Gen Loss : 28.491830825805664, \t Total Dis Loss : 5.276675437926315e-05\n",
      "Steps : 199400, \t Total Gen Loss : 28.10926628112793, \t Total Dis Loss : 0.00012490754306782037\n",
      "Steps : 199500, \t Total Gen Loss : 25.858661651611328, \t Total Dis Loss : 0.0003454295510891825\n",
      "Steps : 199600, \t Total Gen Loss : 26.80482292175293, \t Total Dis Loss : 0.00012557650916278362\n",
      "Steps : 199700, \t Total Gen Loss : 26.410480499267578, \t Total Dis Loss : 0.00011932046618312597\n",
      "Steps : 199800, \t Total Gen Loss : 26.90618324279785, \t Total Dis Loss : 0.00011959234689129516\n",
      "Steps : 199900, \t Total Gen Loss : 26.92383575439453, \t Total Dis Loss : 7.228527101688087e-05\n",
      "Steps : 200000, \t Total Gen Loss : 27.509815216064453, \t Total Dis Loss : 0.000220674613956362\n",
      "Steps : 200100, \t Total Gen Loss : 25.649568557739258, \t Total Dis Loss : 3.605210076784715e-05\n",
      "Steps : 200200, \t Total Gen Loss : 25.420255661010742, \t Total Dis Loss : 0.00012656104809138924\n",
      "Steps : 200300, \t Total Gen Loss : 26.4472713470459, \t Total Dis Loss : 0.00011263250053161755\n",
      "Steps : 200400, \t Total Gen Loss : 26.661849975585938, \t Total Dis Loss : 0.0003607860708143562\n",
      "Steps : 200500, \t Total Gen Loss : 27.33211898803711, \t Total Dis Loss : 4.452450593817048e-05\n",
      "Steps : 200600, \t Total Gen Loss : 26.171875, \t Total Dis Loss : 2.7156003852724098e-05\n",
      "Steps : 200700, \t Total Gen Loss : 27.36730194091797, \t Total Dis Loss : 0.0001220112681039609\n",
      "Steps : 200800, \t Total Gen Loss : 28.046794891357422, \t Total Dis Loss : 0.0005564840394072235\n",
      "Steps : 200900, \t Total Gen Loss : 29.43444061279297, \t Total Dis Loss : 6.664352986263111e-05\n",
      "Steps : 201000, \t Total Gen Loss : 23.224021911621094, \t Total Dis Loss : 7.36594301997684e-05\n",
      "Steps : 201100, \t Total Gen Loss : 27.867883682250977, \t Total Dis Loss : 4.3398023990448564e-05\n",
      "Steps : 201200, \t Total Gen Loss : 25.085289001464844, \t Total Dis Loss : 5.9368489019107074e-05\n",
      "Steps : 201300, \t Total Gen Loss : 24.585718154907227, \t Total Dis Loss : 6.919021689100191e-05\n",
      "Steps : 201400, \t Total Gen Loss : 27.332015991210938, \t Total Dis Loss : 4.262848960934207e-05\n",
      "Steps : 201500, \t Total Gen Loss : 26.166271209716797, \t Total Dis Loss : 3.417793777771294e-05\n",
      "Steps : 201600, \t Total Gen Loss : 23.826915740966797, \t Total Dis Loss : 0.0001493059826316312\n",
      "Steps : 201700, \t Total Gen Loss : 25.398508071899414, \t Total Dis Loss : 2.0303643395891413e-05\n",
      "Steps : 201800, \t Total Gen Loss : 29.525249481201172, \t Total Dis Loss : 1.2717182471533306e-05\n",
      "Steps : 201900, \t Total Gen Loss : 27.298606872558594, \t Total Dis Loss : 0.00011964631994487718\n",
      "Steps : 202000, \t Total Gen Loss : 26.299880981445312, \t Total Dis Loss : 4.2716408643173054e-05\n",
      "Steps : 202100, \t Total Gen Loss : 28.068492889404297, \t Total Dis Loss : 3.814658703049645e-05\n",
      "Steps : 202200, \t Total Gen Loss : 24.901329040527344, \t Total Dis Loss : 2.0920557290082797e-05\n",
      "Steps : 202300, \t Total Gen Loss : 23.986541748046875, \t Total Dis Loss : 0.0006962346960790455\n",
      "Steps : 202400, \t Total Gen Loss : 27.971691131591797, \t Total Dis Loss : 0.00023971847258508205\n",
      "Steps : 202500, \t Total Gen Loss : 25.495988845825195, \t Total Dis Loss : 0.00035808669053949416\n",
      "Time for epoch 36 is 75.67443060874939 sec\n",
      "Steps : 202600, \t Total Gen Loss : 39.39760971069336, \t Total Dis Loss : 5.1667491788975894e-05\n",
      "Steps : 202700, \t Total Gen Loss : 36.05394744873047, \t Total Dis Loss : 3.964724237448536e-05\n",
      "Steps : 202800, \t Total Gen Loss : 28.98029327392578, \t Total Dis Loss : 0.002402378711849451\n",
      "Steps : 202900, \t Total Gen Loss : 26.472074508666992, \t Total Dis Loss : 0.00020125000446569175\n",
      "Steps : 203000, \t Total Gen Loss : 27.50440216064453, \t Total Dis Loss : 0.00013011226837988943\n",
      "Steps : 203100, \t Total Gen Loss : 28.116968154907227, \t Total Dis Loss : 7.327937055379152e-05\n",
      "Steps : 203200, \t Total Gen Loss : 28.71646499633789, \t Total Dis Loss : 5.5285720009123906e-05\n",
      "Steps : 203300, \t Total Gen Loss : 27.890838623046875, \t Total Dis Loss : 0.00012521701864898205\n",
      "Steps : 203400, \t Total Gen Loss : 27.501903533935547, \t Total Dis Loss : 7.32046682969667e-05\n",
      "Steps : 203500, \t Total Gen Loss : 27.263168334960938, \t Total Dis Loss : 7.515202014474198e-05\n",
      "Steps : 203600, \t Total Gen Loss : 23.958707809448242, \t Total Dis Loss : 0.00022545181855093688\n",
      "Steps : 203700, \t Total Gen Loss : 24.915590286254883, \t Total Dis Loss : 0.00012572227569762617\n",
      "Steps : 203800, \t Total Gen Loss : 30.06281852722168, \t Total Dis Loss : 7.413655839627609e-05\n",
      "Steps : 203900, \t Total Gen Loss : 27.064117431640625, \t Total Dis Loss : 0.0001903223746921867\n",
      "Steps : 204000, \t Total Gen Loss : 23.823352813720703, \t Total Dis Loss : 0.002240169560536742\n",
      "Steps : 204100, \t Total Gen Loss : 27.898738861083984, \t Total Dis Loss : 0.00011850435112137347\n",
      "Steps : 204200, \t Total Gen Loss : 26.617862701416016, \t Total Dis Loss : 0.00017899014346767217\n",
      "Steps : 204300, \t Total Gen Loss : 28.785991668701172, \t Total Dis Loss : 4.682339931605384e-05\n",
      "Steps : 204400, \t Total Gen Loss : 24.717557907104492, \t Total Dis Loss : 0.00011170368816237897\n",
      "Steps : 204500, \t Total Gen Loss : 26.082977294921875, \t Total Dis Loss : 0.00013786468480248004\n",
      "Steps : 204600, \t Total Gen Loss : 26.011207580566406, \t Total Dis Loss : 0.00033389421878382564\n",
      "Steps : 204700, \t Total Gen Loss : 26.132694244384766, \t Total Dis Loss : 0.00020314777793828398\n",
      "Steps : 204800, \t Total Gen Loss : 26.802762985229492, \t Total Dis Loss : 0.00014701612235512584\n",
      "Steps : 204900, \t Total Gen Loss : 28.473915100097656, \t Total Dis Loss : 0.00017725849465932697\n",
      "Steps : 205000, \t Total Gen Loss : 29.471595764160156, \t Total Dis Loss : 1.4242667020880617e-05\n",
      "Steps : 205100, \t Total Gen Loss : 26.919397354125977, \t Total Dis Loss : 4.6748562454013154e-05\n",
      "Steps : 205200, \t Total Gen Loss : 27.612369537353516, \t Total Dis Loss : 8.215600246330723e-06\n",
      "Steps : 205300, \t Total Gen Loss : 27.07257843017578, \t Total Dis Loss : 9.378271897730883e-06\n",
      "Steps : 205400, \t Total Gen Loss : 24.602218627929688, \t Total Dis Loss : 5.1108101615682244e-05\n",
      "Steps : 205500, \t Total Gen Loss : 26.481212615966797, \t Total Dis Loss : 9.539741586195305e-05\n",
      "Steps : 205600, \t Total Gen Loss : 24.761133193969727, \t Total Dis Loss : 8.232372056227177e-05\n",
      "Steps : 205700, \t Total Gen Loss : 28.970050811767578, \t Total Dis Loss : 6.795495573896915e-05\n",
      "Steps : 205800, \t Total Gen Loss : 28.900306701660156, \t Total Dis Loss : 5.341348150977865e-05\n",
      "Steps : 205900, \t Total Gen Loss : 25.132322311401367, \t Total Dis Loss : 5.07127697346732e-05\n",
      "Steps : 206000, \t Total Gen Loss : 27.150794982910156, \t Total Dis Loss : 3.855296745314263e-05\n",
      "Steps : 206100, \t Total Gen Loss : 26.753582000732422, \t Total Dis Loss : 3.3780183002818376e-05\n",
      "Steps : 206200, \t Total Gen Loss : 28.16035270690918, \t Total Dis Loss : 4.9396581744076684e-05\n",
      "Steps : 206300, \t Total Gen Loss : 26.025802612304688, \t Total Dis Loss : 4.211219493299723e-05\n",
      "Steps : 206400, \t Total Gen Loss : 24.826461791992188, \t Total Dis Loss : 1.8082000678987242e-05\n",
      "Steps : 206500, \t Total Gen Loss : 25.943546295166016, \t Total Dis Loss : 3.185667083016597e-05\n",
      "Steps : 206600, \t Total Gen Loss : 25.090999603271484, \t Total Dis Loss : 5.3259762353263795e-05\n",
      "Steps : 206700, \t Total Gen Loss : 30.856815338134766, \t Total Dis Loss : 3.4152126318076625e-05\n",
      "Steps : 206800, \t Total Gen Loss : 30.21247673034668, \t Total Dis Loss : 2.7128433430334553e-05\n",
      "Steps : 206900, \t Total Gen Loss : 29.132978439331055, \t Total Dis Loss : 5.848802538821474e-05\n",
      "Steps : 207000, \t Total Gen Loss : 27.529220581054688, \t Total Dis Loss : 4.795193672180176e-05\n",
      "Steps : 207100, \t Total Gen Loss : 28.084041595458984, \t Total Dis Loss : 3.879166251863353e-05\n",
      "Steps : 207200, \t Total Gen Loss : 30.803621292114258, \t Total Dis Loss : 2.7114316253573634e-05\n",
      "Steps : 207300, \t Total Gen Loss : 26.85870361328125, \t Total Dis Loss : 5.450686876429245e-05\n",
      "Steps : 207400, \t Total Gen Loss : 29.81800651550293, \t Total Dis Loss : 4.700026329373941e-05\n",
      "Steps : 207500, \t Total Gen Loss : 27.206275939941406, \t Total Dis Loss : 3.074918640777469e-05\n",
      "Steps : 207600, \t Total Gen Loss : 29.048004150390625, \t Total Dis Loss : 2.7402289560995996e-05\n",
      "Steps : 207700, \t Total Gen Loss : 27.058250427246094, \t Total Dis Loss : 2.617013524286449e-05\n",
      "Steps : 207800, \t Total Gen Loss : 24.533241271972656, \t Total Dis Loss : 0.0007029452244751155\n",
      "Steps : 207900, \t Total Gen Loss : 24.855297088623047, \t Total Dis Loss : 0.00021556350111495703\n",
      "Steps : 208000, \t Total Gen Loss : 23.565196990966797, \t Total Dis Loss : 6.041657616151497e-05\n",
      "Steps : 208100, \t Total Gen Loss : 25.73908805847168, \t Total Dis Loss : 6.671885785181075e-05\n",
      "Time for epoch 37 is 75.91782999038696 sec\n",
      "Steps : 208200, \t Total Gen Loss : 25.252960205078125, \t Total Dis Loss : 3.6029228795086965e-05\n",
      "Steps : 208300, \t Total Gen Loss : 27.34008026123047, \t Total Dis Loss : 2.948027031379752e-05\n",
      "Steps : 208400, \t Total Gen Loss : 24.491342544555664, \t Total Dis Loss : 0.00039001903496682644\n",
      "Steps : 208500, \t Total Gen Loss : 30.634584426879883, \t Total Dis Loss : 7.888266964073409e-07\n",
      "Steps : 208600, \t Total Gen Loss : 28.312549591064453, \t Total Dis Loss : 7.254476076923311e-05\n",
      "Steps : 208700, \t Total Gen Loss : 31.36009979248047, \t Total Dis Loss : 3.353497595526278e-05\n",
      "Steps : 208800, \t Total Gen Loss : 32.7369499206543, \t Total Dis Loss : 1.214951453221147e-06\n",
      "Steps : 208900, \t Total Gen Loss : 25.24163818359375, \t Total Dis Loss : 2.636150202306453e-05\n",
      "Steps : 209000, \t Total Gen Loss : 29.792438507080078, \t Total Dis Loss : 0.0001473568263463676\n",
      "Steps : 209100, \t Total Gen Loss : 30.319372177124023, \t Total Dis Loss : 5.310883716447279e-05\n",
      "Steps : 209200, \t Total Gen Loss : 32.401695251464844, \t Total Dis Loss : 9.627023246139288e-06\n",
      "Steps : 209300, \t Total Gen Loss : 29.490903854370117, \t Total Dis Loss : 3.7708552554249763e-05\n",
      "Steps : 209400, \t Total Gen Loss : 27.989994049072266, \t Total Dis Loss : 0.0016746337059885263\n",
      "Steps : 209500, \t Total Gen Loss : 23.030261993408203, \t Total Dis Loss : 0.0006769739557057619\n",
      "Steps : 209600, \t Total Gen Loss : 29.253774642944336, \t Total Dis Loss : 6.536308501381427e-05\n",
      "Steps : 209700, \t Total Gen Loss : 29.930099487304688, \t Total Dis Loss : 8.449053711956367e-05\n",
      "Steps : 209800, \t Total Gen Loss : 29.781240463256836, \t Total Dis Loss : 4.4438085751608014e-05\n",
      "Steps : 209900, \t Total Gen Loss : 25.94808578491211, \t Total Dis Loss : 2.870687239919789e-05\n",
      "Steps : 210000, \t Total Gen Loss : 25.25823211669922, \t Total Dis Loss : 0.00015541366883553565\n",
      "Steps : 210100, \t Total Gen Loss : 27.047718048095703, \t Total Dis Loss : 5.098351539345458e-05\n",
      "Steps : 210200, \t Total Gen Loss : 25.769472122192383, \t Total Dis Loss : 0.0006593171856366098\n",
      "Steps : 210300, \t Total Gen Loss : 26.896121978759766, \t Total Dis Loss : 0.0002512539504095912\n",
      "Steps : 210400, \t Total Gen Loss : 27.994842529296875, \t Total Dis Loss : 8.290224650409073e-05\n",
      "Steps : 210500, \t Total Gen Loss : 27.10896873474121, \t Total Dis Loss : 6.46325497655198e-05\n",
      "Steps : 210600, \t Total Gen Loss : 27.55832290649414, \t Total Dis Loss : 7.581320096505806e-05\n",
      "Steps : 210700, \t Total Gen Loss : 23.570560455322266, \t Total Dis Loss : 0.0001366756041534245\n",
      "Steps : 210800, \t Total Gen Loss : 26.158254623413086, \t Total Dis Loss : 7.645904406672344e-05\n",
      "Steps : 210900, \t Total Gen Loss : 26.968292236328125, \t Total Dis Loss : 0.00021758931688964367\n",
      "Steps : 211000, \t Total Gen Loss : 24.686683654785156, \t Total Dis Loss : 5.46149312867783e-05\n",
      "Steps : 211100, \t Total Gen Loss : 25.318275451660156, \t Total Dis Loss : 5.069289181847125e-05\n",
      "Steps : 211200, \t Total Gen Loss : 31.705997467041016, \t Total Dis Loss : 9.591441084921826e-06\n",
      "Steps : 211300, \t Total Gen Loss : 24.669166564941406, \t Total Dis Loss : 1.9741391952265985e-05\n",
      "Steps : 211400, \t Total Gen Loss : 29.2836856842041, \t Total Dis Loss : 1.4773708244320005e-05\n",
      "Steps : 211500, \t Total Gen Loss : 24.497535705566406, \t Total Dis Loss : 0.0015788119053468108\n",
      "Steps : 211600, \t Total Gen Loss : 28.831756591796875, \t Total Dis Loss : 6.761519671272254e-06\n",
      "Steps : 211700, \t Total Gen Loss : 29.756698608398438, \t Total Dis Loss : 1.9244216673541814e-05\n",
      "Steps : 211800, \t Total Gen Loss : 28.390653610229492, \t Total Dis Loss : 4.364829874248244e-05\n",
      "Steps : 211900, \t Total Gen Loss : 26.19906234741211, \t Total Dis Loss : 5.8103501942241564e-05\n",
      "Steps : 212000, \t Total Gen Loss : 25.592666625976562, \t Total Dis Loss : 4.246325261192396e-05\n",
      "Steps : 212100, \t Total Gen Loss : 24.947690963745117, \t Total Dis Loss : 3.1247771403286606e-05\n",
      "Steps : 212200, \t Total Gen Loss : 25.844051361083984, \t Total Dis Loss : 2.814887193380855e-05\n",
      "Steps : 212300, \t Total Gen Loss : 34.63343811035156, \t Total Dis Loss : 0.00720941461622715\n",
      "Steps : 212400, \t Total Gen Loss : 37.15317153930664, \t Total Dis Loss : 5.305591912474483e-05\n",
      "Steps : 212500, \t Total Gen Loss : 37.13059997558594, \t Total Dis Loss : 2.4742281311773695e-05\n",
      "Steps : 212600, \t Total Gen Loss : 34.745399475097656, \t Total Dis Loss : 1.536998024675995e-05\n",
      "Steps : 212700, \t Total Gen Loss : 33.21078109741211, \t Total Dis Loss : 2.454760942782741e-05\n",
      "Steps : 212800, \t Total Gen Loss : 32.210540771484375, \t Total Dis Loss : 2.16149928746745e-05\n",
      "Steps : 212900, \t Total Gen Loss : 24.763704299926758, \t Total Dis Loss : 0.0009497597347944975\n",
      "Steps : 213000, \t Total Gen Loss : 27.649906158447266, \t Total Dis Loss : 0.0002971862268168479\n",
      "Steps : 213100, \t Total Gen Loss : 27.971370697021484, \t Total Dis Loss : 0.00041018115007318556\n",
      "Steps : 213200, \t Total Gen Loss : 25.544469833374023, \t Total Dis Loss : 0.0003826188622042537\n",
      "Steps : 213300, \t Total Gen Loss : 24.434032440185547, \t Total Dis Loss : 0.000211010905331932\n",
      "Steps : 213400, \t Total Gen Loss : 26.994640350341797, \t Total Dis Loss : 2.9824834200553596e-05\n",
      "Steps : 213500, \t Total Gen Loss : 28.67853546142578, \t Total Dis Loss : 0.00011644831829471514\n",
      "Steps : 213600, \t Total Gen Loss : 27.385690689086914, \t Total Dis Loss : 0.00010093383025377989\n",
      "Steps : 213700, \t Total Gen Loss : 29.481536865234375, \t Total Dis Loss : 3.5460907383821905e-05\n",
      "Time for epoch 38 is 69.82154417037964 sec\n",
      "Steps : 213800, \t Total Gen Loss : 28.732406616210938, \t Total Dis Loss : 5.047325976192951e-05\n",
      "Steps : 213900, \t Total Gen Loss : 25.668376922607422, \t Total Dis Loss : 0.00011903708218596876\n",
      "Steps : 214000, \t Total Gen Loss : 28.82789421081543, \t Total Dis Loss : 7.399496098514646e-05\n",
      "Steps : 214100, \t Total Gen Loss : 30.27983283996582, \t Total Dis Loss : 0.00010697569086914882\n",
      "Steps : 214200, \t Total Gen Loss : 29.043811798095703, \t Total Dis Loss : 2.4207711248891428e-05\n",
      "Steps : 214300, \t Total Gen Loss : 27.605987548828125, \t Total Dis Loss : 8.3773338701576e-05\n",
      "Steps : 214400, \t Total Gen Loss : 31.47906494140625, \t Total Dis Loss : 5.152880112291314e-05\n",
      "Steps : 214500, \t Total Gen Loss : 31.788040161132812, \t Total Dis Loss : 7.217584061436355e-05\n",
      "Steps : 214600, \t Total Gen Loss : 27.909849166870117, \t Total Dis Loss : 2.986914296343457e-05\n",
      "Steps : 214700, \t Total Gen Loss : 27.047222137451172, \t Total Dis Loss : 0.00014567260222975165\n",
      "Steps : 214800, \t Total Gen Loss : 28.92630386352539, \t Total Dis Loss : 0.0001264474558411166\n",
      "Steps : 214900, \t Total Gen Loss : 22.99387550354004, \t Total Dis Loss : 0.00034381161094643176\n",
      "Steps : 215000, \t Total Gen Loss : 22.341259002685547, \t Total Dis Loss : 0.0003429126227274537\n",
      "Steps : 215100, \t Total Gen Loss : 25.705699920654297, \t Total Dis Loss : 0.0001018844050122425\n",
      "Steps : 215200, \t Total Gen Loss : 25.53882598876953, \t Total Dis Loss : 0.0006894032703712583\n",
      "Steps : 215300, \t Total Gen Loss : 25.77444839477539, \t Total Dis Loss : 5.093344225315377e-05\n",
      "Steps : 215400, \t Total Gen Loss : 24.360382080078125, \t Total Dis Loss : 0.0001395014114677906\n",
      "Steps : 215500, \t Total Gen Loss : 28.245628356933594, \t Total Dis Loss : 8.61504377098754e-05\n",
      "Steps : 215600, \t Total Gen Loss : 25.989940643310547, \t Total Dis Loss : 0.0005034371861256659\n",
      "Steps : 215700, \t Total Gen Loss : 26.047618865966797, \t Total Dis Loss : 0.0010085217654705048\n",
      "Steps : 215800, \t Total Gen Loss : 39.92987060546875, \t Total Dis Loss : 4.370246097096242e-05\n",
      "Steps : 215900, \t Total Gen Loss : 29.40389633178711, \t Total Dis Loss : 3.734747224370949e-05\n",
      "Steps : 216000, \t Total Gen Loss : 24.827682495117188, \t Total Dis Loss : 0.002759869210422039\n",
      "Steps : 216100, \t Total Gen Loss : 30.291698455810547, \t Total Dis Loss : 8.645562957099173e-06\n",
      "Steps : 216200, \t Total Gen Loss : 26.607389450073242, \t Total Dis Loss : 0.0001519469078630209\n",
      "Steps : 216300, \t Total Gen Loss : 27.840574264526367, \t Total Dis Loss : 9.859590500127524e-05\n",
      "Steps : 216400, \t Total Gen Loss : 32.83744430541992, \t Total Dis Loss : 1.3784957445750479e-05\n",
      "Steps : 216500, \t Total Gen Loss : 26.723695755004883, \t Total Dis Loss : 0.0003777594829443842\n",
      "Steps : 216600, \t Total Gen Loss : 27.988571166992188, \t Total Dis Loss : 1.1221061944961548\n",
      "Steps : 216700, \t Total Gen Loss : 28.386287689208984, \t Total Dis Loss : 0.0002185449266107753\n",
      "Steps : 216800, \t Total Gen Loss : 27.211641311645508, \t Total Dis Loss : 0.0002071973285637796\n",
      "Steps : 216900, \t Total Gen Loss : 28.316314697265625, \t Total Dis Loss : 5.116784814163111e-05\n",
      "Steps : 217000, \t Total Gen Loss : 27.131431579589844, \t Total Dis Loss : 2.06710410566302e-05\n",
      "Steps : 217100, \t Total Gen Loss : 26.531692504882812, \t Total Dis Loss : 0.0002080052363453433\n",
      "Steps : 217200, \t Total Gen Loss : 26.662742614746094, \t Total Dis Loss : 0.0004569033335428685\n",
      "Steps : 217300, \t Total Gen Loss : 26.4617919921875, \t Total Dis Loss : 3.2594816730124876e-05\n",
      "Steps : 217400, \t Total Gen Loss : 26.41156768798828, \t Total Dis Loss : 0.0006963962805457413\n",
      "Steps : 217500, \t Total Gen Loss : 33.09284210205078, \t Total Dis Loss : 2.160008625651244e-05\n",
      "Steps : 217600, \t Total Gen Loss : 33.73964309692383, \t Total Dis Loss : 0.00011834832548629493\n",
      "Steps : 217700, \t Total Gen Loss : 28.19875144958496, \t Total Dis Loss : 0.00010637039667926729\n",
      "Steps : 217800, \t Total Gen Loss : 29.483684539794922, \t Total Dis Loss : 0.0013115857727825642\n",
      "Steps : 217900, \t Total Gen Loss : 34.12213897705078, \t Total Dis Loss : 1.566198443470057e-05\n",
      "Steps : 218000, \t Total Gen Loss : 28.160011291503906, \t Total Dis Loss : 0.00026461592642590404\n",
      "Steps : 218100, \t Total Gen Loss : 26.526668548583984, \t Total Dis Loss : 0.00027402094565331936\n",
      "Steps : 218200, \t Total Gen Loss : 27.533464431762695, \t Total Dis Loss : 0.0009035228285938501\n",
      "Steps : 218300, \t Total Gen Loss : 30.274799346923828, \t Total Dis Loss : 0.0008738747565075755\n",
      "Steps : 218400, \t Total Gen Loss : 28.023941040039062, \t Total Dis Loss : 0.0024245833046734333\n",
      "Steps : 218500, \t Total Gen Loss : 24.879430770874023, \t Total Dis Loss : 0.0006051785894669592\n",
      "Steps : 218600, \t Total Gen Loss : 33.585506439208984, \t Total Dis Loss : 4.108056236873381e-05\n",
      "Steps : 218700, \t Total Gen Loss : 38.09349060058594, \t Total Dis Loss : 1.6405754649895243e-05\n",
      "Steps : 218800, \t Total Gen Loss : 34.63494873046875, \t Total Dis Loss : 6.608656258322299e-05\n",
      "Steps : 218900, \t Total Gen Loss : 32.440757751464844, \t Total Dis Loss : 0.0005928122554905713\n",
      "Steps : 219000, \t Total Gen Loss : 31.633525848388672, \t Total Dis Loss : 3.727942021214403e-05\n",
      "Steps : 219100, \t Total Gen Loss : 29.784276962280273, \t Total Dis Loss : 6.332774501061067e-05\n",
      "Steps : 219200, \t Total Gen Loss : 28.57158851623535, \t Total Dis Loss : 2.7209589461563155e-05\n",
      "Steps : 219300, \t Total Gen Loss : 29.715187072753906, \t Total Dis Loss : 0.00015684602840337902\n",
      "Time for epoch 39 is 69.55201148986816 sec\n",
      "Steps : 219400, \t Total Gen Loss : 32.242820739746094, \t Total Dis Loss : 5.8737055951496586e-05\n",
      "Steps : 219500, \t Total Gen Loss : 27.60961151123047, \t Total Dis Loss : 0.0017599982675164938\n",
      "Steps : 219600, \t Total Gen Loss : 30.1986083984375, \t Total Dis Loss : 2.601042069727555e-05\n",
      "Steps : 219700, \t Total Gen Loss : 29.3397274017334, \t Total Dis Loss : 0.00039637822192162275\n",
      "Steps : 219800, \t Total Gen Loss : 27.04663848876953, \t Total Dis Loss : 0.0010069645941257477\n",
      "Steps : 219900, \t Total Gen Loss : 29.658069610595703, \t Total Dis Loss : 1.6785816114861518e-05\n",
      "Steps : 220000, \t Total Gen Loss : 28.30738639831543, \t Total Dis Loss : 0.0003198632621206343\n",
      "Steps : 220100, \t Total Gen Loss : 30.822738647460938, \t Total Dis Loss : 4.4645195885095745e-05\n",
      "Steps : 220200, \t Total Gen Loss : 29.659366607666016, \t Total Dis Loss : 0.00020826587569899857\n",
      "Steps : 220300, \t Total Gen Loss : 29.8372859954834, \t Total Dis Loss : 4.588813317241147e-05\n",
      "Steps : 220400, \t Total Gen Loss : 29.937623977661133, \t Total Dis Loss : 8.977826655609533e-05\n",
      "Steps : 220500, \t Total Gen Loss : 26.765859603881836, \t Total Dis Loss : 0.00016548701387364417\n",
      "Steps : 220600, \t Total Gen Loss : 29.959430694580078, \t Total Dis Loss : 0.00011338764306856319\n",
      "Steps : 220700, \t Total Gen Loss : 35.43133544921875, \t Total Dis Loss : 4.813060877495445e-05\n",
      "Steps : 220800, \t Total Gen Loss : 37.3820915222168, \t Total Dis Loss : 0.0004272848891559988\n",
      "Steps : 220900, \t Total Gen Loss : 33.52634048461914, \t Total Dis Loss : 1.1948266546824016e-05\n",
      "Steps : 221000, \t Total Gen Loss : 28.11656951904297, \t Total Dis Loss : 0.0003455827245488763\n",
      "Steps : 221100, \t Total Gen Loss : 26.531776428222656, \t Total Dis Loss : 4.421199264470488e-05\n",
      "Steps : 221200, \t Total Gen Loss : 28.367191314697266, \t Total Dis Loss : 0.00015002024883870035\n",
      "Steps : 221300, \t Total Gen Loss : 30.017784118652344, \t Total Dis Loss : 9.481794222665485e-06\n",
      "Steps : 221400, \t Total Gen Loss : 27.28965187072754, \t Total Dis Loss : 0.00018443663429934531\n",
      "Steps : 221500, \t Total Gen Loss : 26.65957260131836, \t Total Dis Loss : 3.02048083540285e-05\n",
      "Steps : 221600, \t Total Gen Loss : 31.877498626708984, \t Total Dis Loss : 0.00015349796740338206\n",
      "Steps : 221700, \t Total Gen Loss : 30.536563873291016, \t Total Dis Loss : 2.879849125747569e-05\n",
      "Steps : 221800, \t Total Gen Loss : 27.219894409179688, \t Total Dis Loss : 0.0007155484636314213\n",
      "Steps : 221900, \t Total Gen Loss : 29.209562301635742, \t Total Dis Loss : 0.000195835527847521\n",
      "Steps : 222000, \t Total Gen Loss : 25.000442504882812, \t Total Dis Loss : 0.0004515188920777291\n",
      "Steps : 222100, \t Total Gen Loss : 27.790258407592773, \t Total Dis Loss : 9.014700481202453e-05\n",
      "Steps : 222200, \t Total Gen Loss : 28.3986759185791, \t Total Dis Loss : 0.04408961907029152\n",
      "Steps : 222300, \t Total Gen Loss : 31.652729034423828, \t Total Dis Loss : 0.00032363401260226965\n",
      "Steps : 222400, \t Total Gen Loss : 28.79568099975586, \t Total Dis Loss : 8.158027048921213e-05\n",
      "Steps : 222500, \t Total Gen Loss : 26.126033782958984, \t Total Dis Loss : 8.961711137089878e-05\n",
      "Steps : 222600, \t Total Gen Loss : 30.086822509765625, \t Total Dis Loss : 2.713130379561335e-05\n",
      "Steps : 222700, \t Total Gen Loss : 27.194400787353516, \t Total Dis Loss : 0.00015612113929819316\n",
      "Steps : 222800, \t Total Gen Loss : 29.36959457397461, \t Total Dis Loss : 0.0003754596400540322\n",
      "Steps : 222900, \t Total Gen Loss : 31.633113861083984, \t Total Dis Loss : 0.00015574500139337033\n",
      "Steps : 223000, \t Total Gen Loss : 27.759469985961914, \t Total Dis Loss : 2.7444488296168856e-05\n",
      "Steps : 223100, \t Total Gen Loss : 30.772733688354492, \t Total Dis Loss : 9.372366912430152e-05\n",
      "Steps : 223200, \t Total Gen Loss : 30.66020965576172, \t Total Dis Loss : 4.722375251731137e-06\n",
      "Steps : 223300, \t Total Gen Loss : 27.775196075439453, \t Total Dis Loss : 3.096669024671428e-05\n",
      "Steps : 223400, \t Total Gen Loss : 26.91339111328125, \t Total Dis Loss : 2.1730556909460574e-05\n",
      "Steps : 223500, \t Total Gen Loss : 28.453128814697266, \t Total Dis Loss : 2.8372305678203702e-05\n",
      "Steps : 223600, \t Total Gen Loss : 24.705101013183594, \t Total Dis Loss : 3.2096861104946584e-05\n",
      "Steps : 223700, \t Total Gen Loss : 30.708120346069336, \t Total Dis Loss : 2.4625442165415734e-05\n",
      "Steps : 223800, \t Total Gen Loss : 29.244226455688477, \t Total Dis Loss : 3.259530058130622e-05\n",
      "Steps : 223900, \t Total Gen Loss : 28.555763244628906, \t Total Dis Loss : 2.7833499189000577e-05\n",
      "Steps : 224000, \t Total Gen Loss : 26.054834365844727, \t Total Dis Loss : 5.1968643674626946e-05\n",
      "Steps : 224100, \t Total Gen Loss : 24.260208129882812, \t Total Dis Loss : 0.0011816896731033921\n",
      "Steps : 224200, \t Total Gen Loss : 26.282196044921875, \t Total Dis Loss : 3.207018016837537e-05\n",
      "Steps : 224300, \t Total Gen Loss : 25.95360565185547, \t Total Dis Loss : 0.00025775289395824075\n",
      "Steps : 224400, \t Total Gen Loss : 27.668458938598633, \t Total Dis Loss : 9.488422074355185e-05\n",
      "Steps : 224500, \t Total Gen Loss : 25.310970306396484, \t Total Dis Loss : 0.00021669699344784021\n",
      "Steps : 224600, \t Total Gen Loss : 30.8378849029541, \t Total Dis Loss : 0.00010904610098805279\n",
      "Steps : 224700, \t Total Gen Loss : 28.95537567138672, \t Total Dis Loss : 0.00015459046699106693\n",
      "Steps : 224800, \t Total Gen Loss : 25.97595977783203, \t Total Dis Loss : 9.89884210866876e-05\n",
      "Steps : 224900, \t Total Gen Loss : 25.357711791992188, \t Total Dis Loss : 0.0003580307529773563\n",
      "Steps : 225000, \t Total Gen Loss : 23.740966796875, \t Total Dis Loss : 6.212593871168792e-05\n",
      "Time for epoch 40 is 70.9368724822998 sec\n",
      "Steps : 225100, \t Total Gen Loss : 22.199655532836914, \t Total Dis Loss : 0.0012883141171187162\n",
      "Steps : 225200, \t Total Gen Loss : 23.24225616455078, \t Total Dis Loss : 0.0021415017545223236\n",
      "Steps : 225300, \t Total Gen Loss : 23.035526275634766, \t Total Dis Loss : 0.00031079965992830694\n",
      "Steps : 225400, \t Total Gen Loss : 27.839632034301758, \t Total Dis Loss : 0.001960321795195341\n",
      "Steps : 225500, \t Total Gen Loss : 25.2322940826416, \t Total Dis Loss : 2.5041119442903437e-05\n",
      "Steps : 225600, \t Total Gen Loss : 27.169288635253906, \t Total Dis Loss : 4.258481931174174e-05\n",
      "Steps : 225700, \t Total Gen Loss : 26.88502311706543, \t Total Dis Loss : 8.618900756118819e-05\n",
      "Steps : 225800, \t Total Gen Loss : 24.26146697998047, \t Total Dis Loss : 0.0002460159594193101\n",
      "Steps : 225900, \t Total Gen Loss : 24.53420639038086, \t Total Dis Loss : 0.00012659917410928756\n",
      "Steps : 226000, \t Total Gen Loss : 23.185264587402344, \t Total Dis Loss : 0.00010510317952139303\n",
      "Steps : 226100, \t Total Gen Loss : 32.21250915527344, \t Total Dis Loss : 2.7887286705663428e-05\n",
      "Steps : 226200, \t Total Gen Loss : 27.25613784790039, \t Total Dis Loss : 0.00011733175051631406\n",
      "Steps : 226300, \t Total Gen Loss : 26.673770904541016, \t Total Dis Loss : 4.206224548397586e-05\n",
      "Steps : 226400, \t Total Gen Loss : 26.881332397460938, \t Total Dis Loss : 7.109243597369641e-05\n",
      "Steps : 226500, \t Total Gen Loss : 25.8758544921875, \t Total Dis Loss : 6.210117862792686e-05\n",
      "Steps : 226600, \t Total Gen Loss : 27.905858993530273, \t Total Dis Loss : 6.847420991107356e-06\n",
      "Steps : 226700, \t Total Gen Loss : 26.938369750976562, \t Total Dis Loss : 4.6845681936247274e-05\n",
      "Steps : 226800, \t Total Gen Loss : 27.453657150268555, \t Total Dis Loss : 1.439562674931949e-05\n",
      "Steps : 226900, \t Total Gen Loss : 25.463254928588867, \t Total Dis Loss : 5.367711128201336e-05\n",
      "Steps : 227000, \t Total Gen Loss : 27.419586181640625, \t Total Dis Loss : 7.143260154407471e-05\n",
      "Steps : 227100, \t Total Gen Loss : 28.353179931640625, \t Total Dis Loss : 2.6934118068311363e-05\n",
      "Steps : 227200, \t Total Gen Loss : 28.11206817626953, \t Total Dis Loss : 6.868082709843293e-05\n",
      "Steps : 227300, \t Total Gen Loss : 24.616825103759766, \t Total Dis Loss : 0.0004804092750418931\n",
      "Steps : 227400, \t Total Gen Loss : 25.864099502563477, \t Total Dis Loss : 9.993116691475734e-05\n",
      "Steps : 227500, \t Total Gen Loss : 23.00101089477539, \t Total Dis Loss : 0.0002403796388534829\n",
      "Steps : 227600, \t Total Gen Loss : 23.475849151611328, \t Total Dis Loss : 0.00017495459178462625\n",
      "Steps : 227700, \t Total Gen Loss : 28.75796127319336, \t Total Dis Loss : 5.602174860541709e-05\n",
      "Steps : 227800, \t Total Gen Loss : 27.949569702148438, \t Total Dis Loss : 0.00013279238191898912\n",
      "Steps : 227900, \t Total Gen Loss : 24.010570526123047, \t Total Dis Loss : 0.000146067060995847\n",
      "Steps : 228000, \t Total Gen Loss : 26.938121795654297, \t Total Dis Loss : 0.6345299482345581\n",
      "Steps : 228100, \t Total Gen Loss : 27.07766342163086, \t Total Dis Loss : 0.00014375703176483512\n",
      "Steps : 228200, \t Total Gen Loss : 27.0583553314209, \t Total Dis Loss : 0.000165598452440463\n",
      "Steps : 228300, \t Total Gen Loss : 26.719642639160156, \t Total Dis Loss : 5.52597084606532e-05\n",
      "Steps : 228400, \t Total Gen Loss : 27.19788360595703, \t Total Dis Loss : 6.883020250825211e-05\n",
      "Steps : 228500, \t Total Gen Loss : 26.494651794433594, \t Total Dis Loss : 0.00010622819536365569\n",
      "Steps : 228600, \t Total Gen Loss : 27.734432220458984, \t Total Dis Loss : 8.08135446277447e-05\n",
      "Steps : 228700, \t Total Gen Loss : 26.270523071289062, \t Total Dis Loss : 5.6749264331301674e-05\n",
      "Steps : 228800, \t Total Gen Loss : 27.735000610351562, \t Total Dis Loss : 0.00015621782222297043\n",
      "Steps : 228900, \t Total Gen Loss : 31.620750427246094, \t Total Dis Loss : 1.4745473890798166e-05\n",
      "Steps : 229000, \t Total Gen Loss : 36.209068298339844, \t Total Dis Loss : 0.00048814399633556604\n",
      "Steps : 229100, \t Total Gen Loss : 33.692230224609375, \t Total Dis Loss : 0.00021033184020780027\n",
      "Steps : 229200, \t Total Gen Loss : 27.604511260986328, \t Total Dis Loss : 6.701114762108773e-05\n",
      "Steps : 229300, \t Total Gen Loss : 28.158103942871094, \t Total Dis Loss : 7.69437465351075e-05\n",
      "Steps : 229400, \t Total Gen Loss : 27.61278533935547, \t Total Dis Loss : 0.001893996843136847\n",
      "Steps : 229500, \t Total Gen Loss : 28.00899887084961, \t Total Dis Loss : 0.00023624274763278663\n",
      "Steps : 229600, \t Total Gen Loss : 27.446044921875, \t Total Dis Loss : 0.0001305963087361306\n",
      "Steps : 229700, \t Total Gen Loss : 26.50174331665039, \t Total Dis Loss : 8.939106191974133e-05\n",
      "Steps : 229800, \t Total Gen Loss : 38.05500793457031, \t Total Dis Loss : 0.0005281391786411405\n",
      "Steps : 229900, \t Total Gen Loss : 30.237699508666992, \t Total Dis Loss : 0.00019903927750419825\n",
      "Steps : 230000, \t Total Gen Loss : 26.964143753051758, \t Total Dis Loss : 0.0001529987930553034\n",
      "Steps : 230100, \t Total Gen Loss : 29.281469345092773, \t Total Dis Loss : 6.639018101850525e-05\n",
      "Steps : 230200, \t Total Gen Loss : 23.6646728515625, \t Total Dis Loss : 0.00022106066171545535\n",
      "Steps : 230300, \t Total Gen Loss : 28.133922576904297, \t Total Dis Loss : 0.00017161888536065817\n",
      "Steps : 230400, \t Total Gen Loss : 27.295564651489258, \t Total Dis Loss : 5.734361911891028e-05\n",
      "Steps : 230500, \t Total Gen Loss : 26.292572021484375, \t Total Dis Loss : 5.706829688278958e-05\n",
      "Steps : 230600, \t Total Gen Loss : 25.71381187438965, \t Total Dis Loss : 0.0005168636562302709\n",
      "Time for epoch 41 is 69.51347303390503 sec\n",
      "Steps : 230700, \t Total Gen Loss : 25.676372528076172, \t Total Dis Loss : 0.0008235406130552292\n",
      "Steps : 230800, \t Total Gen Loss : 27.577058792114258, \t Total Dis Loss : 0.00012374385551083833\n",
      "Steps : 230900, \t Total Gen Loss : 25.1761474609375, \t Total Dis Loss : 0.00010010712139774114\n",
      "Steps : 231000, \t Total Gen Loss : 24.431602478027344, \t Total Dis Loss : 9.97129682218656e-05\n",
      "Steps : 231100, \t Total Gen Loss : 24.491003036499023, \t Total Dis Loss : 6.280664820224047e-05\n",
      "Steps : 231200, \t Total Gen Loss : 23.07837677001953, \t Total Dis Loss : 0.00033025944139808416\n",
      "Steps : 231300, \t Total Gen Loss : 28.153696060180664, \t Total Dis Loss : 0.00011767563410103321\n",
      "Steps : 231400, \t Total Gen Loss : 24.812358856201172, \t Total Dis Loss : 6.456174742197618e-05\n",
      "Steps : 231500, \t Total Gen Loss : 26.370241165161133, \t Total Dis Loss : 8.82167587406002e-05\n",
      "Steps : 231600, \t Total Gen Loss : 27.645374298095703, \t Total Dis Loss : 7.702984294155613e-05\n",
      "Steps : 231700, \t Total Gen Loss : 32.77120590209961, \t Total Dis Loss : 4.405885920277797e-05\n",
      "Steps : 231800, \t Total Gen Loss : 24.953868865966797, \t Total Dis Loss : 0.00014847240527160466\n",
      "Steps : 231900, \t Total Gen Loss : 27.791458129882812, \t Total Dis Loss : 7.47064987081103e-05\n",
      "Steps : 232000, \t Total Gen Loss : 27.502574920654297, \t Total Dis Loss : 4.9281108658760786e-05\n",
      "Steps : 232100, \t Total Gen Loss : 23.731395721435547, \t Total Dis Loss : 5.7872795878211036e-05\n",
      "Steps : 232200, \t Total Gen Loss : 27.133987426757812, \t Total Dis Loss : 9.180631604976952e-05\n",
      "Steps : 232300, \t Total Gen Loss : 26.66187286376953, \t Total Dis Loss : 4.2529798520263284e-05\n",
      "Steps : 232400, \t Total Gen Loss : 25.892047882080078, \t Total Dis Loss : 5.2417279221117496e-05\n",
      "Steps : 232500, \t Total Gen Loss : 26.55230712890625, \t Total Dis Loss : 3.2956977520370856e-05\n",
      "Steps : 232600, \t Total Gen Loss : 29.703983306884766, \t Total Dis Loss : 0.0004092559975106269\n",
      "Steps : 232700, \t Total Gen Loss : 24.418182373046875, \t Total Dis Loss : 9.809701441554353e-05\n",
      "Steps : 232800, \t Total Gen Loss : 27.48780059814453, \t Total Dis Loss : 6.158476753626019e-05\n",
      "Steps : 232900, \t Total Gen Loss : 27.03075408935547, \t Total Dis Loss : 2.7041405701311305e-05\n",
      "Steps : 233000, \t Total Gen Loss : 28.49155616760254, \t Total Dis Loss : 6.441502046072856e-05\n",
      "Steps : 233100, \t Total Gen Loss : 26.539138793945312, \t Total Dis Loss : 4.234755760990083e-05\n",
      "Steps : 233200, \t Total Gen Loss : 26.426105499267578, \t Total Dis Loss : 3.176573954988271e-05\n",
      "Steps : 233300, \t Total Gen Loss : 28.8165225982666, \t Total Dis Loss : 4.465411257115193e-05\n",
      "Steps : 233400, \t Total Gen Loss : 26.153724670410156, \t Total Dis Loss : 6.215482426341623e-05\n",
      "Steps : 233500, \t Total Gen Loss : 26.437503814697266, \t Total Dis Loss : 1.4150572496873792e-05\n",
      "Steps : 233600, \t Total Gen Loss : 25.813011169433594, \t Total Dis Loss : 1.8315698980586603e-05\n",
      "Steps : 233700, \t Total Gen Loss : 26.504653930664062, \t Total Dis Loss : 1.6207992302952334e-05\n",
      "Steps : 233800, \t Total Gen Loss : 27.679912567138672, \t Total Dis Loss : 1.2498796422732994e-05\n",
      "Steps : 233900, \t Total Gen Loss : 24.846263885498047, \t Total Dis Loss : 0.00010392806143499911\n",
      "Steps : 234000, \t Total Gen Loss : 26.109054565429688, \t Total Dis Loss : 3.4433105611242354e-05\n",
      "Steps : 234100, \t Total Gen Loss : 27.25426483154297, \t Total Dis Loss : 4.213021384202875e-05\n",
      "Steps : 234200, \t Total Gen Loss : 27.131946563720703, \t Total Dis Loss : 2.587818744359538e-05\n",
      "Steps : 234300, \t Total Gen Loss : 26.50394630432129, \t Total Dis Loss : 2.688882887014188e-05\n",
      "Steps : 234400, \t Total Gen Loss : 25.890560150146484, \t Total Dis Loss : 1.601970143383369e-05\n",
      "Steps : 234500, \t Total Gen Loss : 26.231101989746094, \t Total Dis Loss : 3.545673826010898e-05\n",
      "Steps : 234600, \t Total Gen Loss : 24.2796630859375, \t Total Dis Loss : 2.7322817913955078e-05\n",
      "Steps : 234700, \t Total Gen Loss : 28.27228546142578, \t Total Dis Loss : 2.1140756871318445e-05\n",
      "Steps : 234800, \t Total Gen Loss : 26.201860427856445, \t Total Dis Loss : 1.554291338834446e-05\n",
      "Steps : 234900, \t Total Gen Loss : 21.774370193481445, \t Total Dis Loss : 0.0002970526984427124\n",
      "Steps : 235000, \t Total Gen Loss : 24.639829635620117, \t Total Dis Loss : 0.00012722784595098346\n",
      "Steps : 235100, \t Total Gen Loss : 24.3840389251709, \t Total Dis Loss : 0.00010845594079000875\n",
      "Steps : 235200, \t Total Gen Loss : 24.029865264892578, \t Total Dis Loss : 6.112508708611131e-05\n",
      "Steps : 235300, \t Total Gen Loss : 27.518043518066406, \t Total Dis Loss : 7.358449511229992e-05\n",
      "Steps : 235400, \t Total Gen Loss : 28.222333908081055, \t Total Dis Loss : 0.00021442348952405155\n",
      "Steps : 235500, \t Total Gen Loss : 30.365659713745117, \t Total Dis Loss : 3.275840572314337e-05\n",
      "Steps : 235600, \t Total Gen Loss : 24.601974487304688, \t Total Dis Loss : 2.8276317607378587e-05\n",
      "Steps : 235700, \t Total Gen Loss : 26.840904235839844, \t Total Dis Loss : 3.781507984967902e-05\n",
      "Steps : 235800, \t Total Gen Loss : 26.863935470581055, \t Total Dis Loss : 5.731580313295126e-05\n",
      "Steps : 235900, \t Total Gen Loss : 24.852310180664062, \t Total Dis Loss : 8.157344564097002e-05\n",
      "Steps : 236000, \t Total Gen Loss : 28.753246307373047, \t Total Dis Loss : 0.00023782008793205023\n",
      "Steps : 236100, \t Total Gen Loss : 25.751850128173828, \t Total Dis Loss : 2.7085092369816266e-05\n",
      "Steps : 236200, \t Total Gen Loss : 27.139392852783203, \t Total Dis Loss : 1.976047678908799e-05\n",
      "Time for epoch 42 is 69.50114178657532 sec\n",
      "Steps : 236300, \t Total Gen Loss : 27.19148063659668, \t Total Dis Loss : 3.947985533159226e-05\n",
      "Steps : 236400, \t Total Gen Loss : 25.594280242919922, \t Total Dis Loss : 3.857954288832843e-05\n",
      "Steps : 236500, \t Total Gen Loss : 24.5949764251709, \t Total Dis Loss : 2.433463123452384e-05\n",
      "Steps : 236600, \t Total Gen Loss : 27.08245849609375, \t Total Dis Loss : 3.1028277589939535e-05\n",
      "Steps : 236700, \t Total Gen Loss : 25.859203338623047, \t Total Dis Loss : 1.706751572783105e-05\n",
      "Steps : 236800, \t Total Gen Loss : 27.90448570251465, \t Total Dis Loss : 9.762477020558435e-06\n",
      "Steps : 236900, \t Total Gen Loss : 29.9207763671875, \t Total Dis Loss : 3.86959218303673e-05\n",
      "Steps : 237000, \t Total Gen Loss : 28.526031494140625, \t Total Dis Loss : 3.701654350152239e-05\n",
      "Steps : 237100, \t Total Gen Loss : 24.88290786743164, \t Total Dis Loss : 6.324685818981379e-05\n",
      "Steps : 237200, \t Total Gen Loss : 29.055919647216797, \t Total Dis Loss : 6.243405368877575e-05\n",
      "Steps : 237300, \t Total Gen Loss : 27.780284881591797, \t Total Dis Loss : 2.5270508558605798e-05\n",
      "Steps : 237400, \t Total Gen Loss : 25.74280548095703, \t Total Dis Loss : 1.4393274796020705e-05\n",
      "Steps : 237500, \t Total Gen Loss : 26.18975257873535, \t Total Dis Loss : 0.0001398336753481999\n",
      "Steps : 237600, \t Total Gen Loss : 27.926301956176758, \t Total Dis Loss : 7.647127858945169e-06\n",
      "Steps : 237700, \t Total Gen Loss : 27.023601531982422, \t Total Dis Loss : 6.494352874142351e-06\n",
      "Steps : 237800, \t Total Gen Loss : 25.076412200927734, \t Total Dis Loss : 0.0006323212292045355\n",
      "Steps : 237900, \t Total Gen Loss : 25.244224548339844, \t Total Dis Loss : 0.000397755007725209\n",
      "Steps : 238000, \t Total Gen Loss : 26.052200317382812, \t Total Dis Loss : 8.617163075541612e-06\n",
      "Steps : 238100, \t Total Gen Loss : 28.695903778076172, \t Total Dis Loss : 0.027831345796585083\n",
      "Steps : 238200, \t Total Gen Loss : 29.261388778686523, \t Total Dis Loss : 0.00011692892439896241\n",
      "Steps : 238300, \t Total Gen Loss : 24.54849624633789, \t Total Dis Loss : 0.000980314682237804\n",
      "Steps : 238400, \t Total Gen Loss : 25.049524307250977, \t Total Dis Loss : 5.164713365957141e-05\n",
      "Steps : 238500, \t Total Gen Loss : 27.53599739074707, \t Total Dis Loss : 0.00039039173861965537\n",
      "Steps : 238600, \t Total Gen Loss : 27.857322692871094, \t Total Dis Loss : 6.904153269715607e-05\n",
      "Steps : 238700, \t Total Gen Loss : 26.870594024658203, \t Total Dis Loss : 9.058843716047704e-05\n",
      "Steps : 238800, \t Total Gen Loss : 24.82138442993164, \t Total Dis Loss : 3.724807538674213e-05\n",
      "Steps : 238900, \t Total Gen Loss : 25.297515869140625, \t Total Dis Loss : 4.6924764319555834e-05\n",
      "Steps : 239000, \t Total Gen Loss : 23.77651596069336, \t Total Dis Loss : 7.931757136248052e-05\n",
      "Steps : 239100, \t Total Gen Loss : 25.726346969604492, \t Total Dis Loss : 3.521088365232572e-05\n",
      "Steps : 239200, \t Total Gen Loss : 27.87017059326172, \t Total Dis Loss : 8.042583795031533e-05\n",
      "Steps : 239300, \t Total Gen Loss : 24.43524932861328, \t Total Dis Loss : 0.0011019842932000756\n",
      "Steps : 239400, \t Total Gen Loss : 26.693206787109375, \t Total Dis Loss : 6.470298831118271e-05\n",
      "Steps : 239500, \t Total Gen Loss : 27.29375457763672, \t Total Dis Loss : 0.00031002151081338525\n",
      "Steps : 239600, \t Total Gen Loss : 26.300209045410156, \t Total Dis Loss : 0.0014706450747326016\n",
      "Steps : 239700, \t Total Gen Loss : 28.099336624145508, \t Total Dis Loss : 0.00036115734837949276\n",
      "Steps : 239800, \t Total Gen Loss : 25.98160743713379, \t Total Dis Loss : 0.00010633770580170676\n",
      "Steps : 239900, \t Total Gen Loss : 29.534358978271484, \t Total Dis Loss : 0.00017062049300875515\n",
      "Steps : 240000, \t Total Gen Loss : 25.98126220703125, \t Total Dis Loss : 0.001756093930453062\n",
      "Steps : 240100, \t Total Gen Loss : 29.566078186035156, \t Total Dis Loss : 6.220120121724904e-05\n",
      "Steps : 240200, \t Total Gen Loss : 25.25244903564453, \t Total Dis Loss : 8.392720337724313e-05\n",
      "Steps : 240300, \t Total Gen Loss : 25.783676147460938, \t Total Dis Loss : 7.592565088998526e-05\n",
      "Steps : 240400, \t Total Gen Loss : 30.392841339111328, \t Total Dis Loss : 4.400514080771245e-05\n",
      "Steps : 240500, \t Total Gen Loss : 26.39736557006836, \t Total Dis Loss : 7.068078411975875e-05\n",
      "Steps : 240600, \t Total Gen Loss : 27.293651580810547, \t Total Dis Loss : 3.460558582446538e-05\n",
      "Steps : 240700, \t Total Gen Loss : 27.168136596679688, \t Total Dis Loss : 6.422722799470648e-05\n",
      "Steps : 240800, \t Total Gen Loss : 27.199668884277344, \t Total Dis Loss : 6.804841541452333e-05\n",
      "Steps : 240900, \t Total Gen Loss : 28.411869049072266, \t Total Dis Loss : 0.00014298464520834386\n",
      "Steps : 241000, \t Total Gen Loss : 27.155752182006836, \t Total Dis Loss : 0.00010906957322731614\n",
      "Steps : 241100, \t Total Gen Loss : 25.808692932128906, \t Total Dis Loss : 6.926509377080947e-05\n",
      "Steps : 241200, \t Total Gen Loss : 22.7908935546875, \t Total Dis Loss : 0.0003150331904180348\n",
      "Steps : 241300, \t Total Gen Loss : 26.445152282714844, \t Total Dis Loss : 0.00012642715591937304\n",
      "Steps : 241400, \t Total Gen Loss : 27.40387725830078, \t Total Dis Loss : 7.47877056710422e-05\n",
      "Steps : 241500, \t Total Gen Loss : 27.809436798095703, \t Total Dis Loss : 5.602572491625324e-05\n",
      "Steps : 241600, \t Total Gen Loss : 26.657459259033203, \t Total Dis Loss : 0.00016083208902273327\n",
      "Steps : 241700, \t Total Gen Loss : 27.76767349243164, \t Total Dis Loss : 8.814007742330432e-05\n",
      "Steps : 241800, \t Total Gen Loss : 27.25859832763672, \t Total Dis Loss : 6.198247865540907e-05\n",
      "Time for epoch 43 is 69.50210642814636 sec\n",
      "Steps : 241900, \t Total Gen Loss : 30.712879180908203, \t Total Dis Loss : 2.524559931771364e-05\n",
      "Steps : 242000, \t Total Gen Loss : 29.16535758972168, \t Total Dis Loss : 4.526754128164612e-05\n",
      "Steps : 242100, \t Total Gen Loss : 24.8737735748291, \t Total Dis Loss : 3.3915170206455514e-05\n",
      "Steps : 242200, \t Total Gen Loss : 29.111879348754883, \t Total Dis Loss : 3.512236798997037e-05\n",
      "Steps : 242300, \t Total Gen Loss : 27.21048355102539, \t Total Dis Loss : 3.0479903216473758e-05\n",
      "Steps : 242400, \t Total Gen Loss : 29.34795379638672, \t Total Dis Loss : 2.9026550691924058e-05\n",
      "Steps : 242500, \t Total Gen Loss : 27.381635665893555, \t Total Dis Loss : 1.2218529263918754e-05\n",
      "Steps : 242600, \t Total Gen Loss : 28.4859676361084, \t Total Dis Loss : 2.0010496882605366e-05\n",
      "Steps : 242700, \t Total Gen Loss : 29.70648193359375, \t Total Dis Loss : 1.6443003914901055e-05\n",
      "Steps : 242800, \t Total Gen Loss : 28.672277450561523, \t Total Dis Loss : 1.4281144103733823e-05\n",
      "Steps : 242900, \t Total Gen Loss : 29.234745025634766, \t Total Dis Loss : 1.5124198398552835e-05\n",
      "Steps : 243000, \t Total Gen Loss : 26.78917121887207, \t Total Dis Loss : 1.2493453141360078e-05\n",
      "Steps : 243100, \t Total Gen Loss : 27.789392471313477, \t Total Dis Loss : 1.1378720955690369e-05\n",
      "Steps : 243200, \t Total Gen Loss : 29.014883041381836, \t Total Dis Loss : 1.2117091500840615e-05\n",
      "Steps : 243300, \t Total Gen Loss : 29.445600509643555, \t Total Dis Loss : 1.1081980119342916e-05\n",
      "Steps : 243400, \t Total Gen Loss : 31.050731658935547, \t Total Dis Loss : 9.299836165155284e-06\n",
      "Steps : 243500, \t Total Gen Loss : 25.78866195678711, \t Total Dis Loss : 1.1835818440886214e-05\n",
      "Steps : 243600, \t Total Gen Loss : 29.10869598388672, \t Total Dis Loss : 9.641717952035833e-06\n",
      "Steps : 243700, \t Total Gen Loss : 27.989084243774414, \t Total Dis Loss : 8.197143870347645e-06\n",
      "Steps : 243800, \t Total Gen Loss : 29.53789710998535, \t Total Dis Loss : 4.108748726139311e-06\n",
      "Steps : 243900, \t Total Gen Loss : 27.6223087310791, \t Total Dis Loss : 8.05022045824444e-06\n",
      "Steps : 244000, \t Total Gen Loss : 26.733097076416016, \t Total Dis Loss : 7.3034116212511435e-06\n",
      "Steps : 244100, \t Total Gen Loss : 26.93796157836914, \t Total Dis Loss : 6.399918220267864e-06\n",
      "Steps : 244200, \t Total Gen Loss : 28.849109649658203, \t Total Dis Loss : 6.316439339570934e-06\n",
      "Steps : 244300, \t Total Gen Loss : 28.614362716674805, \t Total Dis Loss : 0.00015385952428914607\n",
      "Steps : 244400, \t Total Gen Loss : 26.74508285522461, \t Total Dis Loss : 7.730556717433501e-06\n",
      "Steps : 244500, \t Total Gen Loss : 25.44398307800293, \t Total Dis Loss : 0.00015478325076401234\n",
      "Steps : 244600, \t Total Gen Loss : 26.868165969848633, \t Total Dis Loss : 2.1138750526006334e-05\n",
      "Steps : 244700, \t Total Gen Loss : 25.907424926757812, \t Total Dis Loss : 0.00010935586760751903\n",
      "Steps : 244800, \t Total Gen Loss : 26.932785034179688, \t Total Dis Loss : 0.0024934811517596245\n",
      "Steps : 244900, \t Total Gen Loss : 27.33034896850586, \t Total Dis Loss : 2.5359504434163682e-05\n",
      "Steps : 245000, \t Total Gen Loss : 24.99300193786621, \t Total Dis Loss : 0.00012290861923247576\n",
      "Steps : 245100, \t Total Gen Loss : 27.41942024230957, \t Total Dis Loss : 5.8225239627063274e-05\n",
      "Steps : 245200, \t Total Gen Loss : 26.79214859008789, \t Total Dis Loss : 3.0215223887353204e-05\n",
      "Steps : 245300, \t Total Gen Loss : 36.54943084716797, \t Total Dis Loss : 0.0337209478020668\n",
      "Steps : 245400, \t Total Gen Loss : 22.423181533813477, \t Total Dis Loss : 0.0704500675201416\n",
      "Steps : 245500, \t Total Gen Loss : 40.87016296386719, \t Total Dis Loss : 0.005504334345459938\n",
      "Steps : 245600, \t Total Gen Loss : 33.250274658203125, \t Total Dis Loss : 0.002515649888664484\n",
      "Steps : 245700, \t Total Gen Loss : 26.681827545166016, \t Total Dis Loss : 0.08193385601043701\n",
      "Steps : 245800, \t Total Gen Loss : 26.865238189697266, \t Total Dis Loss : 0.007476930506527424\n",
      "Steps : 245900, \t Total Gen Loss : 26.596803665161133, \t Total Dis Loss : 0.005428943783044815\n",
      "Steps : 246000, \t Total Gen Loss : 30.64864730834961, \t Total Dis Loss : 0.00362205202691257\n",
      "Steps : 246100, \t Total Gen Loss : 28.689918518066406, \t Total Dis Loss : 0.007967108860611916\n",
      "Steps : 246200, \t Total Gen Loss : 41.62905502319336, \t Total Dis Loss : 0.004474303685128689\n",
      "Steps : 246300, \t Total Gen Loss : 29.287487030029297, \t Total Dis Loss : 0.01242507342249155\n",
      "Steps : 246400, \t Total Gen Loss : 34.49891662597656, \t Total Dis Loss : 0.0025811861269176006\n",
      "Steps : 246500, \t Total Gen Loss : 25.45034408569336, \t Total Dis Loss : 0.001830967958085239\n",
      "Steps : 246600, \t Total Gen Loss : 30.171146392822266, \t Total Dis Loss : 0.0009029508219100535\n",
      "Steps : 246700, \t Total Gen Loss : 32.379486083984375, \t Total Dis Loss : 0.006003062706440687\n",
      "Steps : 246800, \t Total Gen Loss : 35.221588134765625, \t Total Dis Loss : 0.00047335223644040525\n",
      "Steps : 246900, \t Total Gen Loss : 30.26886749267578, \t Total Dis Loss : 0.002953927032649517\n",
      "Steps : 247000, \t Total Gen Loss : 27.894636154174805, \t Total Dis Loss : 0.0037896183785051107\n",
      "Steps : 247100, \t Total Gen Loss : 28.772083282470703, \t Total Dis Loss : 9.818584658205509e-05\n",
      "Steps : 247200, \t Total Gen Loss : 32.27360916137695, \t Total Dis Loss : 0.0005618536379188299\n",
      "Steps : 247300, \t Total Gen Loss : 35.49324035644531, \t Total Dis Loss : 0.00015912765229586512\n",
      "Steps : 247400, \t Total Gen Loss : 26.78023338317871, \t Total Dis Loss : 0.000475345179438591\n",
      "Steps : 247500, \t Total Gen Loss : 42.77796936035156, \t Total Dis Loss : 8.386885019717738e-05\n",
      "Time for epoch 44 is 69.5097713470459 sec\n",
      "Steps : 247600, \t Total Gen Loss : 41.56134796142578, \t Total Dis Loss : 0.0072237588465213776\n",
      "Steps : 247700, \t Total Gen Loss : 44.496002197265625, \t Total Dis Loss : 0.0020221348386257887\n",
      "Steps : 247800, \t Total Gen Loss : 40.85200500488281, \t Total Dis Loss : 0.0007520733634009957\n",
      "Steps : 247900, \t Total Gen Loss : 36.709774017333984, \t Total Dis Loss : 0.0004167846927884966\n",
      "Steps : 248000, \t Total Gen Loss : 32.76667022705078, \t Total Dis Loss : 0.0009076594724319875\n",
      "Steps : 248100, \t Total Gen Loss : 35.42741394042969, \t Total Dis Loss : 0.00019458009046502411\n",
      "Steps : 248200, \t Total Gen Loss : 37.885032653808594, \t Total Dis Loss : 0.0007815456483513117\n",
      "Steps : 248300, \t Total Gen Loss : 33.70909118652344, \t Total Dis Loss : 0.3281726837158203\n",
      "Steps : 248400, \t Total Gen Loss : 35.51365280151367, \t Total Dis Loss : 0.0004823640629183501\n",
      "Steps : 248500, \t Total Gen Loss : 28.544841766357422, \t Total Dis Loss : 0.0006290288874879479\n",
      "Steps : 248600, \t Total Gen Loss : 35.49886703491211, \t Total Dis Loss : 0.0035777101293206215\n",
      "Steps : 248700, \t Total Gen Loss : 34.70849609375, \t Total Dis Loss : 0.02168659307062626\n",
      "Steps : 248800, \t Total Gen Loss : 28.75554656982422, \t Total Dis Loss : 0.14247596263885498\n",
      "Steps : 248900, \t Total Gen Loss : 33.047672271728516, \t Total Dis Loss : 0.0013391340617090464\n",
      "Steps : 249000, \t Total Gen Loss : 32.81149673461914, \t Total Dis Loss : 0.0004544208641164005\n",
      "Steps : 249100, \t Total Gen Loss : 39.46919631958008, \t Total Dis Loss : 0.002805087948217988\n",
      "Steps : 249200, \t Total Gen Loss : 37.35136413574219, \t Total Dis Loss : 0.0014096571831032634\n",
      "Steps : 249300, \t Total Gen Loss : 37.87897491455078, \t Total Dis Loss : 0.11113111674785614\n",
      "Steps : 249400, \t Total Gen Loss : 42.642547607421875, \t Total Dis Loss : 0.00014571085921488702\n",
      "Steps : 249500, \t Total Gen Loss : 39.53538513183594, \t Total Dis Loss : 0.00010938880586763844\n",
      "Steps : 249600, \t Total Gen Loss : 36.964046478271484, \t Total Dis Loss : 0.00010823587217601016\n",
      "Steps : 249700, \t Total Gen Loss : 32.47088623046875, \t Total Dis Loss : 0.006614827550947666\n",
      "Steps : 249800, \t Total Gen Loss : 35.28411102294922, \t Total Dis Loss : 0.0007604266284033656\n",
      "Steps : 249900, \t Total Gen Loss : 33.39080047607422, \t Total Dis Loss : 0.0004813684499822557\n",
      "Steps : 250000, \t Total Gen Loss : 39.07066345214844, \t Total Dis Loss : 0.00048179071745835245\n",
      "Steps : 250100, \t Total Gen Loss : 34.422611236572266, \t Total Dis Loss : 0.0689527839422226\n",
      "Steps : 250200, \t Total Gen Loss : 34.298683166503906, \t Total Dis Loss : 0.00029266771161928773\n",
      "Steps : 250300, \t Total Gen Loss : 30.006126403808594, \t Total Dis Loss : 0.002343748463317752\n",
      "Steps : 250400, \t Total Gen Loss : 33.33525085449219, \t Total Dis Loss : 6.694185140077025e-05\n",
      "Steps : 250500, \t Total Gen Loss : 32.40406799316406, \t Total Dis Loss : 3.206841938663274e-05\n",
      "Steps : 250600, \t Total Gen Loss : 30.85869026184082, \t Total Dis Loss : 0.00014207849744707346\n",
      "Steps : 250700, \t Total Gen Loss : 31.370075225830078, \t Total Dis Loss : 0.0005433196783997118\n",
      "Steps : 250800, \t Total Gen Loss : 33.7625732421875, \t Total Dis Loss : 0.000430966611020267\n",
      "Steps : 250900, \t Total Gen Loss : 29.875167846679688, \t Total Dis Loss : 0.0006489691440947354\n",
      "Steps : 251000, \t Total Gen Loss : 34.0278205871582, \t Total Dis Loss : 0.0001309160579694435\n",
      "Steps : 251100, \t Total Gen Loss : 32.92591094970703, \t Total Dis Loss : 0.008461641147732735\n",
      "Steps : 251200, \t Total Gen Loss : 37.72870635986328, \t Total Dis Loss : 0.0009906201157718897\n",
      "Steps : 251300, \t Total Gen Loss : 37.96397018432617, \t Total Dis Loss : 5.594161484623328e-05\n",
      "Steps : 251400, \t Total Gen Loss : 47.42692947387695, \t Total Dis Loss : 0.00012462635640986264\n",
      "Steps : 251500, \t Total Gen Loss : 31.910629272460938, \t Total Dis Loss : 0.0003529749810695648\n",
      "Steps : 251600, \t Total Gen Loss : 30.615888595581055, \t Total Dis Loss : 7.234686199808493e-05\n",
      "Steps : 251700, \t Total Gen Loss : 30.82076644897461, \t Total Dis Loss : 0.0008494854555465281\n",
      "Steps : 251800, \t Total Gen Loss : 32.40025329589844, \t Total Dis Loss : 0.0017078531673178077\n",
      "Steps : 251900, \t Total Gen Loss : 31.470073699951172, \t Total Dis Loss : 0.00037612661253660917\n",
      "Steps : 252000, \t Total Gen Loss : 35.920326232910156, \t Total Dis Loss : 0.00024053735251072794\n",
      "Steps : 252100, \t Total Gen Loss : 33.887237548828125, \t Total Dis Loss : 0.0005772585864178836\n",
      "Steps : 252200, \t Total Gen Loss : 37.55952835083008, \t Total Dis Loss : 0.023936716839671135\n",
      "Steps : 252300, \t Total Gen Loss : 31.997276306152344, \t Total Dis Loss : 0.0019205366261303425\n",
      "Steps : 252400, \t Total Gen Loss : 36.35166549682617, \t Total Dis Loss : 0.001141032320447266\n",
      "Steps : 252500, \t Total Gen Loss : 32.547725677490234, \t Total Dis Loss : 0.0005907554877921939\n",
      "Steps : 252600, \t Total Gen Loss : 32.3927001953125, \t Total Dis Loss : 0.00016043716459535062\n",
      "Steps : 252700, \t Total Gen Loss : 36.0370979309082, \t Total Dis Loss : 0.0029508578591048717\n",
      "Steps : 252800, \t Total Gen Loss : 39.57402420043945, \t Total Dis Loss : 0.00012434787640813738\n",
      "Steps : 252900, \t Total Gen Loss : 33.176063537597656, \t Total Dis Loss : 0.0001553251058794558\n",
      "Steps : 253000, \t Total Gen Loss : 39.05508804321289, \t Total Dis Loss : 0.0011335947783663869\n",
      "Steps : 253100, \t Total Gen Loss : 33.54127502441406, \t Total Dis Loss : 0.0005562184960581362\n",
      "Time for epoch 45 is 70.39217376708984 sec\n",
      "Steps : 253200, \t Total Gen Loss : 32.355247497558594, \t Total Dis Loss : 0.001697349245660007\n",
      "Steps : 253300, \t Total Gen Loss : 37.529632568359375, \t Total Dis Loss : 2.4541590391891077e-05\n",
      "Steps : 253400, \t Total Gen Loss : 34.920814514160156, \t Total Dis Loss : 0.0006504360353574157\n",
      "Steps : 253500, \t Total Gen Loss : 32.76771926879883, \t Total Dis Loss : 0.0002504422445781529\n",
      "Steps : 253600, \t Total Gen Loss : 31.148487091064453, \t Total Dis Loss : 0.001352754537947476\n",
      "Steps : 253700, \t Total Gen Loss : 34.13676834106445, \t Total Dis Loss : 0.0002536271349526942\n",
      "Steps : 253800, \t Total Gen Loss : 39.29572296142578, \t Total Dis Loss : 0.0006220953655429184\n",
      "Steps : 253900, \t Total Gen Loss : 35.891357421875, \t Total Dis Loss : 0.00013540500367525965\n",
      "Steps : 254000, \t Total Gen Loss : 33.02376174926758, \t Total Dis Loss : 0.0001017865288304165\n",
      "Steps : 254100, \t Total Gen Loss : 36.217559814453125, \t Total Dis Loss : 4.1861116187646985e-05\n",
      "Steps : 254200, \t Total Gen Loss : 40.84916305541992, \t Total Dis Loss : 0.0004052440926898271\n",
      "Steps : 254300, \t Total Gen Loss : 34.034263610839844, \t Total Dis Loss : 0.00035094653139822185\n",
      "Steps : 254400, \t Total Gen Loss : 36.058937072753906, \t Total Dis Loss : 0.0005077853566035628\n",
      "Steps : 254500, \t Total Gen Loss : 31.601076126098633, \t Total Dis Loss : 0.0007243981817737222\n",
      "Steps : 254600, \t Total Gen Loss : 31.79238510131836, \t Total Dis Loss : 0.00011454135528765619\n",
      "Steps : 254700, \t Total Gen Loss : 41.307708740234375, \t Total Dis Loss : 0.00014207525236997753\n",
      "Steps : 254800, \t Total Gen Loss : 34.20978927612305, \t Total Dis Loss : 8.068959868978709e-05\n",
      "Steps : 254900, \t Total Gen Loss : 34.613590240478516, \t Total Dis Loss : 0.000591019110288471\n",
      "Steps : 255000, \t Total Gen Loss : 31.714101791381836, \t Total Dis Loss : 2.9815491870976985e-05\n",
      "Steps : 255100, \t Total Gen Loss : 37.74103546142578, \t Total Dis Loss : 5.7419834774918854e-05\n",
      "Steps : 255200, \t Total Gen Loss : 37.865482330322266, \t Total Dis Loss : 4.629981049220078e-05\n",
      "Steps : 255300, \t Total Gen Loss : 33.056766510009766, \t Total Dis Loss : 0.0002845225390046835\n",
      "Steps : 255400, \t Total Gen Loss : 33.30488204956055, \t Total Dis Loss : 0.0002885187859646976\n",
      "Steps : 255500, \t Total Gen Loss : 37.02709197998047, \t Total Dis Loss : 0.00020806923566851765\n",
      "Steps : 255600, \t Total Gen Loss : 29.887985229492188, \t Total Dis Loss : 5.424684059107676e-05\n",
      "Steps : 255700, \t Total Gen Loss : 29.882396697998047, \t Total Dis Loss : 0.0001485878019593656\n",
      "Steps : 255800, \t Total Gen Loss : 31.55051040649414, \t Total Dis Loss : 0.0003606446261983365\n",
      "Steps : 255900, \t Total Gen Loss : 34.082794189453125, \t Total Dis Loss : 2.6964318749378435e-05\n",
      "Steps : 256000, \t Total Gen Loss : 27.402324676513672, \t Total Dis Loss : 4.9966165533987805e-05\n",
      "Steps : 256100, \t Total Gen Loss : 28.920751571655273, \t Total Dis Loss : 0.0004463121003936976\n",
      "Steps : 256200, \t Total Gen Loss : 37.339595794677734, \t Total Dis Loss : 2.919076541729737e-05\n",
      "Steps : 256300, \t Total Gen Loss : 36.19105529785156, \t Total Dis Loss : 3.849773293040926e-06\n",
      "Steps : 256400, \t Total Gen Loss : 36.51690673828125, \t Total Dis Loss : 1.9950426576542668e-05\n",
      "Steps : 256500, \t Total Gen Loss : 38.66648864746094, \t Total Dis Loss : 0.039671506732702255\n",
      "Steps : 256600, \t Total Gen Loss : 40.228309631347656, \t Total Dis Loss : 0.0006384534644894302\n",
      "Steps : 256700, \t Total Gen Loss : 41.0828857421875, \t Total Dis Loss : 4.782499672728591e-05\n",
      "Steps : 256800, \t Total Gen Loss : 35.11795425415039, \t Total Dis Loss : 1.8361532056587748e-05\n",
      "Steps : 256900, \t Total Gen Loss : 32.508235931396484, \t Total Dis Loss : 8.630138836451806e-06\n",
      "Steps : 257000, \t Total Gen Loss : 38.368133544921875, \t Total Dis Loss : 0.00015109337982721627\n",
      "Steps : 257100, \t Total Gen Loss : 33.242801666259766, \t Total Dis Loss : 0.0002790187136270106\n",
      "Steps : 257200, \t Total Gen Loss : 32.81157684326172, \t Total Dis Loss : 0.0002298333856742829\n",
      "Steps : 257300, \t Total Gen Loss : 29.903099060058594, \t Total Dis Loss : 0.00013721204595640302\n",
      "Steps : 257400, \t Total Gen Loss : 26.890399932861328, \t Total Dis Loss : 0.000825678464025259\n",
      "Steps : 257500, \t Total Gen Loss : 28.959360122680664, \t Total Dis Loss : 0.0012644440867006779\n",
      "Steps : 257600, \t Total Gen Loss : 30.319652557373047, \t Total Dis Loss : 0.000595662510022521\n",
      "Steps : 257700, \t Total Gen Loss : 34.522132873535156, \t Total Dis Loss : 0.00041955747292377055\n",
      "Steps : 257800, \t Total Gen Loss : 30.48686408996582, \t Total Dis Loss : 0.0003039677976630628\n",
      "Steps : 257900, \t Total Gen Loss : 32.48059844970703, \t Total Dis Loss : 0.00014015252236276865\n",
      "Steps : 258000, \t Total Gen Loss : 31.678850173950195, \t Total Dis Loss : 0.00016348565986845642\n",
      "Steps : 258100, \t Total Gen Loss : 34.911537170410156, \t Total Dis Loss : 0.0001197779129142873\n",
      "Steps : 258200, \t Total Gen Loss : 29.587581634521484, \t Total Dis Loss : 0.00013782463793177158\n",
      "Steps : 258300, \t Total Gen Loss : 29.56436538696289, \t Total Dis Loss : 5.7627294154372066e-05\n",
      "Steps : 258400, \t Total Gen Loss : 27.548404693603516, \t Total Dis Loss : 0.00029783337959088385\n",
      "Steps : 258500, \t Total Gen Loss : 30.447681427001953, \t Total Dis Loss : 0.00010718958219513297\n",
      "Steps : 258600, \t Total Gen Loss : 29.87480926513672, \t Total Dis Loss : 8.665167115395889e-05\n",
      "Steps : 258700, \t Total Gen Loss : 31.59975242614746, \t Total Dis Loss : 5.067612437414937e-05\n",
      "Time for epoch 46 is 69.36293697357178 sec\n",
      "Steps : 258800, \t Total Gen Loss : 35.180580139160156, \t Total Dis Loss : 7.633279165020213e-05\n",
      "Steps : 258900, \t Total Gen Loss : 32.273887634277344, \t Total Dis Loss : 0.000311491807224229\n",
      "Steps : 259000, \t Total Gen Loss : 30.798358917236328, \t Total Dis Loss : 0.000497560715302825\n",
      "Steps : 259100, \t Total Gen Loss : 25.828216552734375, \t Total Dis Loss : 0.0002509922196622938\n",
      "Steps : 259200, \t Total Gen Loss : 27.703397750854492, \t Total Dis Loss : 0.0005727616371586919\n",
      "Steps : 259300, \t Total Gen Loss : 28.42913818359375, \t Total Dis Loss : 0.0004786689532920718\n",
      "Steps : 259400, \t Total Gen Loss : 30.37097930908203, \t Total Dis Loss : 6.251790910027921e-05\n",
      "Steps : 259500, \t Total Gen Loss : 30.88003921508789, \t Total Dis Loss : 6.0871068853884935e-05\n",
      "Steps : 259600, \t Total Gen Loss : 33.36768341064453, \t Total Dis Loss : 0.00010022890637628734\n",
      "Steps : 259700, \t Total Gen Loss : 33.639312744140625, \t Total Dis Loss : 1.788799090718385e-05\n",
      "Steps : 259800, \t Total Gen Loss : 30.567733764648438, \t Total Dis Loss : 6.124706123955548e-05\n",
      "Steps : 259900, \t Total Gen Loss : 31.08040428161621, \t Total Dis Loss : 8.074002107605338e-05\n",
      "Steps : 260000, \t Total Gen Loss : 32.589866638183594, \t Total Dis Loss : 7.491095311706886e-05\n",
      "Steps : 260100, \t Total Gen Loss : 34.32001876831055, \t Total Dis Loss : 0.0007291762158274651\n",
      "Steps : 260200, \t Total Gen Loss : 39.639312744140625, \t Total Dis Loss : 0.001494947005994618\n",
      "Steps : 260300, \t Total Gen Loss : 32.60988998413086, \t Total Dis Loss : 0.00656762532889843\n",
      "Steps : 260400, \t Total Gen Loss : 36.450653076171875, \t Total Dis Loss : 0.00016432396660093218\n",
      "Steps : 260500, \t Total Gen Loss : 32.191688537597656, \t Total Dis Loss : 0.00026272982358932495\n",
      "Steps : 260600, \t Total Gen Loss : 31.78786849975586, \t Total Dis Loss : 0.0006269152509048581\n",
      "Steps : 260700, \t Total Gen Loss : 31.564960479736328, \t Total Dis Loss : 9.803070861380547e-05\n",
      "Steps : 260800, \t Total Gen Loss : 31.130002975463867, \t Total Dis Loss : 0.0005196977872401476\n",
      "Steps : 260900, \t Total Gen Loss : 29.392826080322266, \t Total Dis Loss : 9.403405420016497e-05\n",
      "Steps : 261000, \t Total Gen Loss : 29.863903045654297, \t Total Dis Loss : 7.368040678557009e-05\n",
      "Steps : 261100, \t Total Gen Loss : 34.62430191040039, \t Total Dis Loss : 0.00010138236393686384\n",
      "Steps : 261200, \t Total Gen Loss : 29.93136215209961, \t Total Dis Loss : 0.0027745473198592663\n",
      "Steps : 261300, \t Total Gen Loss : 36.945068359375, \t Total Dis Loss : 0.00033346310374327004\n",
      "Steps : 261400, \t Total Gen Loss : 29.803123474121094, \t Total Dis Loss : 0.00011701349285431206\n",
      "Steps : 261500, \t Total Gen Loss : 36.292930603027344, \t Total Dis Loss : 0.004608997143805027\n",
      "Steps : 261600, \t Total Gen Loss : 43.98207092285156, \t Total Dis Loss : 0.0006205694517120719\n",
      "Steps : 261700, \t Total Gen Loss : 33.62371063232422, \t Total Dis Loss : 9.88831106951693e-06\n",
      "Steps : 261800, \t Total Gen Loss : 35.124603271484375, \t Total Dis Loss : 2.5039680622285232e-05\n",
      "Steps : 261900, \t Total Gen Loss : 33.16300964355469, \t Total Dis Loss : 1.410886034136638e-05\n",
      "Steps : 262000, \t Total Gen Loss : 37.25995635986328, \t Total Dis Loss : 5.640350082103396e-06\n",
      "Steps : 262100, \t Total Gen Loss : 39.27192687988281, \t Total Dis Loss : 5.6485412642359734e-05\n",
      "Steps : 262200, \t Total Gen Loss : 38.83332061767578, \t Total Dis Loss : 3.52536553691607e-05\n",
      "Steps : 262300, \t Total Gen Loss : 35.39525604248047, \t Total Dis Loss : 3.200786159140989e-05\n",
      "Steps : 262400, \t Total Gen Loss : 38.07868194580078, \t Total Dis Loss : 0.000158249051310122\n",
      "Steps : 262500, \t Total Gen Loss : 34.29124450683594, \t Total Dis Loss : 0.00041721982415765524\n",
      "Steps : 262600, \t Total Gen Loss : 37.07075500488281, \t Total Dis Loss : 0.001665350398980081\n",
      "Steps : 262700, \t Total Gen Loss : 30.706703186035156, \t Total Dis Loss : 5.7153090892825276e-05\n",
      "Steps : 262800, \t Total Gen Loss : 37.31462097167969, \t Total Dis Loss : 0.00022298206749837846\n",
      "Steps : 262900, \t Total Gen Loss : 30.765954971313477, \t Total Dis Loss : 0.0018277675844728947\n",
      "Steps : 263000, \t Total Gen Loss : 35.67961120605469, \t Total Dis Loss : 4.448803338163998e-06\n",
      "Steps : 263100, \t Total Gen Loss : 36.47655487060547, \t Total Dis Loss : 0.00017069830209948123\n",
      "Steps : 263200, \t Total Gen Loss : 33.427520751953125, \t Total Dis Loss : 1.8720413208939135e-05\n",
      "Steps : 263300, \t Total Gen Loss : 36.16284942626953, \t Total Dis Loss : 0.0018230124842375517\n",
      "Steps : 263400, \t Total Gen Loss : 29.503856658935547, \t Total Dis Loss : 8.830272236082237e-06\n",
      "Steps : 263500, \t Total Gen Loss : 26.446043014526367, \t Total Dis Loss : 0.0008452614420093596\n",
      "Steps : 263600, \t Total Gen Loss : 29.87609100341797, \t Total Dis Loss : 0.001791215967386961\n",
      "Steps : 263700, \t Total Gen Loss : 29.603641510009766, \t Total Dis Loss : 2.7438189135864377e-05\n",
      "Steps : 263800, \t Total Gen Loss : 31.867229461669922, \t Total Dis Loss : 4.123258804611396e-06\n",
      "Steps : 263900, \t Total Gen Loss : 33.517356872558594, \t Total Dis Loss : 1.271088513021823e-05\n",
      "Steps : 264000, \t Total Gen Loss : 27.11148452758789, \t Total Dis Loss : 7.480781641788781e-05\n",
      "Steps : 264100, \t Total Gen Loss : 32.096351623535156, \t Total Dis Loss : 1.6077610780484974e-05\n",
      "Steps : 264200, \t Total Gen Loss : 28.80682373046875, \t Total Dis Loss : 8.750121196499094e-05\n",
      "Steps : 264300, \t Total Gen Loss : 31.273059844970703, \t Total Dis Loss : 3.5907156416215e-05\n",
      "Time for epoch 47 is 69.31578803062439 sec\n",
      "Steps : 264400, \t Total Gen Loss : 28.382946014404297, \t Total Dis Loss : 0.00034078164026141167\n",
      "Steps : 264500, \t Total Gen Loss : 28.223615646362305, \t Total Dis Loss : 0.00018789913156069815\n",
      "Steps : 264600, \t Total Gen Loss : 30.07293701171875, \t Total Dis Loss : 0.0002228309604106471\n",
      "Steps : 264700, \t Total Gen Loss : 26.084625244140625, \t Total Dis Loss : 0.00021802177070640028\n",
      "Steps : 264800, \t Total Gen Loss : 33.39729309082031, \t Total Dis Loss : 0.00016151214367710054\n",
      "Steps : 264900, \t Total Gen Loss : 30.941162109375, \t Total Dis Loss : 0.0011203495087102056\n",
      "Steps : 265000, \t Total Gen Loss : 31.202655792236328, \t Total Dis Loss : 0.0008719311445020139\n",
      "Steps : 265100, \t Total Gen Loss : 27.986248016357422, \t Total Dis Loss : 7.860839104978368e-05\n",
      "Steps : 265200, \t Total Gen Loss : 33.935508728027344, \t Total Dis Loss : 0.00019889147370122373\n",
      "Steps : 265300, \t Total Gen Loss : 34.343353271484375, \t Total Dis Loss : 2.4047647457337007e-05\n",
      "Steps : 265400, \t Total Gen Loss : 29.782432556152344, \t Total Dis Loss : 0.027525028213858604\n",
      "Steps : 265500, \t Total Gen Loss : 38.181522369384766, \t Total Dis Loss : 3.8998405216261744e-05\n",
      "Steps : 265600, \t Total Gen Loss : 31.093860626220703, \t Total Dis Loss : 0.00444445013999939\n",
      "Steps : 265700, \t Total Gen Loss : 37.8296012878418, \t Total Dis Loss : 0.0009453374077565968\n",
      "Steps : 265800, \t Total Gen Loss : 32.87901306152344, \t Total Dis Loss : 0.0002427237486699596\n",
      "Steps : 265900, \t Total Gen Loss : 29.32196807861328, \t Total Dis Loss : 8.694848656887189e-05\n",
      "Steps : 266000, \t Total Gen Loss : 40.86997985839844, \t Total Dis Loss : 2.5327484763693064e-05\n",
      "Steps : 266100, \t Total Gen Loss : 37.172752380371094, \t Total Dis Loss : 4.026022361358628e-05\n",
      "Steps : 266200, \t Total Gen Loss : 39.843849182128906, \t Total Dis Loss : 0.0023620459251105785\n",
      "Steps : 266300, \t Total Gen Loss : 36.97930908203125, \t Total Dis Loss : 0.0018639141926541924\n",
      "Steps : 266400, \t Total Gen Loss : 34.842987060546875, \t Total Dis Loss : 2.41325433307793e-05\n",
      "Steps : 266500, \t Total Gen Loss : 38.340858459472656, \t Total Dis Loss : 8.042680565267801e-05\n",
      "Steps : 266600, \t Total Gen Loss : 40.92210388183594, \t Total Dis Loss : 0.0002462708798702806\n",
      "Steps : 266700, \t Total Gen Loss : 38.66779327392578, \t Total Dis Loss : 3.120600013062358e-05\n",
      "Steps : 266800, \t Total Gen Loss : 36.96358871459961, \t Total Dis Loss : 0.001863038633018732\n",
      "Steps : 266900, \t Total Gen Loss : 32.355411529541016, \t Total Dis Loss : 0.0010305214673280716\n",
      "Steps : 267000, \t Total Gen Loss : 33.83961486816406, \t Total Dis Loss : 0.00028416907298378646\n",
      "Steps : 267100, \t Total Gen Loss : 36.68549346923828, \t Total Dis Loss : 5.58403626200743e-05\n",
      "Steps : 267200, \t Total Gen Loss : 38.18661117553711, \t Total Dis Loss : 0.000306329398881644\n",
      "Steps : 267300, \t Total Gen Loss : 27.45651626586914, \t Total Dis Loss : 6.240016227820888e-05\n",
      "Steps : 267400, \t Total Gen Loss : 28.5311279296875, \t Total Dis Loss : 9.455361578147858e-05\n",
      "Steps : 267500, \t Total Gen Loss : 31.301040649414062, \t Total Dis Loss : 0.000134057758259587\n",
      "Steps : 267600, \t Total Gen Loss : 30.619848251342773, \t Total Dis Loss : 5.476103979162872e-05\n",
      "Steps : 267700, \t Total Gen Loss : 32.379276275634766, \t Total Dis Loss : 0.0002453627821523696\n",
      "Steps : 267800, \t Total Gen Loss : 34.166873931884766, \t Total Dis Loss : 0.00010012168786488473\n",
      "Steps : 267900, \t Total Gen Loss : 32.01243209838867, \t Total Dis Loss : 4.964459367329255e-05\n",
      "Steps : 268000, \t Total Gen Loss : 30.224132537841797, \t Total Dis Loss : 0.0003153911093249917\n",
      "Steps : 268100, \t Total Gen Loss : 39.86566925048828, \t Total Dis Loss : 0.00023692601826041937\n",
      "Steps : 268200, \t Total Gen Loss : 30.579856872558594, \t Total Dis Loss : 0.00037843125755898654\n",
      "Steps : 268300, \t Total Gen Loss : 36.64913558959961, \t Total Dis Loss : 3.922272298950702e-05\n",
      "Steps : 268400, \t Total Gen Loss : 30.360889434814453, \t Total Dis Loss : 0.0025998824276030064\n",
      "Steps : 268500, \t Total Gen Loss : 30.685976028442383, \t Total Dis Loss : 0.0010559416841715574\n",
      "Steps : 268600, \t Total Gen Loss : 31.71659278869629, \t Total Dis Loss : 0.0005611160886473954\n",
      "Steps : 268700, \t Total Gen Loss : 40.16318893432617, \t Total Dis Loss : 0.0018907953053712845\n",
      "Steps : 268800, \t Total Gen Loss : 34.60831832885742, \t Total Dis Loss : 0.000488908844999969\n",
      "Steps : 268900, \t Total Gen Loss : 34.47765350341797, \t Total Dis Loss : 3.7620924558723345e-05\n",
      "Steps : 269000, \t Total Gen Loss : 36.95782470703125, \t Total Dis Loss : 4.050039206049405e-05\n",
      "Steps : 269100, \t Total Gen Loss : 40.96900177001953, \t Total Dis Loss : 0.015268407762050629\n",
      "Steps : 269200, \t Total Gen Loss : 42.290382385253906, \t Total Dis Loss : 0.0005891154869459569\n",
      "Steps : 269300, \t Total Gen Loss : 39.32096862792969, \t Total Dis Loss : 0.00021302507957443595\n",
      "Steps : 269400, \t Total Gen Loss : 35.09229278564453, \t Total Dis Loss : 9.00038139661774e-06\n",
      "Steps : 269500, \t Total Gen Loss : 28.37670135498047, \t Total Dis Loss : 0.00016848706582095474\n",
      "Steps : 269600, \t Total Gen Loss : 34.95433807373047, \t Total Dis Loss : 4.029819319839589e-05\n",
      "Steps : 269700, \t Total Gen Loss : 35.023292541503906, \t Total Dis Loss : 2.3052998585626483e-05\n",
      "Steps : 269800, \t Total Gen Loss : 38.95704650878906, \t Total Dis Loss : 0.0006042586755938828\n",
      "Steps : 269900, \t Total Gen Loss : 35.65184783935547, \t Total Dis Loss : 2.4473290977766737e-05\n",
      "Steps : 270000, \t Total Gen Loss : 37.18677520751953, \t Total Dis Loss : 0.00011134707892779261\n",
      "Time for epoch 48 is 69.35772609710693 sec\n",
      "Steps : 270100, \t Total Gen Loss : 38.327877044677734, \t Total Dis Loss : 0.00020221441809553653\n",
      "Steps : 270200, \t Total Gen Loss : 33.90850067138672, \t Total Dis Loss : 0.00039415108039975166\n",
      "Steps : 270300, \t Total Gen Loss : 33.40566635131836, \t Total Dis Loss : 0.000851925928145647\n",
      "Steps : 270400, \t Total Gen Loss : 34.48343276977539, \t Total Dis Loss : 0.00010966411355184391\n",
      "Steps : 270500, \t Total Gen Loss : 36.13460159301758, \t Total Dis Loss : 0.00047495358739979565\n",
      "Steps : 270600, \t Total Gen Loss : 33.77680969238281, \t Total Dis Loss : 0.00012117938604205847\n",
      "Steps : 270700, \t Total Gen Loss : 36.11028289794922, \t Total Dis Loss : 0.0006883494206704199\n",
      "Steps : 270800, \t Total Gen Loss : 35.824485778808594, \t Total Dis Loss : 0.0001547302381368354\n",
      "Steps : 270900, \t Total Gen Loss : 38.1331787109375, \t Total Dis Loss : 0.00015465350588783622\n",
      "Steps : 271000, \t Total Gen Loss : 33.80191421508789, \t Total Dis Loss : 0.00025148666463792324\n",
      "Steps : 271100, \t Total Gen Loss : 29.28315544128418, \t Total Dis Loss : 0.00012030493962811306\n",
      "Steps : 271200, \t Total Gen Loss : 29.15500259399414, \t Total Dis Loss : 0.00015168987738434225\n",
      "Steps : 271300, \t Total Gen Loss : 28.917037963867188, \t Total Dis Loss : 0.00041489506838843226\n",
      "Steps : 271400, \t Total Gen Loss : 31.674013137817383, \t Total Dis Loss : 0.0012134825810790062\n",
      "Steps : 271500, \t Total Gen Loss : 29.583572387695312, \t Total Dis Loss : 3.4138291084673256e-05\n",
      "Steps : 271600, \t Total Gen Loss : 30.020572662353516, \t Total Dis Loss : 3.180540443281643e-05\n",
      "Steps : 271700, \t Total Gen Loss : 30.90418243408203, \t Total Dis Loss : 1.9979679564130493e-05\n",
      "Steps : 271800, \t Total Gen Loss : 30.5594482421875, \t Total Dis Loss : 2.1766611098428257e-05\n",
      "Steps : 271900, \t Total Gen Loss : 30.036392211914062, \t Total Dis Loss : 2.9684342734981328e-05\n",
      "Steps : 272000, \t Total Gen Loss : 31.400676727294922, \t Total Dis Loss : 2.534136910981033e-05\n",
      "Steps : 272100, \t Total Gen Loss : 31.147005081176758, \t Total Dis Loss : 4.864474249188788e-05\n",
      "Steps : 272200, \t Total Gen Loss : 27.86510467529297, \t Total Dis Loss : 3.511211252771318e-05\n",
      "Steps : 272300, \t Total Gen Loss : 31.298147201538086, \t Total Dis Loss : 3.478800135781057e-05\n",
      "Steps : 272400, \t Total Gen Loss : 29.74555206298828, \t Total Dis Loss : 4.37841190432664e-05\n",
      "Steps : 272500, \t Total Gen Loss : 30.18914794921875, \t Total Dis Loss : 1.9402141333557665e-05\n",
      "Steps : 272600, \t Total Gen Loss : 29.23824119567871, \t Total Dis Loss : 0.00028716825181618333\n",
      "Steps : 272700, \t Total Gen Loss : 29.28811264038086, \t Total Dis Loss : 0.0001134872727561742\n",
      "Steps : 272800, \t Total Gen Loss : 29.37630844116211, \t Total Dis Loss : 9.024020255310461e-05\n",
      "Steps : 272900, \t Total Gen Loss : 29.655351638793945, \t Total Dis Loss : 4.407172309583984e-05\n",
      "Steps : 273000, \t Total Gen Loss : 26.100311279296875, \t Total Dis Loss : 0.0001419896725565195\n",
      "Steps : 273100, \t Total Gen Loss : 34.70808410644531, \t Total Dis Loss : 0.0002948896726593375\n",
      "Steps : 273200, \t Total Gen Loss : 37.46269226074219, \t Total Dis Loss : 0.00047964107943698764\n",
      "Steps : 273300, \t Total Gen Loss : 33.692604064941406, \t Total Dis Loss : 4.4573869672603905e-05\n",
      "Steps : 273400, \t Total Gen Loss : 31.874168395996094, \t Total Dis Loss : 2.1899631974520162e-05\n",
      "Steps : 273500, \t Total Gen Loss : 31.520143508911133, \t Total Dis Loss : 0.00032582134008407593\n",
      "Steps : 273600, \t Total Gen Loss : 29.332584381103516, \t Total Dis Loss : 0.0008423909894190729\n",
      "Steps : 273700, \t Total Gen Loss : 29.0604305267334, \t Total Dis Loss : 0.00013884692452847958\n",
      "Steps : 273800, \t Total Gen Loss : 35.70005416870117, \t Total Dis Loss : 5.1484057621564716e-05\n",
      "Steps : 273900, \t Total Gen Loss : 40.13675308227539, \t Total Dis Loss : 6.041298547643237e-06\n",
      "Steps : 274000, \t Total Gen Loss : 30.460298538208008, \t Total Dis Loss : 3.093081613769755e-05\n",
      "Steps : 274100, \t Total Gen Loss : 36.423675537109375, \t Total Dis Loss : 1.9714732843567617e-05\n",
      "Steps : 274200, \t Total Gen Loss : 31.407489776611328, \t Total Dis Loss : 0.00017367239343002439\n",
      "Steps : 274300, \t Total Gen Loss : 33.00126647949219, \t Total Dis Loss : 0.0020384739618748426\n",
      "Steps : 274400, \t Total Gen Loss : 29.486297607421875, \t Total Dis Loss : 0.00030014276853762567\n",
      "Steps : 274500, \t Total Gen Loss : 40.28755187988281, \t Total Dis Loss : 3.0137824069242924e-05\n",
      "Steps : 274600, \t Total Gen Loss : 34.78687286376953, \t Total Dis Loss : 0.0028568971902132034\n",
      "Steps : 274700, \t Total Gen Loss : 35.90525817871094, \t Total Dis Loss : 3.435793041717261e-05\n",
      "Steps : 274800, \t Total Gen Loss : 33.35521697998047, \t Total Dis Loss : 0.0002492084458936006\n",
      "Steps : 274900, \t Total Gen Loss : 32.8353271484375, \t Total Dis Loss : 0.0005015420611016452\n",
      "Steps : 275000, \t Total Gen Loss : 35.503700256347656, \t Total Dis Loss : 0.00010127108544111252\n",
      "Steps : 275100, \t Total Gen Loss : 34.79960632324219, \t Total Dis Loss : 8.85536428540945e-05\n",
      "Steps : 275200, \t Total Gen Loss : 31.78605079650879, \t Total Dis Loss : 0.00013304945605341345\n",
      "Steps : 275300, \t Total Gen Loss : 35.58916473388672, \t Total Dis Loss : 0.00011738983448594809\n",
      "Steps : 275400, \t Total Gen Loss : 34.21917724609375, \t Total Dis Loss : 0.001063457690179348\n",
      "Steps : 275500, \t Total Gen Loss : 36.322853088378906, \t Total Dis Loss : 7.033880683593452e-05\n",
      "Steps : 275600, \t Total Gen Loss : 30.51038932800293, \t Total Dis Loss : 0.00019339847494848073\n",
      "Time for epoch 49 is 69.32364058494568 sec\n",
      "Steps : 275700, \t Total Gen Loss : 26.859434127807617, \t Total Dis Loss : 0.000200608788873069\n",
      "Steps : 275800, \t Total Gen Loss : 30.30572509765625, \t Total Dis Loss : 0.00013591420429293066\n",
      "Steps : 275900, \t Total Gen Loss : 32.0417594909668, \t Total Dis Loss : 0.0002791135339066386\n",
      "Steps : 276000, \t Total Gen Loss : 27.408695220947266, \t Total Dis Loss : 0.00016442711057607085\n",
      "Steps : 276100, \t Total Gen Loss : 29.364429473876953, \t Total Dis Loss : 0.00010272425424773246\n",
      "Steps : 276200, \t Total Gen Loss : 33.54498291015625, \t Total Dis Loss : 4.3065585487056524e-05\n",
      "Steps : 276300, \t Total Gen Loss : 27.45618438720703, \t Total Dis Loss : 0.00022339205315802246\n",
      "Steps : 276400, \t Total Gen Loss : 27.153867721557617, \t Total Dis Loss : 4.907049878966063e-05\n",
      "Steps : 276500, \t Total Gen Loss : 31.363502502441406, \t Total Dis Loss : 0.00012157409219071269\n",
      "Steps : 276600, \t Total Gen Loss : 30.627567291259766, \t Total Dis Loss : 0.00010796880087582394\n",
      "Steps : 276700, \t Total Gen Loss : 30.221317291259766, \t Total Dis Loss : 5.971493374090642e-05\n",
      "Steps : 276800, \t Total Gen Loss : 32.87962341308594, \t Total Dis Loss : 3.570958870113827e-05\n",
      "Steps : 276900, \t Total Gen Loss : 33.93293380737305, \t Total Dis Loss : 4.459580304683186e-05\n",
      "Steps : 277000, \t Total Gen Loss : 29.7082462310791, \t Total Dis Loss : 0.0012936070561408997\n",
      "Steps : 277100, \t Total Gen Loss : 34.188743591308594, \t Total Dis Loss : 0.000680103141348809\n",
      "Steps : 277200, \t Total Gen Loss : 36.666236877441406, \t Total Dis Loss : 1.9667229935294017e-05\n",
      "Steps : 277300, \t Total Gen Loss : 33.82697296142578, \t Total Dis Loss : 2.6810466806637123e-05\n",
      "Steps : 277400, \t Total Gen Loss : 35.46649169921875, \t Total Dis Loss : 0.0001283486926695332\n",
      "Steps : 277500, \t Total Gen Loss : 35.52178192138672, \t Total Dis Loss : 0.001307670259848237\n",
      "Steps : 277600, \t Total Gen Loss : 36.32268524169922, \t Total Dis Loss : 0.00042436295188963413\n",
      "Steps : 277700, \t Total Gen Loss : 39.06371307373047, \t Total Dis Loss : 5.424921710073249e-06\n",
      "Steps : 277800, \t Total Gen Loss : 41.68975067138672, \t Total Dis Loss : 0.0002782624214887619\n",
      "Steps : 277900, \t Total Gen Loss : 36.64788818359375, \t Total Dis Loss : 0.00024558050790801644\n",
      "Steps : 278000, \t Total Gen Loss : 30.847686767578125, \t Total Dis Loss : 6.490707164630294e-05\n",
      "Steps : 278100, \t Total Gen Loss : 36.131980895996094, \t Total Dis Loss : 3.381747956154868e-05\n",
      "Steps : 278200, \t Total Gen Loss : 30.563814163208008, \t Total Dis Loss : 0.0003989475080743432\n",
      "Steps : 278300, \t Total Gen Loss : 36.219120025634766, \t Total Dis Loss : 2.535292696848046e-05\n",
      "Steps : 278400, \t Total Gen Loss : 37.7449951171875, \t Total Dis Loss : 1.3909169865655713e-05\n",
      "Steps : 278500, \t Total Gen Loss : 38.94745635986328, \t Total Dis Loss : 5.063940625404939e-05\n",
      "Steps : 278600, \t Total Gen Loss : 35.07378387451172, \t Total Dis Loss : 0.0001577728835400194\n",
      "Steps : 278700, \t Total Gen Loss : 40.77040100097656, \t Total Dis Loss : 0.00034492812119424343\n",
      "Steps : 278800, \t Total Gen Loss : 41.017433166503906, \t Total Dis Loss : 7.778312465234194e-06\n",
      "Steps : 278900, \t Total Gen Loss : 38.54847717285156, \t Total Dis Loss : 1.508270997874206e-05\n",
      "Steps : 279000, \t Total Gen Loss : 34.770469665527344, \t Total Dis Loss : 0.00015148683451116085\n",
      "Steps : 279100, \t Total Gen Loss : 36.45379638671875, \t Total Dis Loss : 2.2706458366883453e-06\n",
      "Steps : 279200, \t Total Gen Loss : 32.33728790283203, \t Total Dis Loss : 3.6545245620800415e-06\n",
      "Steps : 279300, \t Total Gen Loss : 33.645347595214844, \t Total Dis Loss : 9.880121069727466e-06\n",
      "Steps : 279400, \t Total Gen Loss : 35.6685676574707, \t Total Dis Loss : 4.560555680654943e-05\n",
      "Steps : 279500, \t Total Gen Loss : 33.7741813659668, \t Total Dis Loss : 1.5394640286103822e-05\n",
      "Steps : 279600, \t Total Gen Loss : 37.46126937866211, \t Total Dis Loss : 6.4859023041208275e-06\n",
      "Steps : 279700, \t Total Gen Loss : 39.8072509765625, \t Total Dis Loss : 4.657219051296124e-06\n",
      "Steps : 279800, \t Total Gen Loss : 35.867591857910156, \t Total Dis Loss : 1.83731008291943e-05\n",
      "Steps : 279900, \t Total Gen Loss : 37.029014587402344, \t Total Dis Loss : 1.8449288745614467e-06\n",
      "Steps : 280000, \t Total Gen Loss : 33.70634460449219, \t Total Dis Loss : 2.0576522729243152e-05\n",
      "Steps : 280100, \t Total Gen Loss : 38.46021270751953, \t Total Dis Loss : 0.0002075469383271411\n",
      "Steps : 280200, \t Total Gen Loss : 37.43331527709961, \t Total Dis Loss : 2.951387614302803e-06\n",
      "Steps : 280300, \t Total Gen Loss : 35.80241394042969, \t Total Dis Loss : 6.763232704543043e-06\n",
      "Steps : 280400, \t Total Gen Loss : 33.043582916259766, \t Total Dis Loss : 1.0519574971112888e-05\n",
      "Steps : 280500, \t Total Gen Loss : 35.792945861816406, \t Total Dis Loss : 3.175130041199736e-05\n",
      "Steps : 280600, \t Total Gen Loss : 35.53021240234375, \t Total Dis Loss : 3.7434720070450567e-06\n",
      "Steps : 280700, \t Total Gen Loss : 33.608985900878906, \t Total Dis Loss : 4.4784272176912054e-05\n",
      "Steps : 280800, \t Total Gen Loss : 32.395111083984375, \t Total Dis Loss : 1.9871658878400922e-05\n",
      "Steps : 280900, \t Total Gen Loss : 38.37422180175781, \t Total Dis Loss : 4.866954441240523e-06\n",
      "Steps : 281000, \t Total Gen Loss : 36.83041000366211, \t Total Dis Loss : 3.909099177690223e-05\n",
      "Steps : 281100, \t Total Gen Loss : 36.09644317626953, \t Total Dis Loss : 1.4079612810746767e-05\n",
      "Steps : 281200, \t Total Gen Loss : 37.06007766723633, \t Total Dis Loss : 6.933607437531464e-06\n",
      "Time for epoch 50 is 70.16893982887268 sec\n",
      "Steps : 281300, \t Total Gen Loss : 33.29948043823242, \t Total Dis Loss : 1.3290755305206403e-06\n",
      "Steps : 281400, \t Total Gen Loss : 35.55331039428711, \t Total Dis Loss : 6.861403380753472e-05\n",
      "Steps : 281500, \t Total Gen Loss : 34.31073760986328, \t Total Dis Loss : 3.143342837574892e-05\n",
      "Steps : 281600, \t Total Gen Loss : 32.75861740112305, \t Total Dis Loss : 1.2809820873371791e-05\n",
      "Steps : 281700, \t Total Gen Loss : 32.00336456298828, \t Total Dis Loss : 5.1239832828287035e-05\n",
      "Steps : 281800, \t Total Gen Loss : 32.23106002807617, \t Total Dis Loss : 9.80126151262084e-06\n",
      "Steps : 281900, \t Total Gen Loss : 31.114120483398438, \t Total Dis Loss : 5.352758307708427e-05\n",
      "Steps : 282000, \t Total Gen Loss : 30.133607864379883, \t Total Dis Loss : 5.8601559430826455e-05\n",
      "Steps : 282100, \t Total Gen Loss : 31.384899139404297, \t Total Dis Loss : 2.062028943328187e-05\n",
      "Steps : 282200, \t Total Gen Loss : 29.75526237487793, \t Total Dis Loss : 6.3939078245311975e-06\n",
      "Steps : 282300, \t Total Gen Loss : 31.387685775756836, \t Total Dis Loss : 1.965119554370176e-05\n",
      "Steps : 282400, \t Total Gen Loss : 30.906261444091797, \t Total Dis Loss : 1.4554225344909355e-05\n",
      "Steps : 282500, \t Total Gen Loss : 30.7789363861084, \t Total Dis Loss : 4.209182588965632e-05\n",
      "Steps : 282600, \t Total Gen Loss : 32.5738525390625, \t Total Dis Loss : 6.875512917758897e-05\n",
      "Steps : 282700, \t Total Gen Loss : 32.75691604614258, \t Total Dis Loss : 3.105491487076506e-05\n",
      "Steps : 282800, \t Total Gen Loss : 30.428125381469727, \t Total Dis Loss : 1.734723264235072e-05\n",
      "Steps : 282900, \t Total Gen Loss : 30.585966110229492, \t Total Dis Loss : 6.885970833536703e-06\n",
      "Steps : 283000, \t Total Gen Loss : 35.65008544921875, \t Total Dis Loss : 4.175891717750346e-06\n",
      "Steps : 283100, \t Total Gen Loss : 34.294857025146484, \t Total Dis Loss : 4.6697576181031764e-05\n",
      "Steps : 283200, \t Total Gen Loss : 35.65252685546875, \t Total Dis Loss : 2.2629206796409562e-05\n",
      "Steps : 283300, \t Total Gen Loss : 36.428497314453125, \t Total Dis Loss : 3.305370773887262e-05\n",
      "Steps : 283400, \t Total Gen Loss : 30.04320526123047, \t Total Dis Loss : 8.75357654877007e-05\n",
      "Steps : 283500, \t Total Gen Loss : 28.6490421295166, \t Total Dis Loss : 7.338074647122994e-05\n",
      "Steps : 283600, \t Total Gen Loss : 30.939491271972656, \t Total Dis Loss : 9.15767450351268e-05\n",
      "Steps : 283700, \t Total Gen Loss : 26.154647827148438, \t Total Dis Loss : 0.00034755782689899206\n",
      "Steps : 283800, \t Total Gen Loss : 28.003345489501953, \t Total Dis Loss : 2.3541306291008368e-05\n",
      "Steps : 283900, \t Total Gen Loss : 30.541845321655273, \t Total Dis Loss : 1.706409420876298e-05\n",
      "Steps : 284000, \t Total Gen Loss : 29.72002410888672, \t Total Dis Loss : 3.043648393941112e-05\n",
      "Steps : 284100, \t Total Gen Loss : 32.70245361328125, \t Total Dis Loss : 7.1822310019342694e-06\n",
      "Steps : 284200, \t Total Gen Loss : 32.47842025756836, \t Total Dis Loss : 1.0294117601006292e-05\n",
      "Steps : 284300, \t Total Gen Loss : 31.3538875579834, \t Total Dis Loss : 5.382643939810805e-05\n",
      "Steps : 284400, \t Total Gen Loss : 27.212026596069336, \t Total Dis Loss : 0.0002979944401886314\n",
      "Steps : 284500, \t Total Gen Loss : 27.681934356689453, \t Total Dis Loss : 3.072610343224369e-05\n",
      "Steps : 284600, \t Total Gen Loss : 29.922473907470703, \t Total Dis Loss : 2.95576192002045e-05\n",
      "Steps : 284700, \t Total Gen Loss : 29.229015350341797, \t Total Dis Loss : 2.675989526323974e-05\n",
      "Steps : 284800, \t Total Gen Loss : 31.959060668945312, \t Total Dis Loss : 3.062424366362393e-05\n",
      "Steps : 284900, \t Total Gen Loss : 28.144615173339844, \t Total Dis Loss : 3.5632569051813334e-05\n",
      "Steps : 285000, \t Total Gen Loss : 30.370180130004883, \t Total Dis Loss : 1.4312349776446354e-05\n",
      "Steps : 285100, \t Total Gen Loss : 31.626422882080078, \t Total Dis Loss : 4.887467730441131e-05\n",
      "Steps : 285200, \t Total Gen Loss : 34.79563903808594, \t Total Dis Loss : 1.1399120921851136e-05\n",
      "Steps : 285300, \t Total Gen Loss : 29.88010025024414, \t Total Dis Loss : 2.2526757675223053e-05\n",
      "Steps : 285400, \t Total Gen Loss : 31.052343368530273, \t Total Dis Loss : 1.8388820535619743e-05\n",
      "Steps : 285500, \t Total Gen Loss : 31.901844024658203, \t Total Dis Loss : 1.0245402336295228e-05\n",
      "Steps : 285600, \t Total Gen Loss : 29.622404098510742, \t Total Dis Loss : 1.9606910427683033e-05\n",
      "Steps : 285700, \t Total Gen Loss : 30.7962646484375, \t Total Dis Loss : 0.00015126599464565516\n",
      "Steps : 285800, \t Total Gen Loss : 33.13038635253906, \t Total Dis Loss : 1.7551168639329262e-05\n",
      "Steps : 285900, \t Total Gen Loss : 30.11341094970703, \t Total Dis Loss : 4.081858787685633e-05\n",
      "Steps : 286000, \t Total Gen Loss : 31.57339859008789, \t Total Dis Loss : 4.18053095927462e-05\n",
      "Steps : 286100, \t Total Gen Loss : 33.22595977783203, \t Total Dis Loss : 1.73895441548666e-05\n",
      "Steps : 286200, \t Total Gen Loss : 31.656436920166016, \t Total Dis Loss : 9.763688467501197e-06\n",
      "Steps : 286300, \t Total Gen Loss : 30.375606536865234, \t Total Dis Loss : 3.4424067507643485e-06\n",
      "Steps : 286400, \t Total Gen Loss : 28.87403106689453, \t Total Dis Loss : 1.5046476619318128e-05\n",
      "Steps : 286500, \t Total Gen Loss : 30.38066291809082, \t Total Dis Loss : 2.8062754608981777e-06\n",
      "Steps : 286600, \t Total Gen Loss : 29.270336151123047, \t Total Dis Loss : 5.883831818209728e-06\n",
      "Steps : 286700, \t Total Gen Loss : 30.71105194091797, \t Total Dis Loss : 1.3576573110185564e-05\n",
      "Steps : 286800, \t Total Gen Loss : 28.555496215820312, \t Total Dis Loss : 7.239479600684717e-06\n",
      "Time for epoch 51 is 69.05050826072693 sec\n",
      "Steps : 286900, \t Total Gen Loss : 30.896194458007812, \t Total Dis Loss : 3.915276465704665e-06\n",
      "Steps : 287000, \t Total Gen Loss : 27.39479637145996, \t Total Dis Loss : 0.00022045751393307\n",
      "Steps : 287100, \t Total Gen Loss : 35.478759765625, \t Total Dis Loss : 4.565265953715425e-06\n",
      "Steps : 287200, \t Total Gen Loss : 33.058597564697266, \t Total Dis Loss : 6.126700463937595e-05\n",
      "Steps : 287300, \t Total Gen Loss : 31.798351287841797, \t Total Dis Loss : 5.2714807679876685e-05\n",
      "Steps : 287400, \t Total Gen Loss : 34.03649139404297, \t Total Dis Loss : 7.131236088753212e-06\n",
      "Steps : 287500, \t Total Gen Loss : 28.723295211791992, \t Total Dis Loss : 6.296558422036469e-05\n",
      "Steps : 287600, \t Total Gen Loss : 30.597307205200195, \t Total Dis Loss : 7.511497824452817e-05\n",
      "Steps : 287700, \t Total Gen Loss : 30.549036026000977, \t Total Dis Loss : 7.058449682517676e-06\n",
      "Steps : 287800, \t Total Gen Loss : 32.993438720703125, \t Total Dis Loss : 1.3617747754324228e-05\n",
      "Steps : 287900, \t Total Gen Loss : 34.371891021728516, \t Total Dis Loss : 1.4773663679079618e-05\n",
      "Steps : 288000, \t Total Gen Loss : 35.29078674316406, \t Total Dis Loss : 9.075155503524002e-06\n",
      "Steps : 288100, \t Total Gen Loss : 31.011917114257812, \t Total Dis Loss : 1.0420793842058629e-05\n",
      "Steps : 288200, \t Total Gen Loss : 31.78672218322754, \t Total Dis Loss : 5.088585749035701e-05\n",
      "Steps : 288300, \t Total Gen Loss : 33.690032958984375, \t Total Dis Loss : 6.651006697211415e-05\n",
      "Steps : 288400, \t Total Gen Loss : 36.41505432128906, \t Total Dis Loss : 5.695605977962259e-06\n",
      "Steps : 288500, \t Total Gen Loss : 31.42540168762207, \t Total Dis Loss : 2.8654112611548044e-05\n",
      "Steps : 288600, \t Total Gen Loss : 36.48452377319336, \t Total Dis Loss : 1.9541494111763313e-06\n",
      "Steps : 288700, \t Total Gen Loss : 34.752899169921875, \t Total Dis Loss : 3.983788701589219e-06\n",
      "Steps : 288800, \t Total Gen Loss : 27.722810745239258, \t Total Dis Loss : 0.001230851048603654\n",
      "Steps : 288900, \t Total Gen Loss : 30.834903717041016, \t Total Dis Loss : 4.6363857109099627e-05\n",
      "Steps : 289000, \t Total Gen Loss : 32.795494079589844, \t Total Dis Loss : 2.0605169993359596e-05\n",
      "Steps : 289100, \t Total Gen Loss : 34.6580924987793, \t Total Dis Loss : 1.4763729268452153e-05\n",
      "Steps : 289200, \t Total Gen Loss : 30.02312469482422, \t Total Dis Loss : 0.0001298221031902358\n",
      "Steps : 289300, \t Total Gen Loss : 29.930118560791016, \t Total Dis Loss : 1.2662786502914969e-05\n",
      "Steps : 289400, \t Total Gen Loss : 30.369422912597656, \t Total Dis Loss : 8.914947102311999e-05\n",
      "Steps : 289500, \t Total Gen Loss : 35.330875396728516, \t Total Dis Loss : 1.069418703991687e-05\n",
      "Steps : 289600, \t Total Gen Loss : 32.37416458129883, \t Total Dis Loss : 1.7321078757959185e-06\n",
      "Steps : 289700, \t Total Gen Loss : 31.148677825927734, \t Total Dis Loss : 1.2966207577846944e-05\n",
      "Steps : 289800, \t Total Gen Loss : 30.511573791503906, \t Total Dis Loss : 8.492792403558269e-06\n",
      "Steps : 289900, \t Total Gen Loss : 31.28778648376465, \t Total Dis Loss : 4.741382781503489e-06\n",
      "Steps : 290000, \t Total Gen Loss : 30.623260498046875, \t Total Dis Loss : 0.0013245845912024379\n",
      "Steps : 290100, \t Total Gen Loss : 30.938133239746094, \t Total Dis Loss : 4.917361366096884e-06\n",
      "Steps : 290200, \t Total Gen Loss : 28.868045806884766, \t Total Dis Loss : 2.100068195431959e-05\n",
      "Steps : 290300, \t Total Gen Loss : 35.437294006347656, \t Total Dis Loss : 3.5576208574639168e-06\n",
      "Steps : 290400, \t Total Gen Loss : 32.10728454589844, \t Total Dis Loss : 9.129004865826573e-06\n",
      "Steps : 290500, \t Total Gen Loss : 31.056604385375977, \t Total Dis Loss : 7.562834798591211e-05\n",
      "Steps : 290600, \t Total Gen Loss : 31.18341827392578, \t Total Dis Loss : 0.0007361223688349128\n",
      "Steps : 290700, \t Total Gen Loss : 34.72572326660156, \t Total Dis Loss : 4.8682017222745344e-05\n",
      "Steps : 290800, \t Total Gen Loss : 30.14083480834961, \t Total Dis Loss : 2.5700090191094205e-05\n",
      "Steps : 290900, \t Total Gen Loss : 31.06702995300293, \t Total Dis Loss : 1.718644125503488e-05\n",
      "Steps : 291000, \t Total Gen Loss : 28.921489715576172, \t Total Dis Loss : 0.0005310189444571733\n",
      "Steps : 291100, \t Total Gen Loss : 29.03553009033203, \t Total Dis Loss : 0.00026814721059054136\n",
      "Steps : 291200, \t Total Gen Loss : 30.777769088745117, \t Total Dis Loss : 2.6628729756339453e-05\n",
      "Steps : 291300, \t Total Gen Loss : 28.18035888671875, \t Total Dis Loss : 6.556181324413046e-05\n",
      "Steps : 291400, \t Total Gen Loss : 32.305274963378906, \t Total Dis Loss : 8.832788444124162e-05\n",
      "Steps : 291500, \t Total Gen Loss : 29.538827896118164, \t Total Dis Loss : 8.54417430673493e-06\n",
      "Steps : 291600, \t Total Gen Loss : 32.60699462890625, \t Total Dis Loss : 7.2689163062023e-06\n",
      "Steps : 291700, \t Total Gen Loss : 32.41771697998047, \t Total Dis Loss : 1.7003318134811707e-05\n",
      "Steps : 291800, \t Total Gen Loss : 31.817279815673828, \t Total Dis Loss : 8.830254955682904e-06\n",
      "Steps : 291900, \t Total Gen Loss : 31.93446159362793, \t Total Dis Loss : 2.898846787502407e-06\n",
      "Steps : 292000, \t Total Gen Loss : 32.1861572265625, \t Total Dis Loss : 6.3920097090885974e-06\n",
      "Steps : 292100, \t Total Gen Loss : 29.936138153076172, \t Total Dis Loss : 8.158165655913763e-06\n",
      "Steps : 292200, \t Total Gen Loss : 31.1717472076416, \t Total Dis Loss : 0.00011919349344680086\n",
      "Steps : 292300, \t Total Gen Loss : 37.15311050415039, \t Total Dis Loss : 6.31034799880581e-06\n",
      "Steps : 292400, \t Total Gen Loss : 36.40362548828125, \t Total Dis Loss : 7.494237797800452e-05\n",
      "Steps : 292500, \t Total Gen Loss : 34.9193115234375, \t Total Dis Loss : 4.573108526528813e-06\n",
      "Time for epoch 52 is 69.01894092559814 sec\n",
      "Steps : 292600, \t Total Gen Loss : 30.640239715576172, \t Total Dis Loss : 3.2714988265070133e-06\n",
      "Steps : 292700, \t Total Gen Loss : 31.57333755493164, \t Total Dis Loss : 1.3896318478145986e-06\n",
      "Steps : 292800, \t Total Gen Loss : 35.46189880371094, \t Total Dis Loss : 2.1044315872131847e-06\n",
      "Steps : 292900, \t Total Gen Loss : 33.308902740478516, \t Total Dis Loss : 9.419702564628096e-07\n",
      "Steps : 293000, \t Total Gen Loss : 30.59084701538086, \t Total Dis Loss : 0.010931161232292652\n",
      "Steps : 293100, \t Total Gen Loss : 32.97898864746094, \t Total Dis Loss : 6.011591267451877e-06\n",
      "Steps : 293200, \t Total Gen Loss : 32.541175842285156, \t Total Dis Loss : 8.130984497256577e-06\n",
      "Steps : 293300, \t Total Gen Loss : 34.300052642822266, \t Total Dis Loss : 2.9249274575704476e-06\n",
      "Steps : 293400, \t Total Gen Loss : 33.70001983642578, \t Total Dis Loss : 1.3291437426232733e-06\n",
      "Steps : 293500, \t Total Gen Loss : 33.309242248535156, \t Total Dis Loss : 6.133197530289181e-06\n",
      "Steps : 293600, \t Total Gen Loss : 32.82667541503906, \t Total Dis Loss : 6.028993084328249e-06\n",
      "Steps : 293700, \t Total Gen Loss : 27.224700927734375, \t Total Dis Loss : 0.0014030400197952986\n",
      "Steps : 293800, \t Total Gen Loss : 30.23086929321289, \t Total Dis Loss : 0.0001174089265987277\n",
      "Steps : 293900, \t Total Gen Loss : 27.745262145996094, \t Total Dis Loss : 3.8635382225038484e-05\n",
      "Steps : 294000, \t Total Gen Loss : 26.547765731811523, \t Total Dis Loss : 2.6649060600902885e-05\n",
      "Steps : 294100, \t Total Gen Loss : 30.22774887084961, \t Total Dis Loss : 1.1764234841393773e-05\n",
      "Steps : 294200, \t Total Gen Loss : 30.6376953125, \t Total Dis Loss : 4.146250648773275e-05\n",
      "Steps : 294300, \t Total Gen Loss : 29.137779235839844, \t Total Dis Loss : 6.397601828211918e-05\n",
      "Steps : 294400, \t Total Gen Loss : 33.69588851928711, \t Total Dis Loss : 1.2837118447350804e-05\n",
      "Steps : 294500, \t Total Gen Loss : 31.04084014892578, \t Total Dis Loss : 6.118872988736257e-06\n",
      "Steps : 294600, \t Total Gen Loss : 29.625825881958008, \t Total Dis Loss : 1.4896898392180447e-05\n",
      "Steps : 294700, \t Total Gen Loss : 29.34561538696289, \t Total Dis Loss : 0.0006151193520054221\n",
      "Steps : 294800, \t Total Gen Loss : 32.155303955078125, \t Total Dis Loss : 2.2934395019547082e-05\n",
      "Steps : 294900, \t Total Gen Loss : 30.01150131225586, \t Total Dis Loss : 1.4149434719001874e-05\n",
      "Steps : 295000, \t Total Gen Loss : 31.825050354003906, \t Total Dis Loss : 8.423523104283959e-05\n",
      "Steps : 295100, \t Total Gen Loss : 29.7216739654541, \t Total Dis Loss : 2.092756585625466e-05\n",
      "Steps : 295200, \t Total Gen Loss : 30.421098709106445, \t Total Dis Loss : 3.530362664605491e-05\n",
      "Steps : 295300, \t Total Gen Loss : 31.716224670410156, \t Total Dis Loss : 0.0006158201722428203\n",
      "Steps : 295400, \t Total Gen Loss : 32.835838317871094, \t Total Dis Loss : 9.358414899907075e-06\n",
      "Steps : 295500, \t Total Gen Loss : 35.098175048828125, \t Total Dis Loss : 1.705064823909197e-05\n",
      "Steps : 295600, \t Total Gen Loss : 32.5145149230957, \t Total Dis Loss : 1.1187508789589629e-05\n",
      "Steps : 295700, \t Total Gen Loss : 32.21293640136719, \t Total Dis Loss : 7.815991011739243e-06\n",
      "Steps : 295800, \t Total Gen Loss : 29.284027099609375, \t Total Dis Loss : 2.8641240987781202e-06\n",
      "Steps : 295900, \t Total Gen Loss : 31.946882247924805, \t Total Dis Loss : 8.978138794191182e-06\n",
      "Steps : 296000, \t Total Gen Loss : 30.863962173461914, \t Total Dis Loss : 3.808745077549247e-06\n",
      "Steps : 296100, \t Total Gen Loss : 33.55112075805664, \t Total Dis Loss : 6.4394444052595645e-06\n",
      "Steps : 296200, \t Total Gen Loss : 31.644817352294922, \t Total Dis Loss : 5.49383730685804e-06\n",
      "Steps : 296300, \t Total Gen Loss : 34.5273323059082, \t Total Dis Loss : 6.138084245321807e-07\n",
      "Steps : 296400, \t Total Gen Loss : 31.646991729736328, \t Total Dis Loss : 2.8928639039804693e-06\n",
      "Steps : 296500, \t Total Gen Loss : 34.02532958984375, \t Total Dis Loss : 3.589273546822369e-06\n",
      "Steps : 296600, \t Total Gen Loss : 34.28782653808594, \t Total Dis Loss : 2.6621496544976253e-06\n",
      "Steps : 296700, \t Total Gen Loss : 35.1666145324707, \t Total Dis Loss : 5.853897164342925e-06\n",
      "Steps : 296800, \t Total Gen Loss : 31.951379776000977, \t Total Dis Loss : 3.3946114399441285e-06\n",
      "Steps : 296900, \t Total Gen Loss : 32.66953659057617, \t Total Dis Loss : 2.2467104372481117e-06\n",
      "Steps : 297000, \t Total Gen Loss : 33.01573181152344, \t Total Dis Loss : 2.527102878957521e-06\n",
      "Steps : 297100, \t Total Gen Loss : 32.73027801513672, \t Total Dis Loss : 7.517989502048295e-07\n",
      "Steps : 297200, \t Total Gen Loss : 28.746492385864258, \t Total Dis Loss : 1.670157871558331e-05\n",
      "Steps : 297300, \t Total Gen Loss : 31.558727264404297, \t Total Dis Loss : 4.3894256123167e-06\n",
      "Steps : 297400, \t Total Gen Loss : 33.18158721923828, \t Total Dis Loss : 5.479198989633005e-06\n",
      "Steps : 297500, \t Total Gen Loss : 34.41388702392578, \t Total Dis Loss : 2.908756641772925e-06\n",
      "Steps : 297600, \t Total Gen Loss : 33.083377838134766, \t Total Dis Loss : 8.44572059577331e-06\n",
      "Steps : 297700, \t Total Gen Loss : 32.19239807128906, \t Total Dis Loss : 1.5174332474998664e-05\n",
      "Steps : 297800, \t Total Gen Loss : 35.037559509277344, \t Total Dis Loss : 1.03091394976218e-06\n",
      "Steps : 297900, \t Total Gen Loss : 32.525291442871094, \t Total Dis Loss : 3.7814397728652693e-06\n",
      "Steps : 298000, \t Total Gen Loss : 34.3079833984375, \t Total Dis Loss : 7.112362709449371e-06\n",
      "Steps : 298100, \t Total Gen Loss : 32.35194396972656, \t Total Dis Loss : 5.402413080446422e-06\n",
      "Time for epoch 53 is 69.00131821632385 sec\n",
      "Steps : 298200, \t Total Gen Loss : 32.96833801269531, \t Total Dis Loss : 8.081537998805288e-06\n",
      "Steps : 298300, \t Total Gen Loss : 31.784412384033203, \t Total Dis Loss : 1.7171701074403245e-06\n",
      "Steps : 298400, \t Total Gen Loss : 33.48810958862305, \t Total Dis Loss : 6.081742753849539e-07\n",
      "Steps : 298500, \t Total Gen Loss : 33.117759704589844, \t Total Dis Loss : 5.8690311561804265e-06\n",
      "Steps : 298600, \t Total Gen Loss : 35.39619827270508, \t Total Dis Loss : 1.289559008910146e-06\n",
      "Steps : 298700, \t Total Gen Loss : 34.44080352783203, \t Total Dis Loss : 5.093760591989849e-06\n",
      "Steps : 298800, \t Total Gen Loss : 30.9682674407959, \t Total Dis Loss : 2.3845226678531617e-05\n",
      "Steps : 298900, \t Total Gen Loss : 30.211091995239258, \t Total Dis Loss : 2.0926814613630995e-05\n",
      "Steps : 299000, \t Total Gen Loss : 33.647552490234375, \t Total Dis Loss : 0.0005659926100634038\n",
      "Steps : 299100, \t Total Gen Loss : 23.085830688476562, \t Total Dis Loss : 0.0057727275416255\n",
      "Steps : 299200, \t Total Gen Loss : 30.665206909179688, \t Total Dis Loss : 0.00010658489918569103\n",
      "Steps : 299300, \t Total Gen Loss : 30.754974365234375, \t Total Dis Loss : 2.2993299353402108e-05\n",
      "Steps : 299400, \t Total Gen Loss : 34.45001220703125, \t Total Dis Loss : 0.004769330844283104\n",
      "Steps : 299500, \t Total Gen Loss : 34.56067657470703, \t Total Dis Loss : 3.0638107091363054e-06\n",
      "Steps : 299600, \t Total Gen Loss : 34.636234283447266, \t Total Dis Loss : 5.800741746497806e-06\n",
      "Steps : 299700, \t Total Gen Loss : 33.64031219482422, \t Total Dis Loss : 6.605991984542925e-06\n",
      "Steps : 299800, \t Total Gen Loss : 29.623584747314453, \t Total Dis Loss : 0.00010732245573308319\n",
      "Steps : 299900, \t Total Gen Loss : 27.59341812133789, \t Total Dis Loss : 0.00012001575669273734\n",
      "Steps : 300000, \t Total Gen Loss : 34.267799377441406, \t Total Dis Loss : 1.5407031241920777e-05\n",
      "Steps : 300100, \t Total Gen Loss : 31.100666046142578, \t Total Dis Loss : 2.6942145723296562e-06\n",
      "Steps : 300200, \t Total Gen Loss : 31.54143714904785, \t Total Dis Loss : 6.191380180098349e-06\n",
      "Steps : 300300, \t Total Gen Loss : 30.463272094726562, \t Total Dis Loss : 8.726773558009882e-06\n",
      "Steps : 300400, \t Total Gen Loss : 30.043508529663086, \t Total Dis Loss : 2.984644379466772e-05\n",
      "Steps : 300500, \t Total Gen Loss : 29.95412826538086, \t Total Dis Loss : 3.745140566024929e-05\n",
      "Steps : 300600, \t Total Gen Loss : 32.19545364379883, \t Total Dis Loss : 2.779534042929299e-06\n",
      "Steps : 300700, \t Total Gen Loss : 29.188453674316406, \t Total Dis Loss : 4.429868931765668e-05\n",
      "Steps : 300800, \t Total Gen Loss : 35.2548942565918, \t Total Dis Loss : 9.224509653904533e-07\n",
      "Steps : 300900, \t Total Gen Loss : 33.28453063964844, \t Total Dis Loss : 5.395528660301352e-06\n",
      "Steps : 301000, \t Total Gen Loss : 36.42469024658203, \t Total Dis Loss : 2.6377761059848126e-06\n",
      "Steps : 301100, \t Total Gen Loss : 33.90911102294922, \t Total Dis Loss : 4.375751359475544e-06\n",
      "Steps : 301200, \t Total Gen Loss : 31.55873680114746, \t Total Dis Loss : 0.00011010768503183499\n",
      "Steps : 301300, \t Total Gen Loss : 32.79413604736328, \t Total Dis Loss : 2.0251857222319813e-06\n",
      "Steps : 301400, \t Total Gen Loss : 32.740814208984375, \t Total Dis Loss : 3.082038529100828e-05\n",
      "Steps : 301500, \t Total Gen Loss : 30.06083869934082, \t Total Dis Loss : 1.041394079948077e-05\n",
      "Steps : 301600, \t Total Gen Loss : 31.489471435546875, \t Total Dis Loss : 4.227559475111775e-05\n",
      "Steps : 301700, \t Total Gen Loss : 31.382038116455078, \t Total Dis Loss : 4.183339115115814e-06\n",
      "Steps : 301800, \t Total Gen Loss : 31.303627014160156, \t Total Dis Loss : 4.703598278865684e-06\n",
      "Steps : 301900, \t Total Gen Loss : 29.630840301513672, \t Total Dis Loss : 3.118397944490425e-05\n",
      "Steps : 302000, \t Total Gen Loss : 31.238971710205078, \t Total Dis Loss : 5.3104336984688416e-05\n",
      "Steps : 302100, \t Total Gen Loss : 34.19124984741211, \t Total Dis Loss : 3.6573310353560373e-06\n",
      "Steps : 302200, \t Total Gen Loss : 28.193340301513672, \t Total Dis Loss : 0.00010216549708275124\n",
      "Steps : 302300, \t Total Gen Loss : 33.36558532714844, \t Total Dis Loss : 8.057138074946124e-06\n",
      "Steps : 302400, \t Total Gen Loss : 32.25770568847656, \t Total Dis Loss : 7.002626261964906e-06\n",
      "Steps : 302500, \t Total Gen Loss : 31.743745803833008, \t Total Dis Loss : 1.6892958228709176e-05\n",
      "Steps : 302600, \t Total Gen Loss : 30.867549896240234, \t Total Dis Loss : 3.4096631225111196e-06\n",
      "Steps : 302700, \t Total Gen Loss : 34.14996337890625, \t Total Dis Loss : 5.963252988294698e-06\n",
      "Steps : 302800, \t Total Gen Loss : 31.905288696289062, \t Total Dis Loss : 6.200848929438507e-06\n",
      "Steps : 302900, \t Total Gen Loss : 34.25746154785156, \t Total Dis Loss : 3.172930291839293e-06\n",
      "Steps : 303000, \t Total Gen Loss : 32.356475830078125, \t Total Dis Loss : 2.423345904389862e-06\n",
      "Steps : 303100, \t Total Gen Loss : 35.8585090637207, \t Total Dis Loss : 1.3320852758624824e-06\n",
      "Steps : 303200, \t Total Gen Loss : 31.060327529907227, \t Total Dis Loss : 3.905451376340352e-05\n",
      "Steps : 303300, \t Total Gen Loss : 33.841796875, \t Total Dis Loss : 1.7065796100723674e-06\n",
      "Steps : 303400, \t Total Gen Loss : 37.002891540527344, \t Total Dis Loss : 5.635998149955412e-06\n",
      "Steps : 303500, \t Total Gen Loss : 34.91154098510742, \t Total Dis Loss : 1.5714282199041918e-05\n",
      "Steps : 303600, \t Total Gen Loss : 31.043018341064453, \t Total Dis Loss : 0.0004932513111270964\n",
      "Steps : 303700, \t Total Gen Loss : 34.22420883178711, \t Total Dis Loss : 3.1477507036470342e-06\n",
      "Time for epoch 54 is 69.011559009552 sec\n",
      "Steps : 303800, \t Total Gen Loss : 35.90004348754883, \t Total Dis Loss : 1.4134138837107457e-05\n",
      "Steps : 303900, \t Total Gen Loss : 30.844518661499023, \t Total Dis Loss : 1.2672081538767088e-05\n",
      "Steps : 304000, \t Total Gen Loss : 28.57831573486328, \t Total Dis Loss : 4.9584552471060306e-05\n",
      "Steps : 304100, \t Total Gen Loss : 32.55681610107422, \t Total Dis Loss : 3.440964064793661e-05\n",
      "Steps : 304200, \t Total Gen Loss : 30.381547927856445, \t Total Dis Loss : 9.147632226813585e-06\n",
      "Steps : 304300, \t Total Gen Loss : 35.67816925048828, \t Total Dis Loss : 1.0016279929914162e-06\n",
      "Steps : 304400, \t Total Gen Loss : 34.22507858276367, \t Total Dis Loss : 4.122764221392572e-05\n",
      "Steps : 304500, \t Total Gen Loss : 31.862701416015625, \t Total Dis Loss : 3.938613190257456e-06\n",
      "Steps : 304600, \t Total Gen Loss : 31.0562686920166, \t Total Dis Loss : 6.187485269038007e-06\n",
      "Steps : 304700, \t Total Gen Loss : 33.936126708984375, \t Total Dis Loss : 9.318545380665455e-06\n",
      "Steps : 304800, \t Total Gen Loss : 32.03587341308594, \t Total Dis Loss : 1.8276274431627826e-06\n",
      "Steps : 304900, \t Total Gen Loss : 32.20958709716797, \t Total Dis Loss : 5.55935093871085e-06\n",
      "Steps : 305000, \t Total Gen Loss : 36.35643005371094, \t Total Dis Loss : 5.386152224673424e-06\n",
      "Steps : 305100, \t Total Gen Loss : 31.15940284729004, \t Total Dis Loss : 3.127236777800135e-05\n",
      "Steps : 305200, \t Total Gen Loss : 32.317848205566406, \t Total Dis Loss : 1.957348285941407e-05\n",
      "Steps : 305300, \t Total Gen Loss : 30.87712287902832, \t Total Dis Loss : 1.115061513701221e-05\n",
      "Steps : 305400, \t Total Gen Loss : 30.86294174194336, \t Total Dis Loss : 7.93600847828202e-05\n",
      "Steps : 305500, \t Total Gen Loss : 33.87257385253906, \t Total Dis Loss : 1.0756873052741867e-05\n",
      "Steps : 305600, \t Total Gen Loss : 34.2756233215332, \t Total Dis Loss : 1.5887435438344255e-05\n",
      "Steps : 305700, \t Total Gen Loss : 31.546142578125, \t Total Dis Loss : 3.2400457712356e-05\n",
      "Steps : 305800, \t Total Gen Loss : 35.721778869628906, \t Total Dis Loss : 2.011042852245737e-05\n",
      "Steps : 305900, \t Total Gen Loss : 31.148679733276367, \t Total Dis Loss : 5.993206741550239e-06\n",
      "Steps : 306000, \t Total Gen Loss : 31.99481773376465, \t Total Dis Loss : 8.612404599261936e-06\n",
      "Steps : 306100, \t Total Gen Loss : 33.59632873535156, \t Total Dis Loss : 4.191986590740271e-06\n",
      "Steps : 306200, \t Total Gen Loss : 30.409503936767578, \t Total Dis Loss : 0.00021677247423212975\n",
      "Steps : 306300, \t Total Gen Loss : 28.259872436523438, \t Total Dis Loss : 0.00018117325089406222\n",
      "Steps : 306400, \t Total Gen Loss : 30.922969818115234, \t Total Dis Loss : 3.420044595259242e-05\n",
      "Steps : 306500, \t Total Gen Loss : 28.985118865966797, \t Total Dis Loss : 0.0003192336007487029\n",
      "Steps : 306600, \t Total Gen Loss : 31.688270568847656, \t Total Dis Loss : 1.8019167328020558e-05\n",
      "Steps : 306700, \t Total Gen Loss : 31.855751037597656, \t Total Dis Loss : 2.6713303668657318e-05\n",
      "Steps : 306800, \t Total Gen Loss : 29.370529174804688, \t Total Dis Loss : 6.25851025688462e-05\n",
      "Steps : 306900, \t Total Gen Loss : 31.80815887451172, \t Total Dis Loss : 4.314069155952893e-05\n",
      "Steps : 307000, \t Total Gen Loss : 30.43410873413086, \t Total Dis Loss : 4.9229372962145135e-05\n",
      "Steps : 307100, \t Total Gen Loss : 30.19601058959961, \t Total Dis Loss : 2.8960670533706434e-05\n",
      "Steps : 307200, \t Total Gen Loss : 35.53506851196289, \t Total Dis Loss : 5.618840987153817e-06\n",
      "Steps : 307300, \t Total Gen Loss : 32.942832946777344, \t Total Dis Loss : 1.4747387467650697e-05\n",
      "Steps : 307400, \t Total Gen Loss : 33.27580642700195, \t Total Dis Loss : 1.127971131609229e-06\n",
      "Steps : 307500, \t Total Gen Loss : 31.955852508544922, \t Total Dis Loss : 1.540339553685044e-06\n",
      "Steps : 307600, \t Total Gen Loss : 35.438987731933594, \t Total Dis Loss : 2.6966520181304077e-06\n",
      "Steps : 307700, \t Total Gen Loss : 33.70402908325195, \t Total Dis Loss : 3.682673650473589e-06\n",
      "Steps : 307800, \t Total Gen Loss : 33.66839599609375, \t Total Dis Loss : 2.664460453161155e-06\n",
      "Steps : 307900, \t Total Gen Loss : 35.35320281982422, \t Total Dis Loss : 5.092692845209967e-06\n",
      "Steps : 308000, \t Total Gen Loss : 35.73359680175781, \t Total Dis Loss : 5.758224688179325e-06\n",
      "Steps : 308100, \t Total Gen Loss : 32.72895812988281, \t Total Dis Loss : 4.791482410837489e-07\n",
      "Steps : 308200, \t Total Gen Loss : 31.77461051940918, \t Total Dis Loss : 0.0003593771834857762\n",
      "Steps : 308300, \t Total Gen Loss : 31.477190017700195, \t Total Dis Loss : 5.23132475791499e-05\n",
      "Steps : 308400, \t Total Gen Loss : 33.43160629272461, \t Total Dis Loss : 6.147178646642715e-05\n",
      "Steps : 308500, \t Total Gen Loss : 31.23174476623535, \t Total Dis Loss : 3.139687350994791e-06\n",
      "Steps : 308600, \t Total Gen Loss : 32.47190856933594, \t Total Dis Loss : 1.0397844562248792e-05\n",
      "Steps : 308700, \t Total Gen Loss : 28.160306930541992, \t Total Dis Loss : 1.820149554987438e-05\n",
      "Steps : 308800, \t Total Gen Loss : 29.321264266967773, \t Total Dis Loss : 1.4711036783410236e-05\n",
      "Steps : 308900, \t Total Gen Loss : 30.535308837890625, \t Total Dis Loss : 2.0365627278806642e-05\n",
      "Steps : 309000, \t Total Gen Loss : 31.367982864379883, \t Total Dis Loss : 0.00010588925215415657\n",
      "Steps : 309100, \t Total Gen Loss : 31.491043090820312, \t Total Dis Loss : 3.435996768530458e-05\n",
      "Steps : 309200, \t Total Gen Loss : 28.909435272216797, \t Total Dis Loss : 6.231750739971176e-05\n",
      "Steps : 309300, \t Total Gen Loss : 31.599201202392578, \t Total Dis Loss : 4.621807602234185e-05\n",
      "Time for epoch 55 is 69.71066498756409 sec\n",
      "Steps : 309400, \t Total Gen Loss : 31.941631317138672, \t Total Dis Loss : 2.918932295870036e-05\n",
      "Steps : 309500, \t Total Gen Loss : 29.94823455810547, \t Total Dis Loss : 5.032629269408062e-05\n",
      "Steps : 309600, \t Total Gen Loss : 26.889366149902344, \t Total Dis Loss : 7.347483187913895e-05\n",
      "Steps : 309700, \t Total Gen Loss : 32.13174057006836, \t Total Dis Loss : 2.8439739253371954e-05\n",
      "Steps : 309800, \t Total Gen Loss : 32.799171447753906, \t Total Dis Loss : 1.6866550822669524e-06\n",
      "Steps : 309900, \t Total Gen Loss : 33.03352737426758, \t Total Dis Loss : 2.6535001325100893e-06\n",
      "Steps : 310000, \t Total Gen Loss : 33.05191421508789, \t Total Dis Loss : 1.5019065813248744e-06\n",
      "Steps : 310100, \t Total Gen Loss : 32.50587463378906, \t Total Dis Loss : 6.739441005265689e-07\n",
      "Steps : 310200, \t Total Gen Loss : 34.29912185668945, \t Total Dis Loss : 3.0126363981253235e-06\n",
      "Steps : 310300, \t Total Gen Loss : 28.018508911132812, \t Total Dis Loss : 5.628525832435116e-05\n",
      "Steps : 310400, \t Total Gen Loss : 31.420482635498047, \t Total Dis Loss : 4.391514085000381e-05\n",
      "Steps : 310500, \t Total Gen Loss : 30.98304557800293, \t Total Dis Loss : 1.6556923583266325e-05\n",
      "Steps : 310600, \t Total Gen Loss : 29.566844940185547, \t Total Dis Loss : 2.88709816231858e-05\n",
      "Steps : 310700, \t Total Gen Loss : 28.860702514648438, \t Total Dis Loss : 5.439615051727742e-05\n",
      "Steps : 310800, \t Total Gen Loss : 29.151033401489258, \t Total Dis Loss : 4.4583401177078485e-05\n",
      "Steps : 310900, \t Total Gen Loss : 31.60480499267578, \t Total Dis Loss : 1.4613202438340522e-05\n",
      "Steps : 311000, \t Total Gen Loss : 30.552303314208984, \t Total Dis Loss : 9.57255542743951e-06\n",
      "Steps : 311100, \t Total Gen Loss : 33.529212951660156, \t Total Dis Loss : 1.0689248483686242e-05\n",
      "Steps : 311200, \t Total Gen Loss : 30.757543563842773, \t Total Dis Loss : 3.296533577668015e-06\n",
      "Steps : 311300, \t Total Gen Loss : 30.669384002685547, \t Total Dis Loss : 1.084847463062033e-05\n",
      "Steps : 311400, \t Total Gen Loss : 33.95427703857422, \t Total Dis Loss : 1.2393358019835432e-06\n",
      "Steps : 311500, \t Total Gen Loss : 24.71660614013672, \t Total Dis Loss : 0.00022737613471690565\n",
      "Steps : 311600, \t Total Gen Loss : 31.430574417114258, \t Total Dis Loss : 4.890979198535206e-06\n",
      "Steps : 311700, \t Total Gen Loss : 30.788583755493164, \t Total Dis Loss : 0.00963667407631874\n",
      "Steps : 311800, \t Total Gen Loss : 31.572952270507812, \t Total Dis Loss : 3.375820233486593e-05\n",
      "Steps : 311900, \t Total Gen Loss : 31.129531860351562, \t Total Dis Loss : 8.534441803931259e-06\n",
      "Steps : 312000, \t Total Gen Loss : 35.83906173706055, \t Total Dis Loss : 1.1006591194018256e-06\n",
      "Steps : 312100, \t Total Gen Loss : 33.95408248901367, \t Total Dis Loss : 5.396959750214592e-06\n",
      "Steps : 312200, \t Total Gen Loss : 30.741783142089844, \t Total Dis Loss : 7.567836632915714e-07\n",
      "Steps : 312300, \t Total Gen Loss : 34.48125457763672, \t Total Dis Loss : 3.1412046155310236e-06\n",
      "Steps : 312400, \t Total Gen Loss : 34.109344482421875, \t Total Dis Loss : 1.2734409438053262e-06\n",
      "Steps : 312500, \t Total Gen Loss : 32.912353515625, \t Total Dis Loss : 8.741785677557345e-07\n",
      "Steps : 312600, \t Total Gen Loss : 32.64026641845703, \t Total Dis Loss : 9.233080163539853e-06\n",
      "Steps : 312700, \t Total Gen Loss : 35.57518768310547, \t Total Dis Loss : 1.217352746607503e-05\n",
      "Steps : 312800, \t Total Gen Loss : 31.82882308959961, \t Total Dis Loss : 1.7908681911649182e-05\n",
      "Steps : 312900, \t Total Gen Loss : 32.69390106201172, \t Total Dis Loss : 2.8252507036086172e-05\n",
      "Steps : 313000, \t Total Gen Loss : 31.74323844909668, \t Total Dis Loss : 5.375435989662947e-07\n",
      "Steps : 313100, \t Total Gen Loss : 30.381710052490234, \t Total Dis Loss : 6.4259306782332715e-06\n",
      "Steps : 313200, \t Total Gen Loss : 34.1213264465332, \t Total Dis Loss : 6.208735499058093e-07\n",
      "Steps : 313300, \t Total Gen Loss : 34.157249450683594, \t Total Dis Loss : 6.873945039842511e-06\n",
      "Steps : 313400, \t Total Gen Loss : 29.632986068725586, \t Total Dis Loss : 0.0006139127071946859\n",
      "Steps : 313500, \t Total Gen Loss : 25.56612777709961, \t Total Dis Loss : 0.0013575614430010319\n",
      "Steps : 313600, \t Total Gen Loss : 31.513416290283203, \t Total Dis Loss : 2.2938442270969972e-05\n",
      "Steps : 313700, \t Total Gen Loss : 34.919769287109375, \t Total Dis Loss : 3.149479744024575e-05\n",
      "Steps : 313800, \t Total Gen Loss : 29.797569274902344, \t Total Dis Loss : 4.722695848613512e-06\n",
      "Steps : 313900, \t Total Gen Loss : 33.136287689208984, \t Total Dis Loss : 1.4159902093524579e-05\n",
      "Steps : 314000, \t Total Gen Loss : 28.836387634277344, \t Total Dis Loss : 7.605292921653017e-06\n",
      "Steps : 314100, \t Total Gen Loss : 33.227500915527344, \t Total Dis Loss : 2.146578481188044e-05\n",
      "Steps : 314200, \t Total Gen Loss : 29.023799896240234, \t Total Dis Loss : 2.2612068278249353e-05\n",
      "Steps : 314300, \t Total Gen Loss : 28.76836585998535, \t Total Dis Loss : 5.002689431421459e-05\n",
      "Steps : 314400, \t Total Gen Loss : 31.02073860168457, \t Total Dis Loss : 6.969379228394246e-06\n",
      "Steps : 314500, \t Total Gen Loss : 32.906410217285156, \t Total Dis Loss : 2.441348806314636e-05\n",
      "Steps : 314600, \t Total Gen Loss : 29.228954315185547, \t Total Dis Loss : 0.00013989690341986716\n",
      "Steps : 314700, \t Total Gen Loss : 34.45273971557617, \t Total Dis Loss : 3.7895129025855567e-06\n",
      "Steps : 314800, \t Total Gen Loss : 31.794456481933594, \t Total Dis Loss : 1.5527055438724346e-05\n",
      "Steps : 314900, \t Total Gen Loss : 31.353187561035156, \t Total Dis Loss : 1.2599000910995528e-05\n",
      "Steps : 315000, \t Total Gen Loss : 31.51817512512207, \t Total Dis Loss : 7.822192856110632e-06\n",
      "Time for epoch 56 is 69.00283598899841 sec\n",
      "Steps : 315100, \t Total Gen Loss : 31.37044906616211, \t Total Dis Loss : 0.00044623707071878016\n",
      "Steps : 315200, \t Total Gen Loss : 31.02562713623047, \t Total Dis Loss : 3.07688674183737e-06\n",
      "Steps : 315300, \t Total Gen Loss : 31.59939956665039, \t Total Dis Loss : 7.767654096824117e-06\n",
      "Steps : 315400, \t Total Gen Loss : 32.88594055175781, \t Total Dis Loss : 3.183392982464284e-05\n",
      "Steps : 315500, \t Total Gen Loss : 32.37192153930664, \t Total Dis Loss : 1.465984223614214e-05\n",
      "Steps : 315600, \t Total Gen Loss : 32.15260314941406, \t Total Dis Loss : 2.5419817575311754e-06\n",
      "Steps : 315700, \t Total Gen Loss : 35.81992721557617, \t Total Dis Loss : 9.229692295775749e-06\n",
      "Steps : 315800, \t Total Gen Loss : 30.755294799804688, \t Total Dis Loss : 7.541545983258402e-06\n",
      "Steps : 315900, \t Total Gen Loss : 36.671836853027344, \t Total Dis Loss : 4.3480994804667716e-07\n",
      "Steps : 316000, \t Total Gen Loss : 32.85688781738281, \t Total Dis Loss : 3.6239102882973384e-06\n",
      "Steps : 316100, \t Total Gen Loss : 34.55362319946289, \t Total Dis Loss : 1.5957436971802963e-06\n",
      "Steps : 316200, \t Total Gen Loss : 34.23723602294922, \t Total Dis Loss : 2.495867420293507e-06\n",
      "Steps : 316300, \t Total Gen Loss : 30.994935989379883, \t Total Dis Loss : 2.390669578744564e-06\n",
      "Steps : 316400, \t Total Gen Loss : 33.83960723876953, \t Total Dis Loss : 9.855638154476765e-07\n",
      "Steps : 316500, \t Total Gen Loss : 33.946006774902344, \t Total Dis Loss : 6.450633009080775e-06\n",
      "Steps : 316600, \t Total Gen Loss : 31.424610137939453, \t Total Dis Loss : 3.928127625840716e-06\n",
      "Steps : 316700, \t Total Gen Loss : 33.668968200683594, \t Total Dis Loss : 2.0741490516229533e-06\n",
      "Steps : 316800, \t Total Gen Loss : 36.19799041748047, \t Total Dis Loss : 1.885596248030197e-07\n",
      "Steps : 316900, \t Total Gen Loss : 31.42631721496582, \t Total Dis Loss : 1.6919999552555964e-06\n",
      "Steps : 317000, \t Total Gen Loss : 39.80261993408203, \t Total Dis Loss : 2.562740746725467e-06\n",
      "Steps : 317100, \t Total Gen Loss : 39.676490783691406, \t Total Dis Loss : 0.05697556957602501\n",
      "Steps : 317200, \t Total Gen Loss : 32.913124084472656, \t Total Dis Loss : 3.7565787351923063e-06\n",
      "Steps : 317300, \t Total Gen Loss : 35.26993179321289, \t Total Dis Loss : 5.01808472108678e-06\n",
      "Steps : 317400, \t Total Gen Loss : 38.63645553588867, \t Total Dis Loss : 5.042873090133071e-05\n",
      "Steps : 317500, \t Total Gen Loss : 32.554466247558594, \t Total Dis Loss : 6.693072464258876e-06\n",
      "Steps : 317600, \t Total Gen Loss : 37.25176239013672, \t Total Dis Loss : 2.1128435037098825e-06\n",
      "Steps : 317700, \t Total Gen Loss : 34.747520446777344, \t Total Dis Loss : 1.3034459698246792e-05\n",
      "Steps : 317800, \t Total Gen Loss : 31.416784286499023, \t Total Dis Loss : 0.011614382266998291\n",
      "Steps : 317900, \t Total Gen Loss : 35.57489776611328, \t Total Dis Loss : 1.2889253412140533e-05\n",
      "Steps : 318000, \t Total Gen Loss : 31.78908920288086, \t Total Dis Loss : 2.464209137542639e-05\n",
      "Steps : 318100, \t Total Gen Loss : 33.43327331542969, \t Total Dis Loss : 1.0292786100762896e-05\n",
      "Steps : 318200, \t Total Gen Loss : 33.228721618652344, \t Total Dis Loss : 5.063717253506184e-06\n",
      "Steps : 318300, \t Total Gen Loss : 34.05894470214844, \t Total Dis Loss : 3.5483685678627808e-06\n",
      "Steps : 318400, \t Total Gen Loss : 34.750587463378906, \t Total Dis Loss : 2.7418809622759e-05\n",
      "Steps : 318500, \t Total Gen Loss : 34.866600036621094, \t Total Dis Loss : 2.8453191589505877e-06\n",
      "Steps : 318600, \t Total Gen Loss : 34.80070495605469, \t Total Dis Loss : 7.70343467593193e-06\n",
      "Steps : 318700, \t Total Gen Loss : 36.56083679199219, \t Total Dis Loss : 4.206897756375838e-06\n",
      "Steps : 318800, \t Total Gen Loss : 34.142581939697266, \t Total Dis Loss : 3.7642951156158233e-06\n",
      "Steps : 318900, \t Total Gen Loss : 35.318729400634766, \t Total Dis Loss : 5.703709575755056e-06\n",
      "Steps : 319000, \t Total Gen Loss : 34.649322509765625, \t Total Dis Loss : 1.2650085636778385e-06\n",
      "Steps : 319100, \t Total Gen Loss : 36.678279876708984, \t Total Dis Loss : 3.046165602427209e-06\n",
      "Steps : 319200, \t Total Gen Loss : 32.98653793334961, \t Total Dis Loss : 1.917142071761191e-06\n",
      "Steps : 319300, \t Total Gen Loss : 36.2172737121582, \t Total Dis Loss : 5.88655166211538e-06\n",
      "Steps : 319400, \t Total Gen Loss : 35.52756118774414, \t Total Dis Loss : 3.1387007766170427e-06\n",
      "Steps : 319500, \t Total Gen Loss : 32.329280853271484, \t Total Dis Loss : 4.75182241643779e-06\n",
      "Steps : 319600, \t Total Gen Loss : 35.89958190917969, \t Total Dis Loss : 2.296068942087004e-06\n",
      "Steps : 319700, \t Total Gen Loss : 35.5717887878418, \t Total Dis Loss : 4.010357315564761e-06\n",
      "Steps : 319800, \t Total Gen Loss : 32.38409423828125, \t Total Dis Loss : 1.2666605471167713e-05\n",
      "Steps : 319900, \t Total Gen Loss : 35.14545440673828, \t Total Dis Loss : 2.6206134862150066e-06\n",
      "Steps : 320000, \t Total Gen Loss : 32.55617904663086, \t Total Dis Loss : 1.3070579370833002e-05\n",
      "Steps : 320100, \t Total Gen Loss : 33.61322021484375, \t Total Dis Loss : 3.435918551986106e-05\n",
      "Steps : 320200, \t Total Gen Loss : 36.37602615356445, \t Total Dis Loss : 3.4653623970370973e-06\n",
      "Steps : 320300, \t Total Gen Loss : 37.159244537353516, \t Total Dis Loss : 5.405457613960607e-06\n",
      "Steps : 320400, \t Total Gen Loss : 36.21488952636719, \t Total Dis Loss : 8.862457434588578e-06\n",
      "Steps : 320500, \t Total Gen Loss : 32.91682052612305, \t Total Dis Loss : 2.5005865609273314e-06\n",
      "Steps : 320600, \t Total Gen Loss : 33.06765365600586, \t Total Dis Loss : 2.2565855033462867e-06\n",
      "Time for epoch 57 is 68.97646141052246 sec\n",
      "Steps : 320700, \t Total Gen Loss : 35.62705612182617, \t Total Dis Loss : 3.413634885873762e-06\n",
      "Steps : 320800, \t Total Gen Loss : 33.93898010253906, \t Total Dis Loss : 1.4708876733493526e-05\n",
      "Steps : 320900, \t Total Gen Loss : 30.69651985168457, \t Total Dis Loss : 2.2105532480054535e-05\n",
      "Steps : 321000, \t Total Gen Loss : 26.43838119506836, \t Total Dis Loss : 0.0005303893703967333\n",
      "Steps : 321100, \t Total Gen Loss : 34.619876861572266, \t Total Dis Loss : 4.2059036786668e-06\n",
      "Steps : 321200, \t Total Gen Loss : 32.368507385253906, \t Total Dis Loss : 6.788995960960165e-05\n",
      "Steps : 321300, \t Total Gen Loss : 32.283111572265625, \t Total Dis Loss : 1.4634529179602396e-05\n",
      "Steps : 321400, \t Total Gen Loss : 32.064666748046875, \t Total Dis Loss : 1.0574138286756352e-05\n",
      "Steps : 321500, \t Total Gen Loss : 34.01933288574219, \t Total Dis Loss : 7.321314478758723e-05\n",
      "Steps : 321600, \t Total Gen Loss : 32.30877685546875, \t Total Dis Loss : 3.495367536743288e-06\n",
      "Steps : 321700, \t Total Gen Loss : 32.08094787597656, \t Total Dis Loss : 5.823184983455576e-05\n",
      "Steps : 321800, \t Total Gen Loss : 34.37543487548828, \t Total Dis Loss : 3.1053523343871348e-06\n",
      "Steps : 321900, \t Total Gen Loss : 36.96843719482422, \t Total Dis Loss : 1.138764218922006e-05\n",
      "Steps : 322000, \t Total Gen Loss : 32.66236877441406, \t Total Dis Loss : 7.23389439372113e-06\n",
      "Steps : 322100, \t Total Gen Loss : 34.888511657714844, \t Total Dis Loss : 8.412352144659963e-06\n",
      "Steps : 322200, \t Total Gen Loss : 32.232566833496094, \t Total Dis Loss : 8.710604561201762e-06\n",
      "Steps : 322300, \t Total Gen Loss : 28.88496971130371, \t Total Dis Loss : 4.430722401593812e-05\n",
      "Steps : 322400, \t Total Gen Loss : 33.782676696777344, \t Total Dis Loss : 3.35186232405249e-05\n",
      "Steps : 322500, \t Total Gen Loss : 31.221710205078125, \t Total Dis Loss : 1.0356705388403498e-05\n",
      "Steps : 322600, \t Total Gen Loss : 29.703397750854492, \t Total Dis Loss : 2.074738767987583e-05\n",
      "Steps : 322700, \t Total Gen Loss : 34.739723205566406, \t Total Dis Loss : 1.4771610494790366e-06\n",
      "Steps : 322800, \t Total Gen Loss : 30.531042098999023, \t Total Dis Loss : 1.5533190889982507e-05\n",
      "Steps : 322900, \t Total Gen Loss : 31.0267333984375, \t Total Dis Loss : 2.453767228871584e-05\n",
      "Steps : 323000, \t Total Gen Loss : 31.566606521606445, \t Total Dis Loss : 4.3857580749318e-06\n",
      "Steps : 323100, \t Total Gen Loss : 32.58390808105469, \t Total Dis Loss : 8.172961315722205e-06\n",
      "Steps : 323200, \t Total Gen Loss : 32.452171325683594, \t Total Dis Loss : 5.285679890221218e-06\n",
      "Steps : 323300, \t Total Gen Loss : 34.028724670410156, \t Total Dis Loss : 1.0675020348571707e-05\n",
      "Steps : 323400, \t Total Gen Loss : 30.325069427490234, \t Total Dis Loss : 2.2955588065087795e-05\n",
      "Steps : 323500, \t Total Gen Loss : 31.977174758911133, \t Total Dis Loss : 3.863572783302516e-05\n",
      "Steps : 323600, \t Total Gen Loss : 32.0488166809082, \t Total Dis Loss : 5.649815193464747e-06\n",
      "Steps : 323700, \t Total Gen Loss : 32.10502624511719, \t Total Dis Loss : 1.0795155503728893e-05\n",
      "Steps : 323800, \t Total Gen Loss : 29.189905166625977, \t Total Dis Loss : 4.24759928137064e-05\n",
      "Steps : 323900, \t Total Gen Loss : 29.487266540527344, \t Total Dis Loss : 0.00022854265989735723\n",
      "Steps : 324000, \t Total Gen Loss : 35.694175720214844, \t Total Dis Loss : 1.1365516456862679e-06\n",
      "Steps : 324100, \t Total Gen Loss : 38.36082458496094, \t Total Dis Loss : 1.9959966266469564e-06\n",
      "Steps : 324200, \t Total Gen Loss : 37.079071044921875, \t Total Dis Loss : 3.6275673664931674e-06\n",
      "Steps : 324300, \t Total Gen Loss : 37.76251220703125, \t Total Dis Loss : 2.4555142772442196e-06\n",
      "Steps : 324400, \t Total Gen Loss : 39.5555419921875, \t Total Dis Loss : 1.568940547258535e-06\n",
      "Steps : 324500, \t Total Gen Loss : 38.83686065673828, \t Total Dis Loss : 2.139090383934672e-06\n",
      "Steps : 324600, \t Total Gen Loss : 37.558162689208984, \t Total Dis Loss : 4.105913376406534e-06\n",
      "Steps : 324700, \t Total Gen Loss : 35.543304443359375, \t Total Dis Loss : 2.080268814097508e-06\n",
      "Steps : 324800, \t Total Gen Loss : 35.818511962890625, \t Total Dis Loss : 2.388073153269943e-05\n",
      "Steps : 324900, \t Total Gen Loss : 33.88193893432617, \t Total Dis Loss : 1.9960771169280633e-05\n",
      "Steps : 325000, \t Total Gen Loss : 35.50666427612305, \t Total Dis Loss : 3.047384598175995e-05\n",
      "Steps : 325100, \t Total Gen Loss : 32.81938934326172, \t Total Dis Loss : 1.4770731468161102e-05\n",
      "Steps : 325200, \t Total Gen Loss : 35.711647033691406, \t Total Dis Loss : 1.0007546734414063e-05\n",
      "Steps : 325300, \t Total Gen Loss : 32.191246032714844, \t Total Dis Loss : 3.2288303373206872e-06\n",
      "Steps : 325400, \t Total Gen Loss : 32.62739944458008, \t Total Dis Loss : 1.011581662169192e-05\n",
      "Steps : 325500, \t Total Gen Loss : 32.54752731323242, \t Total Dis Loss : 5.9492100263014436e-05\n",
      "Steps : 325600, \t Total Gen Loss : 31.776165008544922, \t Total Dis Loss : 3.508373265503906e-05\n",
      "Steps : 325700, \t Total Gen Loss : 34.742855072021484, \t Total Dis Loss : 2.9759314656985225e-06\n",
      "Steps : 325800, \t Total Gen Loss : 35.38158416748047, \t Total Dis Loss : 1.8090287994709797e-05\n",
      "Steps : 325900, \t Total Gen Loss : 35.466182708740234, \t Total Dis Loss : 1.1168576747877523e-05\n",
      "Steps : 326000, \t Total Gen Loss : 33.786739349365234, \t Total Dis Loss : 2.7055511964135803e-05\n",
      "Steps : 326100, \t Total Gen Loss : 33.62617111206055, \t Total Dis Loss : 0.038155071437358856\n",
      "Steps : 326200, \t Total Gen Loss : 35.07626724243164, \t Total Dis Loss : 8.503525350533891e-06\n",
      "Time for epoch 58 is 68.98460793495178 sec\n",
      "Steps : 326300, \t Total Gen Loss : 34.802162170410156, \t Total Dis Loss : 1.611737025086768e-05\n",
      "Steps : 326400, \t Total Gen Loss : 31.593814849853516, \t Total Dis Loss : 2.369786670897156e-05\n",
      "Steps : 326500, \t Total Gen Loss : 31.596485137939453, \t Total Dis Loss : 3.127437594230287e-05\n",
      "Steps : 326600, \t Total Gen Loss : 32.88335418701172, \t Total Dis Loss : 2.237135049654171e-05\n",
      "Steps : 326700, \t Total Gen Loss : 28.984603881835938, \t Total Dis Loss : 0.0036825668066740036\n",
      "Steps : 326800, \t Total Gen Loss : 32.21211242675781, \t Total Dis Loss : 3.475090124993585e-05\n",
      "Steps : 326900, \t Total Gen Loss : 32.38587951660156, \t Total Dis Loss : 8.717758646525908e-06\n",
      "Steps : 327000, \t Total Gen Loss : 31.783466339111328, \t Total Dis Loss : 1.6355072148144245e-05\n",
      "Steps : 327100, \t Total Gen Loss : 29.50228500366211, \t Total Dis Loss : 1.2641163266380318e-05\n",
      "Steps : 327200, \t Total Gen Loss : 33.684776306152344, \t Total Dis Loss : 1.4951963294151938e-06\n",
      "Steps : 327300, \t Total Gen Loss : 31.6063232421875, \t Total Dis Loss : 1.995438833546359e-05\n",
      "Steps : 327400, \t Total Gen Loss : 35.2381591796875, \t Total Dis Loss : 4.0235927372123115e-06\n",
      "Steps : 327500, \t Total Gen Loss : 31.658496856689453, \t Total Dis Loss : 6.407620276149828e-06\n",
      "Steps : 327600, \t Total Gen Loss : 36.871665954589844, \t Total Dis Loss : 6.077664693293627e-06\n",
      "Steps : 327700, \t Total Gen Loss : 36.37174987792969, \t Total Dis Loss : 1.7501819456811063e-05\n",
      "Steps : 327800, \t Total Gen Loss : 38.1710205078125, \t Total Dis Loss : 1.3101922604619176e-06\n",
      "Steps : 327900, \t Total Gen Loss : 39.50788116455078, \t Total Dis Loss : 1.6787329286671593e-06\n",
      "Steps : 328000, \t Total Gen Loss : 35.41288375854492, \t Total Dis Loss : 2.6042973786388757e-06\n",
      "Steps : 328100, \t Total Gen Loss : 37.097557067871094, \t Total Dis Loss : 7.21481637810939e-06\n",
      "Steps : 328200, \t Total Gen Loss : 37.35493469238281, \t Total Dis Loss : 2.4441473556180426e-07\n",
      "Steps : 328300, \t Total Gen Loss : 33.267005920410156, \t Total Dis Loss : 0.00012142390187364072\n",
      "Steps : 328400, \t Total Gen Loss : 33.01993179321289, \t Total Dis Loss : 0.00024203739303629845\n",
      "Steps : 328500, \t Total Gen Loss : 34.75360870361328, \t Total Dis Loss : 1.6529100321349688e-05\n",
      "Steps : 328600, \t Total Gen Loss : 34.107177734375, \t Total Dis Loss : 1.6485793821630068e-05\n",
      "Steps : 328700, \t Total Gen Loss : 30.18779182434082, \t Total Dis Loss : 1.8232132788398303e-05\n",
      "Steps : 328800, \t Total Gen Loss : 34.01507568359375, \t Total Dis Loss : 4.508077836362645e-06\n",
      "Steps : 328900, \t Total Gen Loss : 35.77521514892578, \t Total Dis Loss : 2.7380206120142248e-06\n",
      "Steps : 329000, \t Total Gen Loss : 34.71768569946289, \t Total Dis Loss : 8.127331057039555e-06\n",
      "Steps : 329100, \t Total Gen Loss : 32.67792892456055, \t Total Dis Loss : 7.651588020962663e-06\n",
      "Steps : 329200, \t Total Gen Loss : 35.25061798095703, \t Total Dis Loss : 1.692671503406018e-05\n",
      "Steps : 329300, \t Total Gen Loss : 33.81155014038086, \t Total Dis Loss : 1.7483099554738146e-06\n",
      "Steps : 329400, \t Total Gen Loss : 32.0855712890625, \t Total Dis Loss : 7.650690349692013e-06\n",
      "Steps : 329500, \t Total Gen Loss : 34.584228515625, \t Total Dis Loss : 3.6447572711040266e-06\n",
      "Steps : 329600, \t Total Gen Loss : 35.07801818847656, \t Total Dis Loss : 1.1104299119324423e-05\n",
      "Steps : 329700, \t Total Gen Loss : 33.29259490966797, \t Total Dis Loss : 3.412580099393381e-06\n",
      "Steps : 329800, \t Total Gen Loss : 30.832252502441406, \t Total Dis Loss : 1.1143534720758907e-05\n",
      "Steps : 329900, \t Total Gen Loss : 36.327239990234375, \t Total Dis Loss : 1.1658347602860886e-06\n",
      "Steps : 330000, \t Total Gen Loss : 33.9489860534668, \t Total Dis Loss : 6.97176164976554e-06\n",
      "Steps : 330100, \t Total Gen Loss : 33.49222183227539, \t Total Dis Loss : 1.4260028819990112e-06\n",
      "Steps : 330200, \t Total Gen Loss : 33.370201110839844, \t Total Dis Loss : 3.009224201377947e-06\n",
      "Steps : 330300, \t Total Gen Loss : 33.6282844543457, \t Total Dis Loss : 7.197646937129321e-06\n",
      "Steps : 330400, \t Total Gen Loss : 33.869808197021484, \t Total Dis Loss : 7.152999614845612e-07\n",
      "Steps : 330500, \t Total Gen Loss : 31.908123016357422, \t Total Dis Loss : 1.5367022569989786e-06\n",
      "Steps : 330600, \t Total Gen Loss : 35.83174514770508, \t Total Dis Loss : 0.025582008063793182\n",
      "Steps : 330700, \t Total Gen Loss : 36.60906982421875, \t Total Dis Loss : 1.3426375517155975e-05\n",
      "Steps : 330800, \t Total Gen Loss : 33.312110900878906, \t Total Dis Loss : 7.191055919975042e-05\n",
      "Steps : 330900, \t Total Gen Loss : 33.29575729370117, \t Total Dis Loss : 4.309587529860437e-05\n",
      "Steps : 331000, \t Total Gen Loss : 32.990928649902344, \t Total Dis Loss : 6.192954060679767e-06\n",
      "Steps : 331100, \t Total Gen Loss : 35.342041015625, \t Total Dis Loss : 2.0170540437902673e-07\n",
      "Steps : 331200, \t Total Gen Loss : 31.944162368774414, \t Total Dis Loss : 1.412411165802041e-06\n",
      "Steps : 331300, \t Total Gen Loss : 31.790790557861328, \t Total Dis Loss : 1.2831069398089312e-06\n",
      "Steps : 331400, \t Total Gen Loss : 31.607749938964844, \t Total Dis Loss : 1.6566617659918847e-06\n",
      "Steps : 331500, \t Total Gen Loss : 34.088375091552734, \t Total Dis Loss : 1.5277751117537264e-06\n",
      "Steps : 331600, \t Total Gen Loss : 35.456153869628906, \t Total Dis Loss : 0.00013907605898566544\n",
      "Steps : 331700, \t Total Gen Loss : 30.023141860961914, \t Total Dis Loss : 6.400154961738735e-05\n",
      "Steps : 331800, \t Total Gen Loss : 33.50543975830078, \t Total Dis Loss : 5.125208190293051e-05\n",
      "Time for epoch 59 is 68.9961187839508 sec\n",
      "Steps : 331900, \t Total Gen Loss : 31.669605255126953, \t Total Dis Loss : 0.0001265982136828825\n",
      "Steps : 332000, \t Total Gen Loss : 30.396533966064453, \t Total Dis Loss : 4.9711194151313975e-05\n",
      "Steps : 332100, \t Total Gen Loss : 34.98005676269531, \t Total Dis Loss : 1.2480020359362243e-06\n",
      "Steps : 332200, \t Total Gen Loss : 32.52857971191406, \t Total Dis Loss : 2.6535755750956014e-05\n",
      "Steps : 332300, \t Total Gen Loss : 32.40149688720703, \t Total Dis Loss : 2.9362701752688736e-05\n",
      "Steps : 332400, \t Total Gen Loss : 36.77446746826172, \t Total Dis Loss : 1.1612659136517323e-06\n",
      "Steps : 332500, \t Total Gen Loss : 36.29570770263672, \t Total Dis Loss : 4.411730358810928e-08\n",
      "Steps : 332600, \t Total Gen Loss : 37.36749267578125, \t Total Dis Loss : 4.986343924429093e-07\n",
      "Steps : 332700, \t Total Gen Loss : 35.95549011230469, \t Total Dis Loss : 2.5028823529282818e-06\n",
      "Steps : 332800, \t Total Gen Loss : 37.778961181640625, \t Total Dis Loss : 1.972949803530355e-06\n",
      "Steps : 332900, \t Total Gen Loss : 31.68923568725586, \t Total Dis Loss : 1.983445145015139e-06\n",
      "Steps : 333000, \t Total Gen Loss : 35.93770217895508, \t Total Dis Loss : 8.509888971275359e-07\n",
      "Steps : 333100, \t Total Gen Loss : 37.808746337890625, \t Total Dis Loss : 7.051210673125752e-07\n",
      "Steps : 333200, \t Total Gen Loss : 36.56928634643555, \t Total Dis Loss : 2.3206372645745432e-07\n",
      "Steps : 333300, \t Total Gen Loss : 36.13056182861328, \t Total Dis Loss : 1.0946556358248927e-05\n",
      "Steps : 333400, \t Total Gen Loss : 38.79249572753906, \t Total Dis Loss : 7.110652688879782e-08\n",
      "Steps : 333500, \t Total Gen Loss : 35.15597915649414, \t Total Dis Loss : 2.131302608177066e-06\n",
      "Steps : 333600, \t Total Gen Loss : 37.662723541259766, \t Total Dis Loss : 1.3105195648677181e-06\n",
      "Steps : 333700, \t Total Gen Loss : 33.4579963684082, \t Total Dis Loss : 4.0543377508583944e-06\n",
      "Steps : 333800, \t Total Gen Loss : 38.546417236328125, \t Total Dis Loss : 8.95812775070226e-07\n",
      "Steps : 333900, \t Total Gen Loss : 34.73102951049805, \t Total Dis Loss : 5.280147874486829e-08\n",
      "Steps : 334000, \t Total Gen Loss : 36.39507293701172, \t Total Dis Loss : 8.687533892270949e-08\n",
      "Steps : 334100, \t Total Gen Loss : 34.443260192871094, \t Total Dis Loss : 5.367414814827498e-07\n",
      "Steps : 334200, \t Total Gen Loss : 31.71540641784668, \t Total Dis Loss : 4.5172330942477856e-07\n",
      "Steps : 334300, \t Total Gen Loss : 36.44976043701172, \t Total Dis Loss : 9.348599405711866e-08\n",
      "Steps : 334400, \t Total Gen Loss : 33.038185119628906, \t Total Dis Loss : 3.6754974530595064e-07\n",
      "Steps : 334500, \t Total Gen Loss : 33.5257568359375, \t Total Dis Loss : 2.298861829785892e-07\n",
      "Steps : 334600, \t Total Gen Loss : 35.090599060058594, \t Total Dis Loss : 1.2284489230296458e-06\n",
      "Steps : 334700, \t Total Gen Loss : 34.641178131103516, \t Total Dis Loss : 3.9122994621720864e-07\n",
      "Steps : 334800, \t Total Gen Loss : 33.326515197753906, \t Total Dis Loss : 1.0051545586975408e-06\n",
      "Steps : 334900, \t Total Gen Loss : 31.69751739501953, \t Total Dis Loss : 7.533573125328985e-07\n",
      "Steps : 335000, \t Total Gen Loss : 36.982078552246094, \t Total Dis Loss : 6.598349045816576e-06\n",
      "Steps : 335100, \t Total Gen Loss : 33.818336486816406, \t Total Dis Loss : 0.000403479760279879\n",
      "Steps : 335200, \t Total Gen Loss : 37.03779983520508, \t Total Dis Loss : 4.656369299027574e-07\n",
      "Steps : 335300, \t Total Gen Loss : 32.58478546142578, \t Total Dis Loss : 1.995189450099133e-06\n",
      "Steps : 335400, \t Total Gen Loss : 40.204864501953125, \t Total Dis Loss : 1.9806570890068542e-06\n",
      "Steps : 335500, \t Total Gen Loss : 32.86811065673828, \t Total Dis Loss : 9.987464864025242e-07\n",
      "Steps : 335600, \t Total Gen Loss : 32.654788970947266, \t Total Dis Loss : 3.3280221032327972e-06\n",
      "Steps : 335700, \t Total Gen Loss : 29.435638427734375, \t Total Dis Loss : 1.3211514669819735e-05\n",
      "Steps : 335800, \t Total Gen Loss : 34.754066467285156, \t Total Dis Loss : 1.918778616527561e-05\n",
      "Steps : 335900, \t Total Gen Loss : 34.52903747558594, \t Total Dis Loss : 5.341996256902348e-06\n",
      "Steps : 336000, \t Total Gen Loss : 28.670700073242188, \t Total Dis Loss : 5.707541731680976e-06\n",
      "Steps : 336100, \t Total Gen Loss : 33.29745101928711, \t Total Dis Loss : 2.9064665341138607e-07\n",
      "Steps : 336200, \t Total Gen Loss : 34.268028259277344, \t Total Dis Loss : 3.413475269553601e-06\n",
      "Steps : 336300, \t Total Gen Loss : 32.86626052856445, \t Total Dis Loss : 6.49790763418423e-07\n",
      "Steps : 336400, \t Total Gen Loss : 29.85830307006836, \t Total Dis Loss : 2.779874012048822e-05\n",
      "Steps : 336500, \t Total Gen Loss : 32.98210144042969, \t Total Dis Loss : 5.407894150266657e-07\n",
      "Steps : 336600, \t Total Gen Loss : 43.04617691040039, \t Total Dis Loss : 8.834354957798496e-06\n",
      "Steps : 336700, \t Total Gen Loss : 34.6181640625, \t Total Dis Loss : 1.0855073014681693e-05\n",
      "Steps : 336800, \t Total Gen Loss : 34.1494026184082, \t Total Dis Loss : 8.51740605867235e-06\n",
      "Steps : 336900, \t Total Gen Loss : 31.317577362060547, \t Total Dis Loss : 8.336298924405128e-05\n",
      "Steps : 337000, \t Total Gen Loss : 30.06926727294922, \t Total Dis Loss : 2.530429082980845e-05\n",
      "Steps : 337100, \t Total Gen Loss : 30.522693634033203, \t Total Dis Loss : 0.0003489898517727852\n",
      "Steps : 337200, \t Total Gen Loss : 33.552978515625, \t Total Dis Loss : 9.055950613401365e-06\n",
      "Steps : 337300, \t Total Gen Loss : 38.8323974609375, \t Total Dis Loss : 5.194024197407998e-07\n",
      "Steps : 337400, \t Total Gen Loss : 38.95606994628906, \t Total Dis Loss : 1.9213935047446284e-06\n",
      "Steps : 337500, \t Total Gen Loss : 37.28689193725586, \t Total Dis Loss : 2.0507561657723272e-06\n",
      "Time for epoch 60 is 69.39614677429199 sec\n",
      "Steps : 337600, \t Total Gen Loss : 31.504064559936523, \t Total Dis Loss : 1.302708733419422e-05\n",
      "Steps : 337700, \t Total Gen Loss : 34.58893585205078, \t Total Dis Loss : 0.0013881111517548561\n",
      "Steps : 337800, \t Total Gen Loss : 33.27125930786133, \t Total Dis Loss : 1.552865251142066e-05\n",
      "Steps : 337900, \t Total Gen Loss : 32.2357177734375, \t Total Dis Loss : 6.032334113115212e-06\n",
      "Steps : 338000, \t Total Gen Loss : 30.68404197692871, \t Total Dis Loss : 5.337035872798879e-06\n",
      "Steps : 338100, \t Total Gen Loss : 34.310516357421875, \t Total Dis Loss : 7.892620487837121e-06\n",
      "Steps : 338200, \t Total Gen Loss : 34.52626419067383, \t Total Dis Loss : 3.517185632517794e-06\n",
      "Steps : 338300, \t Total Gen Loss : 33.88577651977539, \t Total Dis Loss : 9.831344868871383e-06\n",
      "Steps : 338400, \t Total Gen Loss : 33.418663024902344, \t Total Dis Loss : 5.055653673480265e-06\n",
      "Steps : 338500, \t Total Gen Loss : 35.24074172973633, \t Total Dis Loss : 1.3714654414798133e-05\n",
      "Steps : 338600, \t Total Gen Loss : 31.79793930053711, \t Total Dis Loss : 5.6055610002658796e-06\n",
      "Steps : 338700, \t Total Gen Loss : 30.4585018157959, \t Total Dis Loss : 1.6500762285431847e-05\n",
      "Steps : 338800, \t Total Gen Loss : 36.20384979248047, \t Total Dis Loss : 3.220020062144613e-06\n",
      "Steps : 338900, \t Total Gen Loss : 31.543746948242188, \t Total Dis Loss : 1.0780414413602557e-05\n",
      "Steps : 339000, \t Total Gen Loss : 33.962890625, \t Total Dis Loss : 4.9652830966806505e-06\n",
      "Steps : 339100, \t Total Gen Loss : 33.74565505981445, \t Total Dis Loss : 2.1269390799716348e-06\n",
      "Steps : 339200, \t Total Gen Loss : 34.85125732421875, \t Total Dis Loss : 2.2907172024133615e-05\n",
      "Steps : 339300, \t Total Gen Loss : 35.39171600341797, \t Total Dis Loss : 8.657820217194967e-06\n",
      "Steps : 339400, \t Total Gen Loss : 35.76298141479492, \t Total Dis Loss : 1.577723196533043e-05\n",
      "Steps : 339500, \t Total Gen Loss : 32.891944885253906, \t Total Dis Loss : 1.4415131772693712e-06\n",
      "Steps : 339600, \t Total Gen Loss : 36.53104019165039, \t Total Dis Loss : 2.5397653189429548e-06\n",
      "Steps : 339700, \t Total Gen Loss : 36.81813430786133, \t Total Dis Loss : 2.2562836932138453e-07\n",
      "Steps : 339800, \t Total Gen Loss : 41.07121276855469, \t Total Dis Loss : 1.1030955420210375e-06\n",
      "Steps : 339900, \t Total Gen Loss : 33.01368713378906, \t Total Dis Loss : 9.223373353961506e-07\n",
      "Steps : 340000, \t Total Gen Loss : 38.58171081542969, \t Total Dis Loss : 1.560171313030878e-06\n",
      "Steps : 340100, \t Total Gen Loss : 32.1292610168457, \t Total Dis Loss : 5.762181899626739e-06\n",
      "Steps : 340200, \t Total Gen Loss : 35.996124267578125, \t Total Dis Loss : 2.642619278958591e-07\n",
      "Steps : 340300, \t Total Gen Loss : 33.51437759399414, \t Total Dis Loss : 3.527831040628371e-07\n",
      "Steps : 340400, \t Total Gen Loss : 36.13957977294922, \t Total Dis Loss : 2.5657805053924676e-06\n",
      "Steps : 340500, \t Total Gen Loss : 33.71228790283203, \t Total Dis Loss : 1.815634732338367e-06\n",
      "Steps : 340600, \t Total Gen Loss : 35.618167877197266, \t Total Dis Loss : 7.798689694027416e-07\n",
      "Steps : 340700, \t Total Gen Loss : 35.16019821166992, \t Total Dis Loss : 2.1264881070237607e-05\n",
      "Steps : 340800, \t Total Gen Loss : 31.92571258544922, \t Total Dis Loss : 6.094146556279156e-06\n",
      "Steps : 340900, \t Total Gen Loss : 34.548072814941406, \t Total Dis Loss : 5.840763606101973e-06\n",
      "Steps : 341000, \t Total Gen Loss : 36.75608825683594, \t Total Dis Loss : 2.165188197977841e-05\n",
      "Steps : 341100, \t Total Gen Loss : 35.089942932128906, \t Total Dis Loss : 5.43571513844654e-06\n",
      "Steps : 341200, \t Total Gen Loss : 31.23777198791504, \t Total Dis Loss : 0.0002833592297974974\n",
      "Steps : 341300, \t Total Gen Loss : 32.66773986816406, \t Total Dis Loss : 0.0023559522815048695\n",
      "Steps : 341400, \t Total Gen Loss : 33.14963150024414, \t Total Dis Loss : 8.614533726358786e-05\n",
      "Steps : 341500, \t Total Gen Loss : 31.43313980102539, \t Total Dis Loss : 0.00012740778038278222\n",
      "Steps : 341600, \t Total Gen Loss : 31.266773223876953, \t Total Dis Loss : 3.172595097566955e-05\n",
      "Steps : 341700, \t Total Gen Loss : 29.485713958740234, \t Total Dis Loss : 4.664052084990544e-06\n",
      "Steps : 341800, \t Total Gen Loss : 30.70612144470215, \t Total Dis Loss : 4.253158749634167e-06\n",
      "Steps : 341900, \t Total Gen Loss : 32.0992546081543, \t Total Dis Loss : 7.187173468992114e-05\n",
      "Steps : 342000, \t Total Gen Loss : 30.152212142944336, \t Total Dis Loss : 5.394869367592037e-05\n",
      "Steps : 342100, \t Total Gen Loss : 35.798301696777344, \t Total Dis Loss : 2.903060931203072e-06\n",
      "Steps : 342200, \t Total Gen Loss : 30.41684913635254, \t Total Dis Loss : 5.348196282284334e-06\n",
      "Steps : 342300, \t Total Gen Loss : 34.35048294067383, \t Total Dis Loss : 3.726005843418534e-06\n",
      "Steps : 342400, \t Total Gen Loss : 30.5860595703125, \t Total Dis Loss : 6.938326987437904e-05\n",
      "Steps : 342500, \t Total Gen Loss : 34.270450592041016, \t Total Dis Loss : 1.7760181663106778e-06\n",
      "Steps : 342600, \t Total Gen Loss : 31.269428253173828, \t Total Dis Loss : 1.5123694083740702e-06\n",
      "Steps : 342700, \t Total Gen Loss : 31.66065216064453, \t Total Dis Loss : 7.053621811792254e-05\n",
      "Steps : 342800, \t Total Gen Loss : 34.10429763793945, \t Total Dis Loss : 2.195730075982283e-06\n",
      "Steps : 342900, \t Total Gen Loss : 32.884483337402344, \t Total Dis Loss : 2.507982571842149e-06\n",
      "Steps : 343000, \t Total Gen Loss : 36.755279541015625, \t Total Dis Loss : 8.625767077319324e-05\n",
      "Steps : 343100, \t Total Gen Loss : 33.74303436279297, \t Total Dis Loss : 3.386396201676689e-05\n",
      "Time for epoch 61 is 69.01279187202454 sec\n",
      "Steps : 343200, \t Total Gen Loss : 30.08175277709961, \t Total Dis Loss : 2.6463701487955404e-06\n",
      "Steps : 343300, \t Total Gen Loss : 34.17296600341797, \t Total Dis Loss : 7.020453267614357e-07\n",
      "Steps : 343400, \t Total Gen Loss : 28.671863555908203, \t Total Dis Loss : 4.648684353014687e-06\n",
      "Steps : 343500, \t Total Gen Loss : 32.17909240722656, \t Total Dis Loss : 3.6285312035033712e-06\n",
      "Steps : 343600, \t Total Gen Loss : 35.2186164855957, \t Total Dis Loss : 9.40032350627007e-07\n",
      "Steps : 343700, \t Total Gen Loss : 34.86534881591797, \t Total Dis Loss : 7.943650416564196e-06\n",
      "Steps : 343800, \t Total Gen Loss : 34.01195526123047, \t Total Dis Loss : 3.7964019838909735e-07\n",
      "Steps : 343900, \t Total Gen Loss : 30.401559829711914, \t Total Dis Loss : 5.787749159935629e-06\n",
      "Steps : 344000, \t Total Gen Loss : 31.343050003051758, \t Total Dis Loss : 4.2012177914330096e-07\n",
      "Steps : 344100, \t Total Gen Loss : 35.39588165283203, \t Total Dis Loss : 7.058656592562329e-06\n",
      "Steps : 344200, \t Total Gen Loss : 38.644798278808594, \t Total Dis Loss : 4.375714866000635e-07\n",
      "Steps : 344300, \t Total Gen Loss : 32.95945739746094, \t Total Dis Loss : 4.128514774492942e-07\n",
      "Steps : 344400, \t Total Gen Loss : 30.596416473388672, \t Total Dis Loss : 0.0002630034869071096\n",
      "Steps : 344500, \t Total Gen Loss : 28.85774803161621, \t Total Dis Loss : 3.466446287347935e-05\n",
      "Steps : 344600, \t Total Gen Loss : 29.696439743041992, \t Total Dis Loss : 4.137942232773639e-05\n",
      "Steps : 344700, \t Total Gen Loss : 30.416828155517578, \t Total Dis Loss : 3.788449976127595e-05\n",
      "Steps : 344800, \t Total Gen Loss : 31.740196228027344, \t Total Dis Loss : 3.404037852305919e-05\n",
      "Steps : 344900, \t Total Gen Loss : 28.251140594482422, \t Total Dis Loss : 0.00012367578165140003\n",
      "Steps : 345000, \t Total Gen Loss : 33.35536193847656, \t Total Dis Loss : 1.2853811313107144e-05\n",
      "Steps : 345100, \t Total Gen Loss : 32.67886734008789, \t Total Dis Loss : 5.623963716061553e-06\n",
      "Steps : 345200, \t Total Gen Loss : 32.34134292602539, \t Total Dis Loss : 2.221245267719496e-05\n",
      "Steps : 345300, \t Total Gen Loss : 36.79588317871094, \t Total Dis Loss : 2.4832283997966442e-06\n",
      "Steps : 345400, \t Total Gen Loss : 38.42731857299805, \t Total Dis Loss : 8.797058512755029e-07\n",
      "Steps : 345500, \t Total Gen Loss : 34.74399185180664, \t Total Dis Loss : 5.733717443945352e-06\n",
      "Steps : 345600, \t Total Gen Loss : 35.83976745605469, \t Total Dis Loss : 1.0989484735546284e-06\n",
      "Steps : 345700, \t Total Gen Loss : 31.98839569091797, \t Total Dis Loss : 6.208587365108542e-06\n",
      "Steps : 345800, \t Total Gen Loss : 31.723342895507812, \t Total Dis Loss : 0.00013584032421931624\n",
      "Steps : 345900, \t Total Gen Loss : 31.724876403808594, \t Total Dis Loss : 8.219550181820523e-06\n",
      "Steps : 346000, \t Total Gen Loss : 32.295379638671875, \t Total Dis Loss : 2.3508021058660233e-06\n",
      "Steps : 346100, \t Total Gen Loss : 33.93144607543945, \t Total Dis Loss : 1.0742877748270985e-05\n",
      "Steps : 346200, \t Total Gen Loss : 33.745079040527344, \t Total Dis Loss : 2.263397163915215e-06\n",
      "Steps : 346300, \t Total Gen Loss : 36.05845642089844, \t Total Dis Loss : 1.6032701921631087e-07\n",
      "Steps : 346400, \t Total Gen Loss : 35.74586486816406, \t Total Dis Loss : 1.388716896144615e-06\n",
      "Steps : 346500, \t Total Gen Loss : 37.142311096191406, \t Total Dis Loss : 4.3368640945118386e-07\n",
      "Steps : 346600, \t Total Gen Loss : 35.722572326660156, \t Total Dis Loss : 2.874342612813052e-07\n",
      "Steps : 346700, \t Total Gen Loss : 30.062158584594727, \t Total Dis Loss : 6.324850119199255e-07\n",
      "Steps : 346800, \t Total Gen Loss : 37.27088928222656, \t Total Dis Loss : 1.343637336503889e-06\n",
      "Steps : 346900, \t Total Gen Loss : 32.84419250488281, \t Total Dis Loss : 1.934135434566997e-05\n",
      "Steps : 347000, \t Total Gen Loss : 33.56296920776367, \t Total Dis Loss : 3.1768563530931715e-06\n",
      "Steps : 347100, \t Total Gen Loss : 34.04278564453125, \t Total Dis Loss : 1.1264926797593944e-05\n",
      "Steps : 347200, \t Total Gen Loss : 32.6046142578125, \t Total Dis Loss : 1.550491447233071e-06\n",
      "Steps : 347300, \t Total Gen Loss : 31.390024185180664, \t Total Dis Loss : 1.3310769645613618e-05\n",
      "Steps : 347400, \t Total Gen Loss : 31.595809936523438, \t Total Dis Loss : 9.671523457654985e-07\n",
      "Steps : 347500, \t Total Gen Loss : 36.00831604003906, \t Total Dis Loss : 4.5435035644914024e-07\n",
      "Steps : 347600, \t Total Gen Loss : 30.935035705566406, \t Total Dis Loss : 2.1037994883954525e-06\n",
      "Steps : 347700, \t Total Gen Loss : 34.52283477783203, \t Total Dis Loss : 7.306382485694485e-07\n",
      "Steps : 347800, \t Total Gen Loss : 31.883935928344727, \t Total Dis Loss : 1.558821622893447e-06\n",
      "Steps : 347900, \t Total Gen Loss : 31.585372924804688, \t Total Dis Loss : 4.692314632848138e-06\n",
      "Steps : 348000, \t Total Gen Loss : 32.33281707763672, \t Total Dis Loss : 1.3847435411662445e-06\n",
      "Steps : 348100, \t Total Gen Loss : 31.281949996948242, \t Total Dis Loss : 1.9777266061282717e-05\n",
      "Steps : 348200, \t Total Gen Loss : 36.887603759765625, \t Total Dis Loss : 7.040030141070019e-07\n",
      "Steps : 348300, \t Total Gen Loss : 32.76678466796875, \t Total Dis Loss : 2.654108902788721e-05\n",
      "Steps : 348400, \t Total Gen Loss : 34.11432647705078, \t Total Dis Loss : 6.8024787651665974e-06\n",
      "Steps : 348500, \t Total Gen Loss : 32.392921447753906, \t Total Dis Loss : 3.0268497539509553e-06\n",
      "Steps : 348600, \t Total Gen Loss : 34.52294158935547, \t Total Dis Loss : 2.825771389325382e-06\n",
      "Steps : 348700, \t Total Gen Loss : 34.900535583496094, \t Total Dis Loss : 9.280605013373133e-07\n",
      "Time for epoch 62 is 68.99524474143982 sec\n",
      "Steps : 348800, \t Total Gen Loss : 32.87281036376953, \t Total Dis Loss : 1.3617606782645453e-05\n",
      "Steps : 348900, \t Total Gen Loss : 32.88970947265625, \t Total Dis Loss : 1.911918843688909e-06\n",
      "Steps : 349000, \t Total Gen Loss : 35.254302978515625, \t Total Dis Loss : 4.00664293920272e-06\n",
      "Steps : 349100, \t Total Gen Loss : 39.03309631347656, \t Total Dis Loss : 5.124039716974949e-07\n",
      "Steps : 349200, \t Total Gen Loss : 38.25727844238281, \t Total Dis Loss : 4.829328645428177e-06\n",
      "Steps : 349300, \t Total Gen Loss : 34.61118698120117, \t Total Dis Loss : 6.6112165768572595e-06\n",
      "Steps : 349400, \t Total Gen Loss : 28.879745483398438, \t Total Dis Loss : 0.00019214977510273457\n",
      "Steps : 349500, \t Total Gen Loss : 32.331825256347656, \t Total Dis Loss : 3.6719775380333886e-06\n",
      "Steps : 349600, \t Total Gen Loss : 31.584026336669922, \t Total Dis Loss : 9.318587217421737e-06\n",
      "Steps : 349700, \t Total Gen Loss : 34.16557693481445, \t Total Dis Loss : 4.422538950166199e-06\n",
      "Steps : 349800, \t Total Gen Loss : 34.47469711303711, \t Total Dis Loss : 4.268910743121523e-06\n",
      "Steps : 349900, \t Total Gen Loss : 36.11667251586914, \t Total Dis Loss : 1.0724129424488638e-06\n",
      "Steps : 350000, \t Total Gen Loss : 34.02375030517578, \t Total Dis Loss : 2.450433385092765e-06\n",
      "Steps : 350100, \t Total Gen Loss : 40.51214599609375, \t Total Dis Loss : 0.0012960013700649142\n",
      "Steps : 350200, \t Total Gen Loss : 38.91139221191406, \t Total Dis Loss : 1.2563382369989995e-05\n",
      "Steps : 350300, \t Total Gen Loss : 36.43156433105469, \t Total Dis Loss : 4.8110423449543305e-06\n",
      "Steps : 350400, \t Total Gen Loss : 37.923187255859375, \t Total Dis Loss : 4.731398803414777e-06\n",
      "Steps : 350500, \t Total Gen Loss : 36.367618560791016, \t Total Dis Loss : 3.6145385820418596e-05\n",
      "Steps : 350600, \t Total Gen Loss : 34.85841369628906, \t Total Dis Loss : 2.1201135496085044e-06\n",
      "Steps : 350700, \t Total Gen Loss : 36.04115295410156, \t Total Dis Loss : 1.314261771767633e-05\n",
      "Steps : 350800, \t Total Gen Loss : 35.219329833984375, \t Total Dis Loss : 2.1298004867276177e-05\n",
      "Steps : 350900, \t Total Gen Loss : 37.83557891845703, \t Total Dis Loss : 1.0551708555794903e-06\n",
      "Steps : 351000, \t Total Gen Loss : 36.06359100341797, \t Total Dis Loss : 4.758394425152801e-06\n",
      "Steps : 351100, \t Total Gen Loss : 36.97875213623047, \t Total Dis Loss : 2.178913746320177e-06\n",
      "Steps : 351200, \t Total Gen Loss : 37.39299774169922, \t Total Dis Loss : 1.4684353573102271e-06\n",
      "Steps : 351300, \t Total Gen Loss : 37.62400817871094, \t Total Dis Loss : 5.483943823492154e-07\n",
      "Steps : 351400, \t Total Gen Loss : 29.961265563964844, \t Total Dis Loss : 8.595008694101125e-06\n",
      "Steps : 351500, \t Total Gen Loss : 39.327293395996094, \t Total Dis Loss : 1.338554375251988e-05\n",
      "Steps : 351600, \t Total Gen Loss : 38.027259826660156, \t Total Dis Loss : 6.030439863025094e-07\n",
      "Steps : 351700, \t Total Gen Loss : 35.88504409790039, \t Total Dis Loss : 1.3430739045361406e-06\n",
      "Steps : 351800, \t Total Gen Loss : 36.064430236816406, \t Total Dis Loss : 8.13463088888966e-07\n",
      "Steps : 351900, \t Total Gen Loss : 33.410858154296875, \t Total Dis Loss : 1.683687514741905e-06\n",
      "Steps : 352000, \t Total Gen Loss : 35.071128845214844, \t Total Dis Loss : 2.8845201995864045e-06\n",
      "Steps : 352100, \t Total Gen Loss : 28.100875854492188, \t Total Dis Loss : 0.00012594956206157804\n",
      "Steps : 352200, \t Total Gen Loss : 31.288448333740234, \t Total Dis Loss : 8.255690045189112e-05\n",
      "Steps : 352300, \t Total Gen Loss : 28.769067764282227, \t Total Dis Loss : 4.0486174839315936e-05\n",
      "Steps : 352400, \t Total Gen Loss : 33.00407409667969, \t Total Dis Loss : 9.761368710314855e-05\n",
      "Steps : 352500, \t Total Gen Loss : 34.399513244628906, \t Total Dis Loss : 9.860747240963974e-07\n",
      "Steps : 352600, \t Total Gen Loss : 30.847888946533203, \t Total Dis Loss : 2.9690856990782777e-06\n",
      "Steps : 352700, \t Total Gen Loss : 33.765838623046875, \t Total Dis Loss : 1.7837188579505892e-06\n",
      "Steps : 352800, \t Total Gen Loss : 35.37853240966797, \t Total Dis Loss : 4.955040822096635e-07\n",
      "Steps : 352900, \t Total Gen Loss : 37.673641204833984, \t Total Dis Loss : 1.0716365750340628e-06\n",
      "Steps : 353000, \t Total Gen Loss : 34.55461502075195, \t Total Dis Loss : 1.2181274541944731e-05\n",
      "Steps : 353100, \t Total Gen Loss : 38.85731506347656, \t Total Dis Loss : 1.2002924449916463e-06\n",
      "Steps : 353200, \t Total Gen Loss : 37.485774993896484, \t Total Dis Loss : 6.886992196086794e-06\n",
      "Steps : 353300, \t Total Gen Loss : 37.32276916503906, \t Total Dis Loss : 7.187543928921514e-07\n",
      "Steps : 353400, \t Total Gen Loss : 35.44737243652344, \t Total Dis Loss : 2.287428742420161e-06\n",
      "Steps : 353500, \t Total Gen Loss : 35.087852478027344, \t Total Dis Loss : 3.2229132784777903e-07\n",
      "Steps : 353600, \t Total Gen Loss : 36.31721496582031, \t Total Dis Loss : 2.8026408926962176e-06\n",
      "Steps : 353700, \t Total Gen Loss : 34.66120529174805, \t Total Dis Loss : 4.844468094233889e-06\n",
      "Steps : 353800, \t Total Gen Loss : 34.921836853027344, \t Total Dis Loss : 3.7851170873182127e-06\n",
      "Steps : 353900, \t Total Gen Loss : 39.565277099609375, \t Total Dis Loss : 2.4088302552627283e-07\n",
      "Steps : 354000, \t Total Gen Loss : 31.608243942260742, \t Total Dis Loss : 8.453365012428549e-07\n",
      "Steps : 354100, \t Total Gen Loss : 33.308265686035156, \t Total Dis Loss : 1.4995667697803583e-05\n",
      "Steps : 354200, \t Total Gen Loss : 32.94873809814453, \t Total Dis Loss : 2.321840611330117e-06\n",
      "Steps : 354300, \t Total Gen Loss : 32.47360610961914, \t Total Dis Loss : 1.4395039215742145e-05\n",
      "Time for epoch 63 is 68.973628282547 sec\n",
      "Steps : 354400, \t Total Gen Loss : 32.06982421875, \t Total Dis Loss : 1.1141835784655996e-05\n",
      "Steps : 354500, \t Total Gen Loss : 31.639026641845703, \t Total Dis Loss : 4.682997314375825e-06\n",
      "Steps : 354600, \t Total Gen Loss : 37.09116744995117, \t Total Dis Loss : 1.1439582294769934e-06\n",
      "Steps : 354700, \t Total Gen Loss : 34.64375305175781, \t Total Dis Loss : 1.0644595931807999e-05\n",
      "Steps : 354800, \t Total Gen Loss : 34.25602340698242, \t Total Dis Loss : 3.0316346055769827e-06\n",
      "Steps : 354900, \t Total Gen Loss : 35.54970169067383, \t Total Dis Loss : 2.2645839692359004e-07\n",
      "Steps : 355000, \t Total Gen Loss : 33.86322784423828, \t Total Dis Loss : 1.6642607079120353e-05\n",
      "Steps : 355100, \t Total Gen Loss : 30.110435485839844, \t Total Dis Loss : 5.476198566611856e-06\n",
      "Steps : 355200, \t Total Gen Loss : 32.529502868652344, \t Total Dis Loss : 6.02356612944277e-06\n",
      "Steps : 355300, \t Total Gen Loss : 35.041656494140625, \t Total Dis Loss : 2.8676945476036053e-06\n",
      "Steps : 355400, \t Total Gen Loss : 33.84619140625, \t Total Dis Loss : 1.0240533811156638e-05\n",
      "Steps : 355500, \t Total Gen Loss : 35.66009521484375, \t Total Dis Loss : 1.104932016460225e-05\n",
      "Steps : 355600, \t Total Gen Loss : 34.70893859863281, \t Total Dis Loss : 4.543632530840114e-05\n",
      "Steps : 355700, \t Total Gen Loss : 35.812530517578125, \t Total Dis Loss : 2.3395952666760422e-05\n",
      "Steps : 355800, \t Total Gen Loss : 38.10775375366211, \t Total Dis Loss : 0.0001056069741025567\n",
      "Steps : 355900, \t Total Gen Loss : 33.778831481933594, \t Total Dis Loss : 1.2098195156795555e-06\n",
      "Steps : 356000, \t Total Gen Loss : 34.07457733154297, \t Total Dis Loss : 9.097789757106511e-07\n",
      "Steps : 356100, \t Total Gen Loss : 32.60115051269531, \t Total Dis Loss : 4.0228637772088405e-06\n",
      "Steps : 356200, \t Total Gen Loss : 38.134796142578125, \t Total Dis Loss : 1.7676817151368596e-05\n",
      "Steps : 356300, \t Total Gen Loss : 32.87373352050781, \t Total Dis Loss : 2.9021675800322555e-06\n",
      "Steps : 356400, \t Total Gen Loss : 35.91161346435547, \t Total Dis Loss : 5.623302058666013e-06\n",
      "Steps : 356500, \t Total Gen Loss : 34.06098175048828, \t Total Dis Loss : 8.886403520591557e-06\n",
      "Steps : 356600, \t Total Gen Loss : 34.18698501586914, \t Total Dis Loss : 4.313697445468279e-06\n",
      "Steps : 356700, \t Total Gen Loss : 33.02850341796875, \t Total Dis Loss : 1.0398571248515509e-05\n",
      "Steps : 356800, \t Total Gen Loss : 36.49241638183594, \t Total Dis Loss : 3.413696322240867e-05\n",
      "Steps : 356900, \t Total Gen Loss : 32.6715087890625, \t Total Dis Loss : 3.245677362428978e-05\n",
      "Steps : 357000, \t Total Gen Loss : 38.265907287597656, \t Total Dis Loss : 4.871424721386575e-07\n",
      "Steps : 357100, \t Total Gen Loss : 35.66029357910156, \t Total Dis Loss : 1.3544652119890088e-06\n",
      "Steps : 357200, \t Total Gen Loss : 36.45998001098633, \t Total Dis Loss : 6.241841674636817e-06\n",
      "Steps : 357300, \t Total Gen Loss : 29.051218032836914, \t Total Dis Loss : 0.00011348677799105644\n",
      "Steps : 357400, \t Total Gen Loss : 31.84564781188965, \t Total Dis Loss : 0.00012125001376261935\n",
      "Steps : 357500, \t Total Gen Loss : 32.47789001464844, \t Total Dis Loss : 6.643726464972133e-06\n",
      "Steps : 357600, \t Total Gen Loss : 34.057071685791016, \t Total Dis Loss : 2.5870054741972126e-05\n",
      "Steps : 357700, \t Total Gen Loss : 30.231121063232422, \t Total Dis Loss : 9.749989112606272e-05\n",
      "Steps : 357800, \t Total Gen Loss : 30.81114959716797, \t Total Dis Loss : 4.907340189674869e-05\n",
      "Steps : 357900, \t Total Gen Loss : 31.288482666015625, \t Total Dis Loss : 3.3853048080345616e-05\n",
      "Steps : 358000, \t Total Gen Loss : 34.162635803222656, \t Total Dis Loss : 3.542582271620631e-05\n",
      "Steps : 358100, \t Total Gen Loss : 29.267690658569336, \t Total Dis Loss : 1.9477078240015544e-05\n",
      "Steps : 358200, \t Total Gen Loss : 29.645559310913086, \t Total Dis Loss : 1.0875189218495507e-05\n",
      "Steps : 358300, \t Total Gen Loss : 31.604297637939453, \t Total Dis Loss : 1.1109777005913202e-05\n",
      "Steps : 358400, \t Total Gen Loss : 29.301990509033203, \t Total Dis Loss : 1.9536893887561746e-05\n",
      "Steps : 358500, \t Total Gen Loss : 35.55908966064453, \t Total Dis Loss : 1.0098963230120717e-06\n",
      "Steps : 358600, \t Total Gen Loss : 33.898887634277344, \t Total Dis Loss : 2.8037525225954596e-06\n",
      "Steps : 358700, \t Total Gen Loss : 34.61792755126953, \t Total Dis Loss : 1.2896776752313599e-05\n",
      "Steps : 358800, \t Total Gen Loss : 34.34579086303711, \t Total Dis Loss : 1.346548287983751e-05\n",
      "Steps : 358900, \t Total Gen Loss : 39.111228942871094, \t Total Dis Loss : 4.49585604656022e-06\n",
      "Steps : 359000, \t Total Gen Loss : 31.809249877929688, \t Total Dis Loss : 1.592458829691168e-06\n",
      "Steps : 359100, \t Total Gen Loss : 30.22222900390625, \t Total Dis Loss : 2.885106368921697e-05\n",
      "Steps : 359200, \t Total Gen Loss : 32.695350646972656, \t Total Dis Loss : 5.567791959038004e-05\n",
      "Steps : 359300, \t Total Gen Loss : 32.98896789550781, \t Total Dis Loss : 5.549284833250567e-05\n",
      "Steps : 359400, \t Total Gen Loss : 28.19066047668457, \t Total Dis Loss : 9.653573943069205e-05\n",
      "Steps : 359500, \t Total Gen Loss : 33.90447998046875, \t Total Dis Loss : 4.415155103743018e-07\n",
      "Steps : 359600, \t Total Gen Loss : 34.11952209472656, \t Total Dis Loss : 5.7789511629380286e-06\n",
      "Steps : 359700, \t Total Gen Loss : 31.083616256713867, \t Total Dis Loss : 2.5671224648249336e-06\n",
      "Steps : 359800, \t Total Gen Loss : 28.345277786254883, \t Total Dis Loss : 0.00022872688714414835\n",
      "Steps : 359900, \t Total Gen Loss : 32.28779602050781, \t Total Dis Loss : 3.6521647416520864e-05\n",
      "Steps : 360000, \t Total Gen Loss : 31.185733795166016, \t Total Dis Loss : 2.330573624931276e-05\n",
      "Time for epoch 64 is 68.95848488807678 sec\n",
      "Steps : 360100, \t Total Gen Loss : 31.108386993408203, \t Total Dis Loss : 1.937581146194134e-05\n",
      "Steps : 360200, \t Total Gen Loss : 35.373268127441406, \t Total Dis Loss : 1.2613136277650483e-06\n",
      "Steps : 360300, \t Total Gen Loss : 33.357017517089844, \t Total Dis Loss : 6.394782758434303e-06\n",
      "Steps : 360400, \t Total Gen Loss : 30.68210220336914, \t Total Dis Loss : 3.316621950943954e-05\n",
      "Steps : 360500, \t Total Gen Loss : 34.618247985839844, \t Total Dis Loss : 9.0975208877353e-06\n",
      "Steps : 360600, \t Total Gen Loss : 35.12364959716797, \t Total Dis Loss : 1.61998696057708e-06\n",
      "Steps : 360700, \t Total Gen Loss : 38.46809005737305, \t Total Dis Loss : 8.507869893037423e-07\n",
      "Steps : 360800, \t Total Gen Loss : 33.86647033691406, \t Total Dis Loss : 1.8906250716099748e-06\n",
      "Steps : 360900, \t Total Gen Loss : 36.52497863769531, \t Total Dis Loss : 1.3502942692866782e-06\n",
      "Steps : 361000, \t Total Gen Loss : 33.154170989990234, \t Total Dis Loss : 5.358326689020032e-06\n",
      "Steps : 361100, \t Total Gen Loss : 35.24122619628906, \t Total Dis Loss : 6.602650728382287e-07\n",
      "Steps : 361200, \t Total Gen Loss : 37.062744140625, \t Total Dis Loss : 1.955101424755412e-06\n",
      "Steps : 361300, \t Total Gen Loss : 35.23262023925781, \t Total Dis Loss : 2.3211123334476724e-06\n",
      "Steps : 361400, \t Total Gen Loss : 35.69339370727539, \t Total Dis Loss : 4.999349130230257e-07\n",
      "Steps : 361500, \t Total Gen Loss : 35.205142974853516, \t Total Dis Loss : 2.45167211687658e-06\n",
      "Steps : 361600, \t Total Gen Loss : 34.40180969238281, \t Total Dis Loss : 2.6566945962258615e-05\n",
      "Steps : 361700, \t Total Gen Loss : 35.02709197998047, \t Total Dis Loss : 3.5017648770008236e-05\n",
      "Steps : 361800, \t Total Gen Loss : 35.99644470214844, \t Total Dis Loss : 9.326773238171882e-07\n",
      "Steps : 361900, \t Total Gen Loss : 32.35980987548828, \t Total Dis Loss : 3.3681540116958786e-06\n",
      "Steps : 362000, \t Total Gen Loss : 39.560203552246094, \t Total Dis Loss : 0.0037429246585816145\n",
      "Steps : 362100, \t Total Gen Loss : 44.61907196044922, \t Total Dis Loss : 5.223100743023679e-06\n",
      "Steps : 362200, \t Total Gen Loss : 38.85419464111328, \t Total Dis Loss : 1.730895178297942e-06\n",
      "Steps : 362300, \t Total Gen Loss : 40.96152114868164, \t Total Dis Loss : 2.7554119697015267e-06\n",
      "Steps : 362400, \t Total Gen Loss : 36.98521423339844, \t Total Dis Loss : 1.965014917004737e-06\n",
      "Steps : 362500, \t Total Gen Loss : 38.15324401855469, \t Total Dis Loss : 6.415522420866182e-06\n",
      "Steps : 362600, \t Total Gen Loss : 37.99563217163086, \t Total Dis Loss : 6.225042398000369e-06\n",
      "Steps : 362700, \t Total Gen Loss : 36.16168975830078, \t Total Dis Loss : 9.923269317368977e-06\n",
      "Steps : 362800, \t Total Gen Loss : 38.306095123291016, \t Total Dis Loss : 4.979921868653037e-06\n",
      "Steps : 362900, \t Total Gen Loss : 36.99858093261719, \t Total Dis Loss : 1.171476355921186e-06\n",
      "Steps : 363000, \t Total Gen Loss : 34.26968765258789, \t Total Dis Loss : 0.0007378552109003067\n",
      "Steps : 363100, \t Total Gen Loss : 31.23087501525879, \t Total Dis Loss : 4.750787411467172e-06\n",
      "Steps : 363200, \t Total Gen Loss : 38.03950119018555, \t Total Dis Loss : 1.692761770755169e-07\n",
      "Steps : 363300, \t Total Gen Loss : 32.492820739746094, \t Total Dis Loss : 9.86303530225996e-06\n",
      "Steps : 363400, \t Total Gen Loss : 32.03171920776367, \t Total Dis Loss : 1.446975375074544e-06\n",
      "Steps : 363500, \t Total Gen Loss : 32.1112174987793, \t Total Dis Loss : 8.579166205890942e-06\n",
      "Steps : 363600, \t Total Gen Loss : 33.23611068725586, \t Total Dis Loss : 1.2881876045867102e-06\n",
      "Steps : 363700, \t Total Gen Loss : 32.640869140625, \t Total Dis Loss : 1.9721617263712687e-06\n",
      "Steps : 363800, \t Total Gen Loss : 32.46883773803711, \t Total Dis Loss : 8.379634891753085e-06\n",
      "Steps : 363900, \t Total Gen Loss : 34.650238037109375, \t Total Dis Loss : 5.343608791008592e-06\n",
      "Steps : 364000, \t Total Gen Loss : 32.5773811340332, \t Total Dis Loss : 6.839950401626993e-06\n",
      "Steps : 364100, \t Total Gen Loss : 33.90390396118164, \t Total Dis Loss : 3.689441712140251e-07\n",
      "Steps : 364200, \t Total Gen Loss : 33.298126220703125, \t Total Dis Loss : 1.2510618034866638e-06\n",
      "Steps : 364300, \t Total Gen Loss : 30.374591827392578, \t Total Dis Loss : 8.30288354336517e-06\n",
      "Steps : 364400, \t Total Gen Loss : 33.343360900878906, \t Total Dis Loss : 1.3734912727159099e-06\n",
      "Steps : 364500, \t Total Gen Loss : 33.055179595947266, \t Total Dis Loss : 6.52243397780694e-05\n",
      "Steps : 364600, \t Total Gen Loss : 30.11709976196289, \t Total Dis Loss : 8.010334568098187e-05\n",
      "Steps : 364700, \t Total Gen Loss : 31.357868194580078, \t Total Dis Loss : 7.752774763503112e-06\n",
      "Steps : 364800, \t Total Gen Loss : 35.33000946044922, \t Total Dis Loss : 5.522726951312507e-07\n",
      "Steps : 364900, \t Total Gen Loss : 33.946861267089844, \t Total Dis Loss : 2.0997606497985544e-06\n",
      "Steps : 365000, \t Total Gen Loss : 35.18634796142578, \t Total Dis Loss : 2.7290407160762697e-06\n",
      "Steps : 365100, \t Total Gen Loss : 33.52131652832031, \t Total Dis Loss : 2.6350028292654315e-06\n",
      "Steps : 365200, \t Total Gen Loss : 30.751739501953125, \t Total Dis Loss : 1.2525881174951792e-05\n",
      "Steps : 365300, \t Total Gen Loss : 33.03776550292969, \t Total Dis Loss : 1.8173783473685035e-06\n",
      "Steps : 365400, \t Total Gen Loss : 29.988521575927734, \t Total Dis Loss : 1.0675955309125129e-05\n",
      "Steps : 365500, \t Total Gen Loss : 31.380369186401367, \t Total Dis Loss : 9.42641963774804e-06\n",
      "Steps : 365600, \t Total Gen Loss : 33.07855987548828, \t Total Dis Loss : 4.9074465096055064e-06\n",
      "Time for epoch 65 is 69.42718386650085 sec\n",
      "Steps : 365700, \t Total Gen Loss : 31.317546844482422, \t Total Dis Loss : 1.7624732208787464e-05\n",
      "Steps : 365800, \t Total Gen Loss : 29.73093032836914, \t Total Dis Loss : 0.00012799582327716053\n",
      "Steps : 365900, \t Total Gen Loss : 34.55241394042969, \t Total Dis Loss : 1.0754198228823952e-05\n",
      "Steps : 366000, \t Total Gen Loss : 33.16098403930664, \t Total Dis Loss : 1.3911177347836201e-06\n",
      "Steps : 366100, \t Total Gen Loss : 30.006505966186523, \t Total Dis Loss : 1.1180813999089878e-05\n",
      "Steps : 366200, \t Total Gen Loss : 34.34039306640625, \t Total Dis Loss : 6.538642765008262e-07\n",
      "Steps : 366300, \t Total Gen Loss : 32.99351501464844, \t Total Dis Loss : 4.3669814431268605e-07\n",
      "Steps : 366400, \t Total Gen Loss : 33.372337341308594, \t Total Dis Loss : 3.6145884223515168e-06\n",
      "Steps : 366500, \t Total Gen Loss : 35.6156005859375, \t Total Dis Loss : 4.183009423286421e-07\n",
      "Steps : 366600, \t Total Gen Loss : 34.59656524658203, \t Total Dis Loss : 3.493712574709207e-05\n",
      "Steps : 366700, \t Total Gen Loss : 32.94385528564453, \t Total Dis Loss : 2.217540895799175e-06\n",
      "Steps : 366800, \t Total Gen Loss : 34.15406799316406, \t Total Dis Loss : 7.188491508713923e-07\n",
      "Steps : 366900, \t Total Gen Loss : 32.665245056152344, \t Total Dis Loss : 1.9869086827384308e-05\n",
      "Steps : 367000, \t Total Gen Loss : 33.05340576171875, \t Total Dis Loss : 2.9785137485305313e-06\n",
      "Steps : 367100, \t Total Gen Loss : 35.164588928222656, \t Total Dis Loss : 2.323220940070314e-07\n",
      "Steps : 367200, \t Total Gen Loss : 32.64244842529297, \t Total Dis Loss : 1.5899327081569936e-06\n",
      "Steps : 367300, \t Total Gen Loss : 31.734495162963867, \t Total Dis Loss : 2.0627125195460394e-05\n",
      "Steps : 367400, \t Total Gen Loss : 31.381372451782227, \t Total Dis Loss : 5.536940079764463e-06\n",
      "Steps : 367500, \t Total Gen Loss : 30.532562255859375, \t Total Dis Loss : 6.905949589963711e-07\n",
      "Steps : 367600, \t Total Gen Loss : 29.219497680664062, \t Total Dis Loss : 3.230617949157022e-06\n",
      "Steps : 367700, \t Total Gen Loss : 29.110031127929688, \t Total Dis Loss : 3.911953899660148e-05\n",
      "Steps : 367800, \t Total Gen Loss : 35.192787170410156, \t Total Dis Loss : 7.49209266359685e-06\n",
      "Steps : 367900, \t Total Gen Loss : 28.25946617126465, \t Total Dis Loss : 3.652872328530066e-05\n",
      "Steps : 368000, \t Total Gen Loss : 32.14752960205078, \t Total Dis Loss : 1.2114640412619337e-05\n",
      "Steps : 368100, \t Total Gen Loss : 33.81683349609375, \t Total Dis Loss : 2.3366014829662163e-06\n",
      "Steps : 368200, \t Total Gen Loss : 31.404560089111328, \t Total Dis Loss : 1.18004663818283e-06\n",
      "Steps : 368300, \t Total Gen Loss : 32.57605743408203, \t Total Dis Loss : 1.4347955357152387e-06\n",
      "Steps : 368400, \t Total Gen Loss : 25.897186279296875, \t Total Dis Loss : 0.001809066510759294\n",
      "Steps : 368500, \t Total Gen Loss : 33.56790542602539, \t Total Dis Loss : 1.6484842490172014e-05\n",
      "Steps : 368600, \t Total Gen Loss : 38.6120719909668, \t Total Dis Loss : 1.3624275197798852e-05\n",
      "Steps : 368700, \t Total Gen Loss : 31.870407104492188, \t Total Dis Loss : 4.0527316741645336e-05\n",
      "Steps : 368800, \t Total Gen Loss : 35.858665466308594, \t Total Dis Loss : 6.318949090200476e-06\n",
      "Steps : 368900, \t Total Gen Loss : 35.43074035644531, \t Total Dis Loss : 0.00016913545550778508\n",
      "Steps : 369000, \t Total Gen Loss : 31.962329864501953, \t Total Dis Loss : 2.9059961889288388e-05\n",
      "Steps : 369100, \t Total Gen Loss : 32.42367172241211, \t Total Dis Loss : 6.047695933375508e-06\n",
      "Steps : 369200, \t Total Gen Loss : 31.0478515625, \t Total Dis Loss : 4.319507297623204e-06\n",
      "Steps : 369300, \t Total Gen Loss : 29.496646881103516, \t Total Dis Loss : 0.00010720423597376794\n",
      "Steps : 369400, \t Total Gen Loss : 36.51634979248047, \t Total Dis Loss : 2.386866754022776e-06\n",
      "Steps : 369500, \t Total Gen Loss : 36.29481506347656, \t Total Dis Loss : 1.2115137906221207e-06\n",
      "Steps : 369600, \t Total Gen Loss : 34.89826202392578, \t Total Dis Loss : 1.6136004887812305e-06\n",
      "Steps : 369700, \t Total Gen Loss : 34.01774978637695, \t Total Dis Loss : 2.600734296720475e-05\n",
      "Steps : 369800, \t Total Gen Loss : 38.8450813293457, \t Total Dis Loss : 1.141271695814794e-06\n",
      "Steps : 369900, \t Total Gen Loss : 40.56340026855469, \t Total Dis Loss : 5.037569280830212e-06\n",
      "Steps : 370000, \t Total Gen Loss : 37.28419494628906, \t Total Dis Loss : 2.183723211146571e-07\n",
      "Steps : 370100, \t Total Gen Loss : 38.810081481933594, \t Total Dis Loss : 1.2128779189879424e-06\n",
      "Steps : 370200, \t Total Gen Loss : 33.223514556884766, \t Total Dis Loss : 3.234774794691475e-06\n",
      "Steps : 370300, \t Total Gen Loss : 33.94441223144531, \t Total Dis Loss : 7.217491202027304e-06\n",
      "Steps : 370400, \t Total Gen Loss : 34.10273742675781, \t Total Dis Loss : 6.9368529693747405e-06\n",
      "Steps : 370500, \t Total Gen Loss : 37.72639465332031, \t Total Dis Loss : 8.464981510769576e-05\n",
      "Steps : 370600, \t Total Gen Loss : 35.66317367553711, \t Total Dis Loss : 2.6126051579922205e-06\n",
      "Steps : 370700, \t Total Gen Loss : 35.0564079284668, \t Total Dis Loss : 1.864859427769261e-06\n",
      "Steps : 370800, \t Total Gen Loss : 36.39104080200195, \t Total Dis Loss : 1.1296598358967458e-06\n",
      "Steps : 370900, \t Total Gen Loss : 37.04612350463867, \t Total Dis Loss : 3.078630243180669e-06\n",
      "Steps : 371000, \t Total Gen Loss : 36.851585388183594, \t Total Dis Loss : 7.596112936880672e-06\n",
      "Steps : 371100, \t Total Gen Loss : 32.39454650878906, \t Total Dis Loss : 6.00871999267838e-06\n",
      "Steps : 371200, \t Total Gen Loss : 33.57674026489258, \t Total Dis Loss : 1.2656645594688598e-05\n",
      "Time for epoch 66 is 69.03149724006653 sec\n",
      "Steps : 371300, \t Total Gen Loss : 34.039794921875, \t Total Dis Loss : 1.940793026733445e-06\n",
      "Steps : 371400, \t Total Gen Loss : 34.680763244628906, \t Total Dis Loss : 2.4007667889236473e-06\n",
      "Steps : 371500, \t Total Gen Loss : 33.45263671875, \t Total Dis Loss : 2.534641680540517e-06\n",
      "Steps : 371600, \t Total Gen Loss : 35.30738830566406, \t Total Dis Loss : 2.484722699591657e-06\n",
      "Steps : 371700, \t Total Gen Loss : 36.16484832763672, \t Total Dis Loss : 3.9927554098539986e-06\n",
      "Steps : 371800, \t Total Gen Loss : 38.86925506591797, \t Total Dis Loss : 3.6487565466813976e-06\n",
      "Steps : 371900, \t Total Gen Loss : 36.54494094848633, \t Total Dis Loss : 2.8560471037053503e-06\n",
      "Steps : 372000, \t Total Gen Loss : 34.90989685058594, \t Total Dis Loss : 4.5577183982459246e-07\n",
      "Steps : 372100, \t Total Gen Loss : 33.79664611816406, \t Total Dis Loss : 2.3908610273792874e-06\n",
      "Steps : 372200, \t Total Gen Loss : 35.459991455078125, \t Total Dis Loss : 1.5845090501898085e-06\n",
      "Steps : 372300, \t Total Gen Loss : 34.13091278076172, \t Total Dis Loss : 3.7836334740859456e-06\n",
      "Steps : 372400, \t Total Gen Loss : 35.240604400634766, \t Total Dis Loss : 2.7219186904403614e-06\n",
      "Steps : 372500, \t Total Gen Loss : 37.4907112121582, \t Total Dis Loss : 2.432816700093099e-06\n",
      "Steps : 372600, \t Total Gen Loss : 36.73249816894531, \t Total Dis Loss : 1.5443836218764773e-06\n",
      "Steps : 372700, \t Total Gen Loss : 37.24784469604492, \t Total Dis Loss : 5.990655154164415e-07\n",
      "Steps : 372800, \t Total Gen Loss : 36.490943908691406, \t Total Dis Loss : 1.27866258026188e-06\n",
      "Steps : 372900, \t Total Gen Loss : 38.18360137939453, \t Total Dis Loss : 1.3911612768424675e-06\n",
      "Steps : 373000, \t Total Gen Loss : 40.222190856933594, \t Total Dis Loss : 2.0474976736295503e-07\n",
      "Steps : 373100, \t Total Gen Loss : 39.16514205932617, \t Total Dis Loss : 3.2682046935406106e-07\n",
      "Steps : 373200, \t Total Gen Loss : 39.72574234008789, \t Total Dis Loss : 3.3146163787023397e-06\n",
      "Steps : 373300, \t Total Gen Loss : 39.170555114746094, \t Total Dis Loss : 7.695854264966329e-07\n",
      "Steps : 373400, \t Total Gen Loss : 36.755271911621094, \t Total Dis Loss : 1.221972297571483e-06\n",
      "Steps : 373500, \t Total Gen Loss : 37.69502258300781, \t Total Dis Loss : 1.3801529803458834e-06\n",
      "Steps : 373600, \t Total Gen Loss : 35.7906494140625, \t Total Dis Loss : 1.8338357676839223e-06\n",
      "Steps : 373700, \t Total Gen Loss : 34.84833526611328, \t Total Dis Loss : 6.338761977531249e-07\n",
      "Steps : 373800, \t Total Gen Loss : 39.214759826660156, \t Total Dis Loss : 5.05518585214304e-07\n",
      "Steps : 373900, \t Total Gen Loss : 39.426334381103516, \t Total Dis Loss : 3.1042661703395424e-06\n",
      "Steps : 374000, \t Total Gen Loss : 35.27373504638672, \t Total Dis Loss : 2.3720754143141676e-06\n",
      "Steps : 374100, \t Total Gen Loss : 35.2527961730957, \t Total Dis Loss : 5.714323378924746e-07\n",
      "Steps : 374200, \t Total Gen Loss : 36.789634704589844, \t Total Dis Loss : 1.6149176644830732e-06\n",
      "Steps : 374300, \t Total Gen Loss : 33.612144470214844, \t Total Dis Loss : 1.373377699565026e-06\n",
      "Steps : 374400, \t Total Gen Loss : 36.25886154174805, \t Total Dis Loss : 1.3018993740843143e-06\n",
      "Steps : 374500, \t Total Gen Loss : 37.22374725341797, \t Total Dis Loss : 2.5138916726064053e-07\n",
      "Steps : 374600, \t Total Gen Loss : 36.79419708251953, \t Total Dis Loss : 1.508145146544848e-06\n",
      "Steps : 374700, \t Total Gen Loss : 37.34705352783203, \t Total Dis Loss : 6.996006050030701e-07\n",
      "Steps : 374800, \t Total Gen Loss : 34.843505859375, \t Total Dis Loss : 3.252851001889212e-06\n",
      "Steps : 374900, \t Total Gen Loss : 37.701622009277344, \t Total Dis Loss : 8.120388883980922e-06\n",
      "Steps : 375000, \t Total Gen Loss : 39.05366897583008, \t Total Dis Loss : 1.910412720462773e-06\n",
      "Steps : 375100, \t Total Gen Loss : 37.363037109375, \t Total Dis Loss : 1.0412617257316015e-07\n",
      "Steps : 375200, \t Total Gen Loss : 30.571598052978516, \t Total Dis Loss : 5.358098496799357e-05\n",
      "Steps : 375300, \t Total Gen Loss : 32.61935806274414, \t Total Dis Loss : 4.615973011823371e-06\n",
      "Steps : 375400, \t Total Gen Loss : 33.87759017944336, \t Total Dis Loss : 1.2883461749879643e-05\n",
      "Steps : 375500, \t Total Gen Loss : 34.329742431640625, \t Total Dis Loss : 2.9627239200635813e-05\n",
      "Steps : 375600, \t Total Gen Loss : 31.789569854736328, \t Total Dis Loss : 5.322667129803449e-05\n",
      "Steps : 375700, \t Total Gen Loss : 35.52201843261719, \t Total Dis Loss : 8.465546130764778e-08\n",
      "Steps : 375800, \t Total Gen Loss : 35.47032165527344, \t Total Dis Loss : 1.9758324469876243e-06\n",
      "Steps : 375900, \t Total Gen Loss : 32.41434097290039, \t Total Dis Loss : 1.4397054656001274e-05\n",
      "Steps : 376000, \t Total Gen Loss : 30.774879455566406, \t Total Dis Loss : 7.712110527791083e-05\n",
      "Steps : 376100, \t Total Gen Loss : 34.506813049316406, \t Total Dis Loss : 5.087290446681436e-06\n",
      "Steps : 376200, \t Total Gen Loss : 29.496841430664062, \t Total Dis Loss : 4.4395554141374305e-05\n",
      "Steps : 376300, \t Total Gen Loss : 33.401695251464844, \t Total Dis Loss : 8.672222975292243e-06\n",
      "Steps : 376400, \t Total Gen Loss : 35.887168884277344, \t Total Dis Loss : 7.62673494136834e-07\n",
      "Steps : 376500, \t Total Gen Loss : 31.968852996826172, \t Total Dis Loss : 1.798474113456905e-05\n",
      "Steps : 376600, \t Total Gen Loss : 33.60451889038086, \t Total Dis Loss : 1.4977900718804449e-05\n",
      "Steps : 376700, \t Total Gen Loss : 31.476104736328125, \t Total Dis Loss : 1.1330360393912997e-05\n",
      "Steps : 376800, \t Total Gen Loss : 37.19276428222656, \t Total Dis Loss : 4.817794660993968e-07\n",
      "Time for epoch 67 is 68.97448372840881 sec\n",
      "Steps : 376900, \t Total Gen Loss : 31.069360733032227, \t Total Dis Loss : 3.1846841466176556e-06\n",
      "Steps : 377000, \t Total Gen Loss : 32.1708869934082, \t Total Dis Loss : 2.1547748474404216e-06\n",
      "Steps : 377100, \t Total Gen Loss : 38.183631896972656, \t Total Dis Loss : 2.1415569790406153e-05\n",
      "Steps : 377200, \t Total Gen Loss : 33.592742919921875, \t Total Dis Loss : 0.0002176638226956129\n",
      "Steps : 377300, \t Total Gen Loss : 35.138954162597656, \t Total Dis Loss : 3.444804633545573e-06\n",
      "Steps : 377400, \t Total Gen Loss : 38.480438232421875, \t Total Dis Loss : 8.712022463441826e-07\n",
      "Steps : 377500, \t Total Gen Loss : 35.693824768066406, \t Total Dis Loss : 6.225451943464577e-05\n",
      "Steps : 377600, \t Total Gen Loss : 39.36968231201172, \t Total Dis Loss : 1.6804820290872158e-07\n",
      "Steps : 377700, \t Total Gen Loss : 35.818443298339844, \t Total Dis Loss : 5.0846256272052415e-06\n",
      "Steps : 377800, \t Total Gen Loss : 35.07516098022461, \t Total Dis Loss : 9.691910963738337e-05\n",
      "Steps : 377900, \t Total Gen Loss : 33.836551666259766, \t Total Dis Loss : 1.3366164239414502e-05\n",
      "Steps : 378000, \t Total Gen Loss : 40.045562744140625, \t Total Dis Loss : 7.2772809289745055e-06\n",
      "Steps : 378100, \t Total Gen Loss : 33.16996383666992, \t Total Dis Loss : 4.1833282011793926e-05\n",
      "Steps : 378200, \t Total Gen Loss : 35.750091552734375, \t Total Dis Loss : 4.771775638801046e-05\n",
      "Steps : 378300, \t Total Gen Loss : 39.344200134277344, \t Total Dis Loss : 4.399129920784617e-06\n",
      "Steps : 378400, \t Total Gen Loss : 39.204803466796875, \t Total Dis Loss : 2.6109898954018718e-06\n",
      "Steps : 378500, \t Total Gen Loss : 36.56658935546875, \t Total Dis Loss : 2.3316683837038e-06\n",
      "Steps : 378600, \t Total Gen Loss : 39.12770462036133, \t Total Dis Loss : 2.2825734902198747e-07\n",
      "Steps : 378700, \t Total Gen Loss : 30.519710540771484, \t Total Dis Loss : 0.00014265981735661626\n",
      "Steps : 378800, \t Total Gen Loss : 29.588027954101562, \t Total Dis Loss : 1.8830372937372886e-05\n",
      "Steps : 378900, \t Total Gen Loss : 32.22772979736328, \t Total Dis Loss : 8.388874994125217e-05\n",
      "Steps : 379000, \t Total Gen Loss : 33.69861602783203, \t Total Dis Loss : 2.3255661290022545e-06\n",
      "Steps : 379100, \t Total Gen Loss : 30.773296356201172, \t Total Dis Loss : 3.340896364534274e-05\n",
      "Steps : 379200, \t Total Gen Loss : 35.16223907470703, \t Total Dis Loss : 3.816976459347643e-07\n",
      "Steps : 379300, \t Total Gen Loss : 33.405677795410156, \t Total Dis Loss : 1.5674153246436617e-06\n",
      "Steps : 379400, \t Total Gen Loss : 37.40187454223633, \t Total Dis Loss : 2.078629950119648e-05\n",
      "Steps : 379500, \t Total Gen Loss : 32.4219856262207, \t Total Dis Loss : 8.811653970042244e-06\n",
      "Steps : 379600, \t Total Gen Loss : 34.48894500732422, \t Total Dis Loss : 1.4704698969580932e-06\n",
      "Steps : 379700, \t Total Gen Loss : 34.0501708984375, \t Total Dis Loss : 1.4770291272725444e-05\n",
      "Steps : 379800, \t Total Gen Loss : 37.62562942504883, \t Total Dis Loss : 2.4379485807912715e-07\n",
      "Steps : 379900, \t Total Gen Loss : 35.654388427734375, \t Total Dis Loss : 4.642923613573657e-06\n",
      "Steps : 380000, \t Total Gen Loss : 35.547874450683594, \t Total Dis Loss : 3.7980212255206425e-06\n",
      "Steps : 380100, \t Total Gen Loss : 35.780155181884766, \t Total Dis Loss : 2.0418547137524e-06\n",
      "Steps : 380200, \t Total Gen Loss : 37.16034698486328, \t Total Dis Loss : 3.741754335351288e-05\n",
      "Steps : 380300, \t Total Gen Loss : 30.461904525756836, \t Total Dis Loss : 2.0811044123547617e-06\n",
      "Steps : 380400, \t Total Gen Loss : 31.822071075439453, \t Total Dis Loss : 2.5706424366944702e-06\n",
      "Steps : 380500, \t Total Gen Loss : 32.71579360961914, \t Total Dis Loss : 9.098268492380157e-05\n",
      "Steps : 380600, \t Total Gen Loss : 34.32799530029297, \t Total Dis Loss : 2.942282208096003e-06\n",
      "Steps : 380700, \t Total Gen Loss : 33.64533233642578, \t Total Dis Loss : 1.8811683730746154e-06\n",
      "Steps : 380800, \t Total Gen Loss : 36.2733154296875, \t Total Dis Loss : 4.0140088231055415e-07\n",
      "Steps : 380900, \t Total Gen Loss : 32.58295822143555, \t Total Dis Loss : 1.8615144199429778e-06\n",
      "Steps : 381000, \t Total Gen Loss : 32.29649353027344, \t Total Dis Loss : 5.083701125840889e-06\n",
      "Steps : 381100, \t Total Gen Loss : 36.46522521972656, \t Total Dis Loss : 1.7757303112375666e-06\n",
      "Steps : 381200, \t Total Gen Loss : 36.51237106323242, \t Total Dis Loss : 1.500342750659911e-05\n",
      "Steps : 381300, \t Total Gen Loss : 32.948509216308594, \t Total Dis Loss : 2.2483000066131353e-06\n",
      "Steps : 381400, \t Total Gen Loss : 34.80924987792969, \t Total Dis Loss : 2.169383321870555e-07\n",
      "Steps : 381500, \t Total Gen Loss : 30.892728805541992, \t Total Dis Loss : 3.2956520499283215e-06\n",
      "Steps : 381600, \t Total Gen Loss : 34.01582336425781, \t Total Dis Loss : 6.5845651988638565e-06\n",
      "Steps : 381700, \t Total Gen Loss : 32.58599853515625, \t Total Dis Loss : 3.4765976124617737e-06\n",
      "Steps : 381800, \t Total Gen Loss : 32.64558410644531, \t Total Dis Loss : 5.363143827707972e-06\n",
      "Steps : 381900, \t Total Gen Loss : 35.682228088378906, \t Total Dis Loss : 3.9993699374463176e-07\n",
      "Steps : 382000, \t Total Gen Loss : 31.34980583190918, \t Total Dis Loss : 6.0940578805457335e-06\n",
      "Steps : 382100, \t Total Gen Loss : 34.193458557128906, \t Total Dis Loss : 7.582378884762875e-07\n",
      "Steps : 382200, \t Total Gen Loss : 33.037353515625, \t Total Dis Loss : 5.40737255505519e-06\n",
      "Steps : 382300, \t Total Gen Loss : 36.227333068847656, \t Total Dis Loss : 1.6140287698362954e-05\n",
      "Steps : 382400, \t Total Gen Loss : 33.986595153808594, \t Total Dis Loss : 1.8071320937451674e-06\n",
      "Steps : 382500, \t Total Gen Loss : 34.09319305419922, \t Total Dis Loss : 5.533506168831082e-07\n",
      "Time for epoch 68 is 69.01586055755615 sec\n",
      "Steps : 382600, \t Total Gen Loss : 37.565040588378906, \t Total Dis Loss : 6.049085641279817e-06\n",
      "Steps : 382700, \t Total Gen Loss : 33.49512481689453, \t Total Dis Loss : 1.064977027454006e-06\n",
      "Steps : 382800, \t Total Gen Loss : 33.47643280029297, \t Total Dis Loss : 2.7095009045297047e-06\n",
      "Steps : 382900, \t Total Gen Loss : 34.00756072998047, \t Total Dis Loss : 0.0004064603999722749\n",
      "Steps : 383000, \t Total Gen Loss : 30.286605834960938, \t Total Dis Loss : 0.04685656353831291\n",
      "Steps : 383100, \t Total Gen Loss : 34.655540466308594, \t Total Dis Loss : 3.902067874150816e-06\n",
      "Steps : 383200, \t Total Gen Loss : 32.832542419433594, \t Total Dis Loss : 3.6819628803641535e-06\n",
      "Steps : 383300, \t Total Gen Loss : 35.421024322509766, \t Total Dis Loss : 3.2495295272383373e-06\n",
      "Steps : 383400, \t Total Gen Loss : 38.012786865234375, \t Total Dis Loss : 7.812929652573075e-06\n",
      "Steps : 383500, \t Total Gen Loss : 35.75473403930664, \t Total Dis Loss : 3.227901743230177e-06\n",
      "Steps : 383600, \t Total Gen Loss : 36.65264892578125, \t Total Dis Loss : 3.2118396120495163e-06\n",
      "Steps : 383700, \t Total Gen Loss : 34.049560546875, \t Total Dis Loss : 2.636152657942148e-06\n",
      "Steps : 383800, \t Total Gen Loss : 33.1215705871582, \t Total Dis Loss : 0.047569390386343\n",
      "Steps : 383900, \t Total Gen Loss : 31.143388748168945, \t Total Dis Loss : 2.7363590561435558e-05\n",
      "Steps : 384000, \t Total Gen Loss : 33.088775634765625, \t Total Dis Loss : 1.7785452655516565e-05\n",
      "Steps : 384100, \t Total Gen Loss : 31.304563522338867, \t Total Dis Loss : 1.0569485311862081e-05\n",
      "Steps : 384200, \t Total Gen Loss : 30.739200592041016, \t Total Dis Loss : 4.512477062235121e-06\n",
      "Steps : 384300, \t Total Gen Loss : 36.574546813964844, \t Total Dis Loss : 2.3684249754296616e-05\n",
      "Steps : 384400, \t Total Gen Loss : 29.48223114013672, \t Total Dis Loss : 3.879295400111005e-05\n",
      "Steps : 384500, \t Total Gen Loss : 34.57861328125, \t Total Dis Loss : 2.4850066893122857e-06\n",
      "Steps : 384600, \t Total Gen Loss : 41.71116638183594, \t Total Dis Loss : 0.00023592791694682091\n",
      "Steps : 384700, \t Total Gen Loss : 40.75779342651367, \t Total Dis Loss : 1.0976809790008701e-05\n",
      "Steps : 384800, \t Total Gen Loss : 38.234283447265625, \t Total Dis Loss : 2.214872438344173e-05\n",
      "Steps : 384900, \t Total Gen Loss : 31.828420639038086, \t Total Dis Loss : 2.1603564448469115e-07\n",
      "Steps : 385000, \t Total Gen Loss : 34.3858528137207, \t Total Dis Loss : 3.023528165613243e-07\n",
      "Steps : 385100, \t Total Gen Loss : 34.67290115356445, \t Total Dis Loss : 7.076896224589291e-08\n",
      "Steps : 385200, \t Total Gen Loss : 34.144683837890625, \t Total Dis Loss : 9.671845191405737e-08\n",
      "Steps : 385300, \t Total Gen Loss : 30.710613250732422, \t Total Dis Loss : 9.138718269241508e-06\n",
      "Steps : 385400, \t Total Gen Loss : 33.772186279296875, \t Total Dis Loss : 6.868671789561631e-06\n",
      "Steps : 385500, \t Total Gen Loss : 25.091581344604492, \t Total Dis Loss : 0.00017361706704832613\n",
      "Steps : 385600, \t Total Gen Loss : 30.369613647460938, \t Total Dis Loss : 3.054910621358431e-06\n",
      "Steps : 385700, \t Total Gen Loss : 34.48119354248047, \t Total Dis Loss : 1.9848816918965895e-06\n",
      "Steps : 385800, \t Total Gen Loss : 31.170780181884766, \t Total Dis Loss : 1.626725133974105e-05\n",
      "Steps : 385900, \t Total Gen Loss : 34.260616302490234, \t Total Dis Loss : 5.747806426370516e-06\n",
      "Steps : 386000, \t Total Gen Loss : 34.56013488769531, \t Total Dis Loss : 3.2650516459398204e-06\n",
      "Steps : 386100, \t Total Gen Loss : 34.921546936035156, \t Total Dis Loss : 2.4714406663406407e-06\n",
      "Steps : 386200, \t Total Gen Loss : 35.537620544433594, \t Total Dis Loss : 1.427886644478349e-07\n",
      "Steps : 386300, \t Total Gen Loss : 33.40879440307617, \t Total Dis Loss : 3.3427158996346407e-06\n",
      "Steps : 386400, \t Total Gen Loss : 35.82697296142578, \t Total Dis Loss : 3.6846532225354167e-07\n",
      "Steps : 386500, \t Total Gen Loss : 34.36632537841797, \t Total Dis Loss : 1.9159236330779095e-07\n",
      "Steps : 386600, \t Total Gen Loss : 34.97200012207031, \t Total Dis Loss : 9.592241667633061e-07\n",
      "Steps : 386700, \t Total Gen Loss : 33.56953048706055, \t Total Dis Loss : 2.9520305133701186e-07\n",
      "Steps : 386800, \t Total Gen Loss : 34.67505645751953, \t Total Dis Loss : 8.36014748983871e-07\n",
      "Steps : 386900, \t Total Gen Loss : 34.69169998168945, \t Total Dis Loss : 6.074845373404969e-07\n",
      "Steps : 387000, \t Total Gen Loss : 29.406919479370117, \t Total Dis Loss : 0.00026063513359986246\n",
      "Steps : 387100, \t Total Gen Loss : 33.11885070800781, \t Total Dis Loss : 3.844265393126989e-06\n",
      "Steps : 387200, \t Total Gen Loss : 34.55597686767578, \t Total Dis Loss : 2.7401777060731547e-06\n",
      "Steps : 387300, \t Total Gen Loss : 35.80416488647461, \t Total Dis Loss : 4.736937739835412e-07\n",
      "Steps : 387400, \t Total Gen Loss : 32.81691360473633, \t Total Dis Loss : 2.986130596127623e-07\n",
      "Steps : 387500, \t Total Gen Loss : 37.14781951904297, \t Total Dis Loss : 5.505101398739498e-06\n",
      "Steps : 387600, \t Total Gen Loss : 34.24019241333008, \t Total Dis Loss : 1.5720080455139396e-06\n",
      "Steps : 387700, \t Total Gen Loss : 37.993003845214844, \t Total Dis Loss : 2.9568940362878493e-07\n",
      "Steps : 387800, \t Total Gen Loss : 37.685874938964844, \t Total Dis Loss : 4.185035777481971e-06\n",
      "Steps : 387900, \t Total Gen Loss : 33.363338470458984, \t Total Dis Loss : 1.5037215916891e-06\n",
      "Steps : 388000, \t Total Gen Loss : 35.90241622924805, \t Total Dis Loss : 0.00019786118355114013\n",
      "Steps : 388100, \t Total Gen Loss : 35.324893951416016, \t Total Dis Loss : 3.5307432426634477e-06\n",
      "Time for epoch 69 is 68.98815369606018 sec\n",
      "Steps : 388200, \t Total Gen Loss : 37.95022964477539, \t Total Dis Loss : 2.2675274635730602e-07\n",
      "Steps : 388300, \t Total Gen Loss : 32.433658599853516, \t Total Dis Loss : 5.70435304325656e-07\n",
      "Steps : 388400, \t Total Gen Loss : 32.697731018066406, \t Total Dis Loss : 5.167498784430791e-07\n",
      "Steps : 388500, \t Total Gen Loss : 37.43366241455078, \t Total Dis Loss : 6.381846105796285e-06\n",
      "Steps : 388600, \t Total Gen Loss : 34.716976165771484, \t Total Dis Loss : 6.187656822476129e-07\n",
      "Steps : 388700, \t Total Gen Loss : 35.344329833984375, \t Total Dis Loss : 8.661645551910624e-06\n",
      "Steps : 388800, \t Total Gen Loss : 33.579978942871094, \t Total Dis Loss : 4.224050940138113e-07\n",
      "Steps : 388900, \t Total Gen Loss : 31.929195404052734, \t Total Dis Loss : 5.9298317864886485e-06\n",
      "Steps : 389000, \t Total Gen Loss : 35.31792449951172, \t Total Dis Loss : 6.831196515122429e-07\n",
      "Steps : 389100, \t Total Gen Loss : 36.038124084472656, \t Total Dis Loss : 5.289844011713285e-06\n",
      "Steps : 389200, \t Total Gen Loss : 35.31393814086914, \t Total Dis Loss : 7.778572808092576e-08\n",
      "Steps : 389300, \t Total Gen Loss : 35.80501937866211, \t Total Dis Loss : 9.248540067119393e-08\n",
      "Steps : 389400, \t Total Gen Loss : 38.51371383666992, \t Total Dis Loss : 1.1474287475721212e-06\n",
      "Steps : 389500, \t Total Gen Loss : 33.91014099121094, \t Total Dis Loss : 4.5732090825367777e-07\n",
      "Steps : 389600, \t Total Gen Loss : 36.68697738647461, \t Total Dis Loss : 3.135826887046278e-07\n",
      "Steps : 389700, \t Total Gen Loss : 33.765380859375, \t Total Dis Loss : 3.684176590468269e-07\n",
      "Steps : 389800, \t Total Gen Loss : 38.11027908325195, \t Total Dis Loss : 1.3458297871693503e-06\n",
      "Steps : 389900, \t Total Gen Loss : 31.76363182067871, \t Total Dis Loss : 0.00503844628110528\n",
      "Steps : 390000, \t Total Gen Loss : 32.56694412231445, \t Total Dis Loss : 0.00032364329672418535\n",
      "Steps : 390100, \t Total Gen Loss : 31.180152893066406, \t Total Dis Loss : 0.0006759562529623508\n",
      "Steps : 390200, \t Total Gen Loss : 29.230796813964844, \t Total Dis Loss : 2.5145614017674234e-06\n",
      "Steps : 390300, \t Total Gen Loss : 34.147621154785156, \t Total Dis Loss : 2.192032525272225e-06\n",
      "Steps : 390400, \t Total Gen Loss : 36.226741790771484, \t Total Dis Loss : 4.8729402806202415e-06\n",
      "Steps : 390500, \t Total Gen Loss : 32.7282600402832, \t Total Dis Loss : 9.54989332058176e-07\n",
      "Steps : 390600, \t Total Gen Loss : 39.44216537475586, \t Total Dis Loss : 1.0644562280504033e-05\n",
      "Steps : 390700, \t Total Gen Loss : 35.70775604248047, \t Total Dis Loss : 8.740248063077161e-07\n",
      "Steps : 390800, \t Total Gen Loss : 36.68995666503906, \t Total Dis Loss : 1.0619052773108706e-05\n",
      "Steps : 390900, \t Total Gen Loss : 28.358983993530273, \t Total Dis Loss : 0.00010072465374832973\n",
      "Steps : 391000, \t Total Gen Loss : 30.59090805053711, \t Total Dis Loss : 0.00015919057477731258\n",
      "Steps : 391100, \t Total Gen Loss : 33.197166442871094, \t Total Dis Loss : 3.5777407902060077e-05\n",
      "Steps : 391200, \t Total Gen Loss : 35.550010681152344, \t Total Dis Loss : 3.8199843402253464e-05\n",
      "Steps : 391300, \t Total Gen Loss : 30.195568084716797, \t Total Dis Loss : 3.2845222449395806e-05\n",
      "Steps : 391400, \t Total Gen Loss : 31.254053115844727, \t Total Dis Loss : 1.547563988424372e-05\n",
      "Steps : 391500, \t Total Gen Loss : 32.06201171875, \t Total Dis Loss : 9.240772669727448e-06\n",
      "Steps : 391600, \t Total Gen Loss : 36.383140563964844, \t Total Dis Loss : 4.116112904739566e-06\n",
      "Steps : 391700, \t Total Gen Loss : 32.62101745605469, \t Total Dis Loss : 1.2883370800409466e-05\n",
      "Steps : 391800, \t Total Gen Loss : 33.56071853637695, \t Total Dis Loss : 1.017086106003262e-06\n",
      "Steps : 391900, \t Total Gen Loss : 34.61569595336914, \t Total Dis Loss : 1.0267465768265538e-05\n",
      "Steps : 392000, \t Total Gen Loss : 36.306549072265625, \t Total Dis Loss : 5.1037832236033864e-06\n",
      "Steps : 392100, \t Total Gen Loss : 34.09125900268555, \t Total Dis Loss : 1.6543868696317077e-05\n",
      "Steps : 392200, \t Total Gen Loss : 35.06944274902344, \t Total Dis Loss : 1.3206251423980575e-05\n",
      "Steps : 392300, \t Total Gen Loss : 33.70469284057617, \t Total Dis Loss : 3.2267234928440303e-06\n",
      "Steps : 392400, \t Total Gen Loss : 33.088356018066406, \t Total Dis Loss : 1.1403065400372725e-05\n",
      "Steps : 392500, \t Total Gen Loss : 32.62858200073242, \t Total Dis Loss : 5.42703537576017e-06\n",
      "Steps : 392600, \t Total Gen Loss : 33.06367111206055, \t Total Dis Loss : 5.935500666964799e-06\n",
      "Steps : 392700, \t Total Gen Loss : 36.41446304321289, \t Total Dis Loss : 3.23343579111679e-06\n",
      "Steps : 392800, \t Total Gen Loss : 33.87818145751953, \t Total Dis Loss : 2.088475184791605e-06\n",
      "Steps : 392900, \t Total Gen Loss : 33.3814697265625, \t Total Dis Loss : 1.028653969115112e-06\n",
      "Steps : 393000, \t Total Gen Loss : 36.14124298095703, \t Total Dis Loss : 5.4177080528461374e-06\n",
      "Steps : 393100, \t Total Gen Loss : 33.28899383544922, \t Total Dis Loss : 4.86515227748896e-06\n",
      "Steps : 393200, \t Total Gen Loss : 37.013118743896484, \t Total Dis Loss : 1.860800921349437e-06\n",
      "Steps : 393300, \t Total Gen Loss : 37.74089050292969, \t Total Dis Loss : 7.821736289770342e-07\n",
      "Steps : 393400, \t Total Gen Loss : 35.389312744140625, \t Total Dis Loss : 9.249387403542642e-06\n",
      "Steps : 393500, \t Total Gen Loss : 31.3472900390625, \t Total Dis Loss : 7.953518661452108e-07\n",
      "Steps : 393600, \t Total Gen Loss : 32.738399505615234, \t Total Dis Loss : 5.2828916523139924e-05\n",
      "Steps : 393700, \t Total Gen Loss : 29.52552032470703, \t Total Dis Loss : 6.76519334774639e-07\n",
      "Time for epoch 70 is 69.8840274810791 sec\n",
      "Steps : 393800, \t Total Gen Loss : 31.85122299194336, \t Total Dis Loss : 1.971818619495025e-06\n",
      "Steps : 393900, \t Total Gen Loss : 33.232948303222656, \t Total Dis Loss : 1.7192025552503765e-05\n",
      "Steps : 394000, \t Total Gen Loss : 35.193302154541016, \t Total Dis Loss : 1.650713556955452e-06\n",
      "Steps : 394100, \t Total Gen Loss : 33.071327209472656, \t Total Dis Loss : 6.54454481718858e-07\n",
      "Steps : 394200, \t Total Gen Loss : 35.913543701171875, \t Total Dis Loss : 2.075531938316999e-06\n",
      "Steps : 394300, \t Total Gen Loss : 33.977203369140625, \t Total Dis Loss : 5.92908122598601e-07\n",
      "Steps : 394400, \t Total Gen Loss : 33.338356018066406, \t Total Dis Loss : 5.397074346547015e-06\n",
      "Steps : 394500, \t Total Gen Loss : 29.951868057250977, \t Total Dis Loss : 1.8556455643192749e-06\n",
      "Steps : 394600, \t Total Gen Loss : 32.38123321533203, \t Total Dis Loss : 6.1034825193928555e-06\n",
      "Steps : 394700, \t Total Gen Loss : 31.613386154174805, \t Total Dis Loss : 2.5878457563521806e-06\n",
      "Steps : 394800, \t Total Gen Loss : 32.85386657714844, \t Total Dis Loss : 4.513430212682579e-06\n",
      "Steps : 394900, \t Total Gen Loss : 35.647491455078125, \t Total Dis Loss : 5.456139660964254e-06\n",
      "Steps : 395000, \t Total Gen Loss : 31.360736846923828, \t Total Dis Loss : 2.9537595764850266e-06\n",
      "Steps : 395100, \t Total Gen Loss : 32.03919982910156, \t Total Dis Loss : 1.0923487025138456e-05\n",
      "Steps : 395200, \t Total Gen Loss : 33.13587951660156, \t Total Dis Loss : 2.6269874524587067e-06\n",
      "Steps : 395300, \t Total Gen Loss : 35.095802307128906, \t Total Dis Loss : 8.203827519537299e-07\n",
      "Steps : 395400, \t Total Gen Loss : 32.617347717285156, \t Total Dis Loss : 2.2310887288767844e-06\n",
      "Steps : 395500, \t Total Gen Loss : 33.75703430175781, \t Total Dis Loss : 1.225213281941251e-06\n",
      "Steps : 395600, \t Total Gen Loss : 37.04363250732422, \t Total Dis Loss : 1.6107724150060676e-06\n",
      "Steps : 395700, \t Total Gen Loss : 35.77452087402344, \t Total Dis Loss : 1.23208428703947e-06\n",
      "Steps : 395800, \t Total Gen Loss : 34.32848358154297, \t Total Dis Loss : 6.036525519448332e-07\n",
      "Steps : 395900, \t Total Gen Loss : 34.793601989746094, \t Total Dis Loss : 8.311657438753173e-06\n",
      "Steps : 396000, \t Total Gen Loss : 33.78483581542969, \t Total Dis Loss : 5.0006660785584245e-06\n",
      "Steps : 396100, \t Total Gen Loss : 35.061641693115234, \t Total Dis Loss : 3.533472408889793e-06\n",
      "Steps : 396200, \t Total Gen Loss : 30.756519317626953, \t Total Dis Loss : 2.019546809606254e-05\n",
      "Steps : 396300, \t Total Gen Loss : 32.868980407714844, \t Total Dis Loss : 4.773143245984102e-06\n",
      "Steps : 396400, \t Total Gen Loss : 37.36228561401367, \t Total Dis Loss : 2.5848473796941107e-06\n",
      "Steps : 396500, \t Total Gen Loss : 31.578981399536133, \t Total Dis Loss : 3.9257706703210715e-06\n",
      "Steps : 396600, \t Total Gen Loss : 32.81328582763672, \t Total Dis Loss : 4.850989625992952e-06\n",
      "Steps : 396700, \t Total Gen Loss : 31.332874298095703, \t Total Dis Loss : 2.5436565920244902e-06\n",
      "Steps : 396800, \t Total Gen Loss : 34.776611328125, \t Total Dis Loss : 2.8136051355431846e-07\n",
      "Steps : 396900, \t Total Gen Loss : 34.6463623046875, \t Total Dis Loss : 6.399498602149833e-07\n",
      "Steps : 397000, \t Total Gen Loss : 32.109214782714844, \t Total Dis Loss : 5.724755283154082e-07\n",
      "Steps : 397100, \t Total Gen Loss : 32.87577819824219, \t Total Dis Loss : 6.840404012109502e-07\n",
      "Steps : 397200, \t Total Gen Loss : 30.89293670654297, \t Total Dis Loss : 3.470296974228404e-07\n",
      "Steps : 397300, \t Total Gen Loss : 35.442710876464844, \t Total Dis Loss : 1.6613586922176182e-05\n",
      "Steps : 397400, \t Total Gen Loss : 32.23262023925781, \t Total Dis Loss : 7.048594852676615e-06\n",
      "Steps : 397500, \t Total Gen Loss : 33.148311614990234, \t Total Dis Loss : 8.345008382093511e-07\n",
      "Steps : 397600, \t Total Gen Loss : 35.31343460083008, \t Total Dis Loss : 1.0486339760973351e-06\n",
      "Steps : 397700, \t Total Gen Loss : 33.91148376464844, \t Total Dis Loss : 1.6138696992129553e-06\n",
      "Steps : 397800, \t Total Gen Loss : 33.50923156738281, \t Total Dis Loss : 5.226014309300808e-06\n",
      "Steps : 397900, \t Total Gen Loss : 30.834138870239258, \t Total Dis Loss : 8.552143845008686e-05\n",
      "Steps : 398000, \t Total Gen Loss : 33.346519470214844, \t Total Dis Loss : 1.0320045475964434e-05\n",
      "Steps : 398100, \t Total Gen Loss : 33.69245529174805, \t Total Dis Loss : 3.44125073752366e-05\n",
      "Steps : 398200, \t Total Gen Loss : 31.508121490478516, \t Total Dis Loss : 3.6412380723049864e-05\n",
      "Steps : 398300, \t Total Gen Loss : 31.620786666870117, \t Total Dis Loss : 3.2190364436246455e-05\n",
      "Steps : 398400, \t Total Gen Loss : 34.08680725097656, \t Total Dis Loss : 1.5865290379224462e-06\n",
      "Steps : 398500, \t Total Gen Loss : 32.87104415893555, \t Total Dis Loss : 2.1176024347369093e-06\n",
      "Steps : 398600, \t Total Gen Loss : 39.155601501464844, \t Total Dis Loss : 2.4055957510427106e-06\n",
      "Steps : 398700, \t Total Gen Loss : 37.43292236328125, \t Total Dis Loss : 4.128520458834828e-07\n",
      "Steps : 398800, \t Total Gen Loss : 33.8690071105957, \t Total Dis Loss : 2.4109729110932676e-06\n",
      "Steps : 398900, \t Total Gen Loss : 35.837196350097656, \t Total Dis Loss : 9.600955763744423e-07\n",
      "Steps : 399000, \t Total Gen Loss : 35.67250061035156, \t Total Dis Loss : 9.913609574141447e-06\n",
      "Steps : 399100, \t Total Gen Loss : 29.98111343383789, \t Total Dis Loss : 7.3001874625333585e-06\n",
      "Steps : 399200, \t Total Gen Loss : 32.3824462890625, \t Total Dis Loss : 8.132056450449454e-07\n",
      "Steps : 399300, \t Total Gen Loss : 31.35521125793457, \t Total Dis Loss : 4.406781954457983e-06\n",
      "Time for epoch 71 is 68.96483516693115 sec\n",
      "Steps : 399400, \t Total Gen Loss : 32.0230598449707, \t Total Dis Loss : 2.8639908578043105e-06\n",
      "Steps : 399500, \t Total Gen Loss : 34.439605712890625, \t Total Dis Loss : 1.5704908946645446e-06\n",
      "Steps : 399600, \t Total Gen Loss : 35.871036529541016, \t Total Dis Loss : 2.6587101729091955e-06\n",
      "Steps : 399700, \t Total Gen Loss : 37.639225006103516, \t Total Dis Loss : 2.83350868812704e-06\n",
      "Steps : 399800, \t Total Gen Loss : 33.84254455566406, \t Total Dis Loss : 1.652733885748603e-06\n",
      "Steps : 399900, \t Total Gen Loss : 32.69316101074219, \t Total Dis Loss : 1.4709586139360908e-05\n",
      "Steps : 400000, \t Total Gen Loss : 32.988258361816406, \t Total Dis Loss : 3.3161802548420383e-06\n",
      "Steps : 400100, \t Total Gen Loss : 36.26689910888672, \t Total Dis Loss : 3.1135543395066634e-05\n",
      "Steps : 400200, \t Total Gen Loss : 33.38435363769531, \t Total Dis Loss : 9.839170161285438e-06\n",
      "Steps : 400300, \t Total Gen Loss : 34.99024200439453, \t Total Dis Loss : 7.858375283831265e-07\n",
      "Steps : 400400, \t Total Gen Loss : 30.573101043701172, \t Total Dis Loss : 1.3993256970934453e-06\n",
      "Steps : 400500, \t Total Gen Loss : 32.85347366333008, \t Total Dis Loss : 3.0364772101165727e-06\n",
      "Steps : 400600, \t Total Gen Loss : 37.068603515625, \t Total Dis Loss : 1.9530434656189755e-06\n",
      "Steps : 400700, \t Total Gen Loss : 30.947891235351562, \t Total Dis Loss : 5.4681709116266575e-06\n",
      "Steps : 400800, \t Total Gen Loss : 33.957244873046875, \t Total Dis Loss : 1.1650481610558927e-06\n",
      "Steps : 400900, \t Total Gen Loss : 32.49260330200195, \t Total Dis Loss : 3.2093676054500975e-06\n",
      "Steps : 401000, \t Total Gen Loss : 32.565216064453125, \t Total Dis Loss : 1.7206888287546462e-06\n",
      "Steps : 401100, \t Total Gen Loss : 36.87779998779297, \t Total Dis Loss : 1.5771890957694268e-06\n",
      "Steps : 401200, \t Total Gen Loss : 33.89457321166992, \t Total Dis Loss : 2.452262151564355e-06\n",
      "Steps : 401300, \t Total Gen Loss : 35.622230529785156, \t Total Dis Loss : 5.78658500671736e-06\n",
      "Steps : 401400, \t Total Gen Loss : 32.38527297973633, \t Total Dis Loss : 2.836454768839758e-06\n",
      "Steps : 401500, \t Total Gen Loss : 35.3248291015625, \t Total Dis Loss : 7.066586249493412e-07\n",
      "Steps : 401600, \t Total Gen Loss : 34.804019927978516, \t Total Dis Loss : 1.298278789363394e-06\n",
      "Steps : 401700, \t Total Gen Loss : 33.56889343261719, \t Total Dis Loss : 4.353094595899165e-07\n",
      "Steps : 401800, \t Total Gen Loss : 31.244487762451172, \t Total Dis Loss : 1.3837642427461105e-06\n",
      "Steps : 401900, \t Total Gen Loss : 36.607582092285156, \t Total Dis Loss : 1.982734602279379e-06\n",
      "Steps : 402000, \t Total Gen Loss : 35.42644500732422, \t Total Dis Loss : 5.74344267079141e-05\n",
      "Steps : 402100, \t Total Gen Loss : 36.624000549316406, \t Total Dis Loss : 1.1585315405682195e-05\n",
      "Steps : 402200, \t Total Gen Loss : 32.90910339355469, \t Total Dis Loss : 0.0002067283057840541\n",
      "Steps : 402300, \t Total Gen Loss : 35.55747985839844, \t Total Dis Loss : 6.923334694874939e-06\n",
      "Steps : 402400, \t Total Gen Loss : 34.97093963623047, \t Total Dis Loss : 6.136352567409631e-06\n",
      "Steps : 402500, \t Total Gen Loss : 36.2441291809082, \t Total Dis Loss : 5.606044396699872e-06\n",
      "Steps : 402600, \t Total Gen Loss : 38.02519226074219, \t Total Dis Loss : 7.525055707446882e-07\n",
      "Steps : 402700, \t Total Gen Loss : 28.271717071533203, \t Total Dis Loss : 2.2587044441024773e-05\n",
      "Steps : 402800, \t Total Gen Loss : 29.351985931396484, \t Total Dis Loss : 7.568764885945711e-07\n",
      "Steps : 402900, \t Total Gen Loss : 30.889707565307617, \t Total Dis Loss : 2.8743746952386573e-05\n",
      "Steps : 403000, \t Total Gen Loss : 30.08075523376465, \t Total Dis Loss : 3.22720043186564e-05\n",
      "Steps : 403100, \t Total Gen Loss : 32.18860626220703, \t Total Dis Loss : 2.0208519799780333e-06\n",
      "Steps : 403200, \t Total Gen Loss : 33.188323974609375, \t Total Dis Loss : 7.197408649517456e-06\n",
      "Steps : 403300, \t Total Gen Loss : 32.90474319458008, \t Total Dis Loss : 3.4659318771446124e-05\n",
      "Steps : 403400, \t Total Gen Loss : 32.70636749267578, \t Total Dis Loss : 0.00014448897854890674\n",
      "Steps : 403500, \t Total Gen Loss : 33.29176712036133, \t Total Dis Loss : 2.0711799152195454e-05\n",
      "Steps : 403600, \t Total Gen Loss : 33.60575866699219, \t Total Dis Loss : 1.2351772056717891e-05\n",
      "Steps : 403700, \t Total Gen Loss : 32.96723175048828, \t Total Dis Loss : 2.2040803742129356e-06\n",
      "Steps : 403800, \t Total Gen Loss : 32.972145080566406, \t Total Dis Loss : 2.8165286494186148e-05\n",
      "Steps : 403900, \t Total Gen Loss : 31.56804084777832, \t Total Dis Loss : 3.0722603696631268e-06\n",
      "Steps : 404000, \t Total Gen Loss : 32.23664855957031, \t Total Dis Loss : 2.2030631043890025e-06\n",
      "Steps : 404100, \t Total Gen Loss : 35.18149185180664, \t Total Dis Loss : 1.0280284186592326e-05\n",
      "Steps : 404200, \t Total Gen Loss : 32.298866271972656, \t Total Dis Loss : 1.8566146536613815e-05\n",
      "Steps : 404300, \t Total Gen Loss : 31.601396560668945, \t Total Dis Loss : 2.4044322344707325e-05\n",
      "Steps : 404400, \t Total Gen Loss : 33.326744079589844, \t Total Dis Loss : 9.413814041181467e-06\n",
      "Steps : 404500, \t Total Gen Loss : 32.923458099365234, \t Total Dis Loss : 7.976039341883734e-06\n",
      "Steps : 404600, \t Total Gen Loss : 31.576757431030273, \t Total Dis Loss : 5.120576588524273e-06\n",
      "Steps : 404700, \t Total Gen Loss : 32.536170959472656, \t Total Dis Loss : 1.4985503185016569e-05\n",
      "Steps : 404800, \t Total Gen Loss : 34.941490173339844, \t Total Dis Loss : 4.327699116402073e-06\n",
      "Steps : 404900, \t Total Gen Loss : 34.03468322753906, \t Total Dis Loss : 1.2572471860039514e-05\n",
      "Steps : 405000, \t Total Gen Loss : 33.829097747802734, \t Total Dis Loss : 3.13240377636248e-07\n",
      "Time for epoch 72 is 69.01519250869751 sec\n",
      "Steps : 405100, \t Total Gen Loss : 35.49246597290039, \t Total Dis Loss : 1.5575766383335576e-06\n",
      "Steps : 405200, \t Total Gen Loss : 38.17902374267578, \t Total Dis Loss : 5.194083314563613e-06\n",
      "Steps : 405300, \t Total Gen Loss : 32.076416015625, \t Total Dis Loss : 7.70242913858965e-05\n",
      "Steps : 405400, \t Total Gen Loss : 31.289806365966797, \t Total Dis Loss : 0.00010273022053297609\n",
      "Steps : 405500, \t Total Gen Loss : 33.98692321777344, \t Total Dis Loss : 6.4438904701091815e-06\n",
      "Steps : 405600, \t Total Gen Loss : 31.379186630249023, \t Total Dis Loss : 1.7173511878354475e-05\n",
      "Steps : 405700, \t Total Gen Loss : 32.29892349243164, \t Total Dis Loss : 9.895326911646407e-06\n",
      "Steps : 405800, \t Total Gen Loss : 33.1859016418457, \t Total Dis Loss : 4.326173439039849e-06\n",
      "Steps : 405900, \t Total Gen Loss : 33.35023880004883, \t Total Dis Loss : 0.00012183291255496442\n",
      "Steps : 406000, \t Total Gen Loss : 31.08112907409668, \t Total Dis Loss : 1.9398949007154442e-06\n",
      "Steps : 406100, \t Total Gen Loss : 30.02733612060547, \t Total Dis Loss : 3.415188257349655e-05\n",
      "Steps : 406200, \t Total Gen Loss : 28.982410430908203, \t Total Dis Loss : 1.450937179470202e-05\n",
      "Steps : 406300, \t Total Gen Loss : 27.69110870361328, \t Total Dis Loss : 1.9419034288148396e-05\n",
      "Steps : 406400, \t Total Gen Loss : 32.505252838134766, \t Total Dis Loss : 3.4215265714010457e-06\n",
      "Steps : 406500, \t Total Gen Loss : 29.273681640625, \t Total Dis Loss : 2.4814557036734186e-05\n",
      "Steps : 406600, \t Total Gen Loss : 31.744855880737305, \t Total Dis Loss : 6.536790897371247e-05\n",
      "Steps : 406700, \t Total Gen Loss : 31.778390884399414, \t Total Dis Loss : 2.5061701308004558e-05\n",
      "Steps : 406800, \t Total Gen Loss : 31.061914443969727, \t Total Dis Loss : 1.2575852451846004e-05\n",
      "Steps : 406900, \t Total Gen Loss : 32.685672760009766, \t Total Dis Loss : 4.246149728714954e-06\n",
      "Steps : 407000, \t Total Gen Loss : 33.3664436340332, \t Total Dis Loss : 5.2430600590014365e-06\n",
      "Steps : 407100, \t Total Gen Loss : 31.65291404724121, \t Total Dis Loss : 7.820579412509687e-06\n",
      "Steps : 407200, \t Total Gen Loss : 32.49982833862305, \t Total Dis Loss : 0.00010261446004733443\n",
      "Steps : 407300, \t Total Gen Loss : 36.201995849609375, \t Total Dis Loss : 6.653263881162275e-06\n",
      "Steps : 407400, \t Total Gen Loss : 34.17502212524414, \t Total Dis Loss : 5.022400728194043e-06\n",
      "Steps : 407500, \t Total Gen Loss : 36.00236892700195, \t Total Dis Loss : 4.3927138904109597e-07\n",
      "Steps : 407600, \t Total Gen Loss : 34.312374114990234, \t Total Dis Loss : 4.096820703125559e-06\n",
      "Steps : 407700, \t Total Gen Loss : 34.974708557128906, \t Total Dis Loss : 2.9831830943294335e-06\n",
      "Steps : 407800, \t Total Gen Loss : 34.2325439453125, \t Total Dis Loss : 8.595222425356042e-06\n",
      "Steps : 407900, \t Total Gen Loss : 37.16891860961914, \t Total Dis Loss : 1.1877795031978167e-06\n",
      "Steps : 408000, \t Total Gen Loss : 37.9596061706543, \t Total Dis Loss : 1.09056702513044e-06\n",
      "Steps : 408100, \t Total Gen Loss : 33.24250793457031, \t Total Dis Loss : 2.511791592496593e-07\n",
      "Steps : 408200, \t Total Gen Loss : 32.85578918457031, \t Total Dis Loss : 4.290750439395197e-06\n",
      "Steps : 408300, \t Total Gen Loss : 34.016536712646484, \t Total Dis Loss : 1.6283455579468864e-06\n",
      "Steps : 408400, \t Total Gen Loss : 36.16804504394531, \t Total Dis Loss : 3.881933935190318e-06\n",
      "Steps : 408500, \t Total Gen Loss : 35.18241500854492, \t Total Dis Loss : 5.177141247258987e-06\n",
      "Steps : 408600, \t Total Gen Loss : 33.28468704223633, \t Total Dis Loss : 3.0680223517265404e-06\n",
      "Steps : 408700, \t Total Gen Loss : 37.01710891723633, \t Total Dis Loss : 2.8400556857377524e-06\n",
      "Steps : 408800, \t Total Gen Loss : 31.85357093811035, \t Total Dis Loss : 4.511056886258302e-06\n",
      "Steps : 408900, \t Total Gen Loss : 32.3630485534668, \t Total Dis Loss : 1.7340778867946938e-05\n",
      "Steps : 409000, \t Total Gen Loss : 36.563385009765625, \t Total Dis Loss : 2.3660646547796205e-05\n",
      "Steps : 409100, \t Total Gen Loss : 41.07267379760742, \t Total Dis Loss : 8.323224278683483e-07\n",
      "Steps : 409200, \t Total Gen Loss : 32.41557312011719, \t Total Dis Loss : 3.538719465723261e-05\n",
      "Steps : 409300, \t Total Gen Loss : 30.597509384155273, \t Total Dis Loss : 0.00037443189648911357\n",
      "Steps : 409400, \t Total Gen Loss : 38.07759094238281, \t Total Dis Loss : 3.3564608656888595e-06\n",
      "Steps : 409500, \t Total Gen Loss : 34.099037170410156, \t Total Dis Loss : 9.62603166954068e-07\n",
      "Steps : 409600, \t Total Gen Loss : 35.409393310546875, \t Total Dis Loss : 2.7560886337596457e-06\n",
      "Steps : 409700, \t Total Gen Loss : 39.82151794433594, \t Total Dis Loss : 4.1873769873745914e-07\n",
      "Steps : 409800, \t Total Gen Loss : 33.777259826660156, \t Total Dis Loss : 1.2208114412715076e-06\n",
      "Steps : 409900, \t Total Gen Loss : 33.53596115112305, \t Total Dis Loss : 2.5911210741469404e-06\n",
      "Steps : 410000, \t Total Gen Loss : 38.819236755371094, \t Total Dis Loss : 2.6781535780173726e-06\n",
      "Steps : 410100, \t Total Gen Loss : 31.42654037475586, \t Total Dis Loss : 2.3962760678841732e-05\n",
      "Steps : 410200, \t Total Gen Loss : 36.97283935546875, \t Total Dis Loss : 4.4198109208082315e-06\n",
      "Steps : 410300, \t Total Gen Loss : 37.863521575927734, \t Total Dis Loss : 3.3183783898493857e-07\n",
      "Steps : 410400, \t Total Gen Loss : 38.81743621826172, \t Total Dis Loss : 2.311155412826338e-06\n",
      "Steps : 410500, \t Total Gen Loss : 37.900108337402344, \t Total Dis Loss : 2.8505864975159056e-06\n",
      "Steps : 410600, \t Total Gen Loss : 37.20915603637695, \t Total Dis Loss : 4.635118671103555e-07\n",
      "Time for epoch 73 is 68.97877812385559 sec\n",
      "Steps : 410700, \t Total Gen Loss : 36.08146286010742, \t Total Dis Loss : 2.255357003377867e-06\n",
      "Steps : 410800, \t Total Gen Loss : 36.33258819580078, \t Total Dis Loss : 7.887448418841814e-07\n",
      "Steps : 410900, \t Total Gen Loss : 39.132728576660156, \t Total Dis Loss : 5.475044986269495e-07\n",
      "Steps : 411000, \t Total Gen Loss : 31.75011444091797, \t Total Dis Loss : 0.00020072428742423654\n",
      "Steps : 411100, \t Total Gen Loss : 29.572620391845703, \t Total Dis Loss : 2.0959721950930543e-05\n",
      "Steps : 411200, \t Total Gen Loss : 31.93141746520996, \t Total Dis Loss : 0.00011480653483886272\n",
      "Steps : 411300, \t Total Gen Loss : 33.32515335083008, \t Total Dis Loss : 4.487968180910684e-05\n",
      "Steps : 411400, \t Total Gen Loss : 29.78455352783203, \t Total Dis Loss : 1.5050288311613258e-05\n",
      "Steps : 411500, \t Total Gen Loss : 35.10222625732422, \t Total Dis Loss : 3.868046860588947e-06\n",
      "Steps : 411600, \t Total Gen Loss : 29.814987182617188, \t Total Dis Loss : 8.012220860109664e-06\n",
      "Steps : 411700, \t Total Gen Loss : 33.451995849609375, \t Total Dis Loss : 5.862135822098935e-06\n",
      "Steps : 411800, \t Total Gen Loss : 31.235267639160156, \t Total Dis Loss : 5.685124961019028e-06\n",
      "Steps : 411900, \t Total Gen Loss : 34.281494140625, \t Total Dis Loss : 1.5009222806838807e-05\n",
      "Steps : 412000, \t Total Gen Loss : 30.095815658569336, \t Total Dis Loss : 5.382230028772028e-06\n",
      "Steps : 412100, \t Total Gen Loss : 33.18635177612305, \t Total Dis Loss : 5.108048299007351e-06\n",
      "Steps : 412200, \t Total Gen Loss : 34.360862731933594, \t Total Dis Loss : 1.0843832569662482e-05\n",
      "Steps : 412300, \t Total Gen Loss : 36.05083465576172, \t Total Dis Loss : 1.196913217427209e-06\n",
      "Steps : 412400, \t Total Gen Loss : 33.099456787109375, \t Total Dis Loss : 3.897639089700533e-06\n",
      "Steps : 412500, \t Total Gen Loss : 33.39689636230469, \t Total Dis Loss : 5.302724275679793e-06\n",
      "Steps : 412600, \t Total Gen Loss : 38.461795806884766, \t Total Dis Loss : 1.1202139376109699e-06\n",
      "Steps : 412700, \t Total Gen Loss : 40.23419952392578, \t Total Dis Loss : 1.832213092711754e-05\n",
      "Steps : 412800, \t Total Gen Loss : 39.39102554321289, \t Total Dis Loss : 2.087952907459112e-06\n",
      "Steps : 412900, \t Total Gen Loss : 37.609615325927734, \t Total Dis Loss : 2.5917324819602072e-06\n",
      "Steps : 413000, \t Total Gen Loss : 38.21765899658203, \t Total Dis Loss : 1.8110619066646905e-06\n",
      "Steps : 413100, \t Total Gen Loss : 37.22580337524414, \t Total Dis Loss : 1.3497660233952047e-07\n",
      "Steps : 413200, \t Total Gen Loss : 35.32177734375, \t Total Dis Loss : 3.4389861980343994e-07\n",
      "Steps : 413300, \t Total Gen Loss : 43.24983596801758, \t Total Dis Loss : 8.758836884226184e-06\n",
      "Steps : 413400, \t Total Gen Loss : 40.082664489746094, \t Total Dis Loss : 2.7597236112342216e-05\n",
      "Steps : 413500, \t Total Gen Loss : 40.79116439819336, \t Total Dis Loss : 1.9609169612522237e-05\n",
      "Steps : 413600, \t Total Gen Loss : 38.40889358520508, \t Total Dis Loss : 1.3507955145541928e-06\n",
      "Steps : 413700, \t Total Gen Loss : 41.75043487548828, \t Total Dis Loss : 8.828249519865494e-06\n",
      "Steps : 413800, \t Total Gen Loss : 40.756187438964844, \t Total Dis Loss : 1.7092155758291483e-05\n",
      "Steps : 413900, \t Total Gen Loss : 40.56117248535156, \t Total Dis Loss : 3.254693001508713e-05\n",
      "Steps : 414000, \t Total Gen Loss : 36.72342300415039, \t Total Dis Loss : 2.6027757371593907e-07\n",
      "Steps : 414100, \t Total Gen Loss : 41.950904846191406, \t Total Dis Loss : 0.00019893348508048803\n",
      "Steps : 414200, \t Total Gen Loss : 39.353431701660156, \t Total Dis Loss : 2.4276789645227836e-07\n",
      "Steps : 414300, \t Total Gen Loss : 39.062713623046875, \t Total Dis Loss : 3.948636731365696e-06\n",
      "Steps : 414400, \t Total Gen Loss : 38.96416473388672, \t Total Dis Loss : 9.720471098262351e-06\n",
      "Steps : 414500, \t Total Gen Loss : 39.67313003540039, \t Total Dis Loss : 4.4840766122433706e-07\n",
      "Steps : 414600, \t Total Gen Loss : 40.06127166748047, \t Total Dis Loss : 4.851410153605684e-07\n",
      "Steps : 414700, \t Total Gen Loss : 38.93304443359375, \t Total Dis Loss : 8.23951751272034e-08\n",
      "Steps : 414800, \t Total Gen Loss : 37.6223258972168, \t Total Dis Loss : 3.750994665097096e-06\n",
      "Steps : 414900, \t Total Gen Loss : 38.94990921020508, \t Total Dis Loss : 3.144314746350574e-07\n",
      "Steps : 415000, \t Total Gen Loss : 26.856924057006836, \t Total Dis Loss : 0.46003028750419617\n",
      "Steps : 415100, \t Total Gen Loss : 38.58203887939453, \t Total Dis Loss : 0.0001464687957195565\n",
      "Steps : 415200, \t Total Gen Loss : 40.74193572998047, \t Total Dis Loss : 1.0416472377983155e-06\n",
      "Steps : 415300, \t Total Gen Loss : 38.411746978759766, \t Total Dis Loss : 8.670588869108542e-08\n",
      "Steps : 415400, \t Total Gen Loss : 39.650550842285156, \t Total Dis Loss : 4.0658218836142623e-07\n",
      "Steps : 415500, \t Total Gen Loss : 38.73123550415039, \t Total Dis Loss : 3.215048423044209e-07\n",
      "Steps : 415600, \t Total Gen Loss : 40.15855407714844, \t Total Dis Loss : 3.558683374649263e-06\n",
      "Steps : 415700, \t Total Gen Loss : 36.030487060546875, \t Total Dis Loss : 3.568790774011177e-08\n",
      "Steps : 415800, \t Total Gen Loss : 40.36656951904297, \t Total Dis Loss : 5.627501877825125e-07\n",
      "Steps : 415900, \t Total Gen Loss : 37.0418815612793, \t Total Dis Loss : 2.6424586394568905e-06\n",
      "Steps : 416000, \t Total Gen Loss : 35.78825378417969, \t Total Dis Loss : 7.032943472040643e-07\n",
      "Steps : 416100, \t Total Gen Loss : 40.54860305786133, \t Total Dis Loss : 4.354673365014605e-06\n",
      "Steps : 416200, \t Total Gen Loss : 37.86716079711914, \t Total Dis Loss : 7.605078167216561e-08\n",
      "Time for epoch 74 is 69.02586698532104 sec\n",
      "Steps : 416300, \t Total Gen Loss : 33.87562561035156, \t Total Dis Loss : 1.7655078465850238e-07\n",
      "Steps : 416400, \t Total Gen Loss : 37.71180725097656, \t Total Dis Loss : 2.729832431214163e-07\n",
      "Steps : 416500, \t Total Gen Loss : 38.15009689331055, \t Total Dis Loss : 1.0419607860967517e-06\n",
      "Steps : 416600, \t Total Gen Loss : 38.0084114074707, \t Total Dis Loss : 1.7928203988049063e-06\n",
      "Steps : 416700, \t Total Gen Loss : 39.22102737426758, \t Total Dis Loss : 3.57029477981996e-07\n",
      "Steps : 416800, \t Total Gen Loss : 42.68331527709961, \t Total Dis Loss : 8.343389481524355e-07\n",
      "Steps : 416900, \t Total Gen Loss : 37.07318115234375, \t Total Dis Loss : 3.697595047924551e-07\n",
      "Steps : 417000, \t Total Gen Loss : 42.897315979003906, \t Total Dis Loss : 2.4473853045492433e-05\n",
      "Steps : 417100, \t Total Gen Loss : 41.02371597290039, \t Total Dis Loss : 6.53548931950354e-06\n",
      "Steps : 417200, \t Total Gen Loss : 39.21210479736328, \t Total Dis Loss : 1.3945301361673046e-05\n",
      "Steps : 417300, \t Total Gen Loss : 38.86564254760742, \t Total Dis Loss : 1.3844168051946326e-06\n",
      "Steps : 417400, \t Total Gen Loss : 41.96086883544922, \t Total Dis Loss : 7.810828606125142e-07\n",
      "Steps : 417500, \t Total Gen Loss : 37.4561653137207, \t Total Dis Loss : 8.380710823985282e-06\n",
      "Steps : 417600, \t Total Gen Loss : 38.798805236816406, \t Total Dis Loss : 8.958330113273405e-07\n",
      "Steps : 417700, \t Total Gen Loss : 38.499916076660156, \t Total Dis Loss : 4.028006878797896e-05\n",
      "Steps : 417800, \t Total Gen Loss : 38.6025390625, \t Total Dis Loss : 2.892955080824322e-06\n",
      "Steps : 417900, \t Total Gen Loss : 36.498329162597656, \t Total Dis Loss : 1.3889101637687418e-06\n",
      "Steps : 418000, \t Total Gen Loss : 34.837318420410156, \t Total Dis Loss : 4.098544081898581e-07\n",
      "Steps : 418100, \t Total Gen Loss : 38.832427978515625, \t Total Dis Loss : 9.227417052670717e-08\n",
      "Steps : 418200, \t Total Gen Loss : 38.86109161376953, \t Total Dis Loss : 1.8304612581232504e-07\n",
      "Steps : 418300, \t Total Gen Loss : 39.7225227355957, \t Total Dis Loss : 1.7442035016301816e-07\n",
      "Steps : 418400, \t Total Gen Loss : 36.05231857299805, \t Total Dis Loss : 7.526575274141578e-08\n",
      "Steps : 418500, \t Total Gen Loss : 38.040916442871094, \t Total Dis Loss : 1.5292729926841275e-07\n",
      "Steps : 418600, \t Total Gen Loss : 39.133056640625, \t Total Dis Loss : 3.064001248276327e-07\n",
      "Steps : 418700, \t Total Gen Loss : 33.83424377441406, \t Total Dis Loss : 1.6371891433664132e-06\n",
      "Steps : 418800, \t Total Gen Loss : 32.56181335449219, \t Total Dis Loss : 1.1517889788592583e-06\n",
      "Steps : 418900, \t Total Gen Loss : 36.651214599609375, \t Total Dis Loss : 2.9379070838331245e-05\n",
      "Steps : 419000, \t Total Gen Loss : 41.618526458740234, \t Total Dis Loss : 7.477660801669117e-07\n",
      "Steps : 419100, \t Total Gen Loss : 30.03189468383789, \t Total Dis Loss : 4.996513325750129e-06\n",
      "Steps : 419200, \t Total Gen Loss : 33.701141357421875, \t Total Dis Loss : 1.3699766441277461e-06\n",
      "Steps : 419300, \t Total Gen Loss : 33.772247314453125, \t Total Dis Loss : 3.507740984787233e-06\n",
      "Steps : 419400, \t Total Gen Loss : 32.44486999511719, \t Total Dis Loss : 2.411172681604512e-05\n",
      "Steps : 419500, \t Total Gen Loss : 34.09669494628906, \t Total Dis Loss : 8.26261384645477e-06\n",
      "Steps : 419600, \t Total Gen Loss : 31.762985229492188, \t Total Dis Loss : 2.7399155442253686e-05\n",
      "Steps : 419700, \t Total Gen Loss : 39.75920867919922, \t Total Dis Loss : 8.373788205062738e-07\n",
      "Steps : 419800, \t Total Gen Loss : 36.515846252441406, \t Total Dis Loss : 6.960008613532409e-05\n",
      "Steps : 419900, \t Total Gen Loss : 37.69841384887695, \t Total Dis Loss : 5.1091378736600745e-06\n",
      "Steps : 420000, \t Total Gen Loss : 38.778663635253906, \t Total Dis Loss : 5.314844884196646e-07\n",
      "Steps : 420100, \t Total Gen Loss : 28.921682357788086, \t Total Dis Loss : 0.0001769303053151816\n",
      "Steps : 420200, \t Total Gen Loss : 28.760969161987305, \t Total Dis Loss : 4.626081135938875e-06\n",
      "Steps : 420300, \t Total Gen Loss : 31.001754760742188, \t Total Dis Loss : 1.8485310647520237e-05\n",
      "Steps : 420400, \t Total Gen Loss : 35.31436538696289, \t Total Dis Loss : 9.904661055770703e-06\n",
      "Steps : 420500, \t Total Gen Loss : 32.657779693603516, \t Total Dis Loss : 6.368328467942774e-05\n",
      "Steps : 420600, \t Total Gen Loss : 35.420005798339844, \t Total Dis Loss : 3.2409610867034644e-05\n",
      "Steps : 420700, \t Total Gen Loss : 30.302490234375, \t Total Dis Loss : 8.533804248145316e-06\n",
      "Steps : 420800, \t Total Gen Loss : 33.00811767578125, \t Total Dis Loss : 5.4314577937475406e-06\n",
      "Steps : 420900, \t Total Gen Loss : 31.639415740966797, \t Total Dis Loss : 4.3972286221105605e-05\n",
      "Steps : 421000, \t Total Gen Loss : 33.43011474609375, \t Total Dis Loss : 2.7577900709729875e-06\n",
      "Steps : 421100, \t Total Gen Loss : 33.4467658996582, \t Total Dis Loss : 2.3404304556606803e-06\n",
      "Steps : 421200, \t Total Gen Loss : 32.50648880004883, \t Total Dis Loss : 1.6634060102660442e-06\n",
      "Steps : 421300, \t Total Gen Loss : 31.451475143432617, \t Total Dis Loss : 1.6382643934775842e-06\n",
      "Steps : 421400, \t Total Gen Loss : 31.908775329589844, \t Total Dis Loss : 6.114520601840923e-06\n",
      "Steps : 421500, \t Total Gen Loss : 32.486167907714844, \t Total Dis Loss : 1.033289390761638e-05\n",
      "Steps : 421600, \t Total Gen Loss : 33.182823181152344, \t Total Dis Loss : 1.2418801816238556e-05\n",
      "Steps : 421700, \t Total Gen Loss : 34.518516540527344, \t Total Dis Loss : 9.439156087864831e-07\n",
      "Steps : 421800, \t Total Gen Loss : 30.78777503967285, \t Total Dis Loss : 1.2985647117602639e-06\n",
      "Time for epoch 75 is 69.8064329624176 sec\n",
      "Steps : 421900, \t Total Gen Loss : 33.17643737792969, \t Total Dis Loss : 1.555398921482265e-05\n",
      "Steps : 422000, \t Total Gen Loss : 32.6802978515625, \t Total Dis Loss : 1.7956008377950639e-06\n",
      "Steps : 422100, \t Total Gen Loss : 31.975120544433594, \t Total Dis Loss : 1.0017716704169288e-05\n",
      "Steps : 422200, \t Total Gen Loss : 32.811378479003906, \t Total Dis Loss : 7.608627583977068e-06\n",
      "Steps : 422300, \t Total Gen Loss : 36.39262390136719, \t Total Dis Loss : 8.718695426068734e-06\n",
      "Steps : 422400, \t Total Gen Loss : 32.77036666870117, \t Total Dis Loss : 2.565485192462802e-05\n",
      "Steps : 422500, \t Total Gen Loss : 31.248811721801758, \t Total Dis Loss : 1.4730454495293088e-05\n",
      "Steps : 422600, \t Total Gen Loss : 32.088565826416016, \t Total Dis Loss : 2.39068924656749e-07\n",
      "Steps : 422700, \t Total Gen Loss : 34.19436264038086, \t Total Dis Loss : 2.908039959947928e-07\n",
      "Steps : 422800, \t Total Gen Loss : 33.60457992553711, \t Total Dis Loss : 5.903517603655928e-07\n",
      "Steps : 422900, \t Total Gen Loss : 36.5241813659668, \t Total Dis Loss : 8.251685699178779e-07\n",
      "Steps : 423000, \t Total Gen Loss : 31.577699661254883, \t Total Dis Loss : 0.005746321752667427\n",
      "Steps : 423100, \t Total Gen Loss : 34.28321838378906, \t Total Dis Loss : 3.5795343933386903e-07\n",
      "Steps : 423200, \t Total Gen Loss : 34.56261444091797, \t Total Dis Loss : 6.458802772613126e-07\n",
      "Steps : 423300, \t Total Gen Loss : 36.087921142578125, \t Total Dis Loss : 2.530352048779605e-06\n",
      "Steps : 423400, \t Total Gen Loss : 31.36406707763672, \t Total Dis Loss : 8.57936640841217e-07\n",
      "Steps : 423500, \t Total Gen Loss : 34.05696487426758, \t Total Dis Loss : 4.998947247258911e-07\n",
      "Steps : 423600, \t Total Gen Loss : 34.101863861083984, \t Total Dis Loss : 6.049076546332799e-06\n",
      "Steps : 423700, \t Total Gen Loss : 34.97581481933594, \t Total Dis Loss : 1.6758783658588072e-06\n",
      "Steps : 423800, \t Total Gen Loss : 34.87557601928711, \t Total Dis Loss : 4.234117625401268e-07\n",
      "Steps : 423900, \t Total Gen Loss : 36.636329650878906, \t Total Dis Loss : 3.3562844237167155e-06\n",
      "Steps : 424000, \t Total Gen Loss : 35.702613830566406, \t Total Dis Loss : 1.8277682102052495e-05\n",
      "Steps : 424100, \t Total Gen Loss : 42.29871368408203, \t Total Dis Loss : 9.27410894746572e-07\n",
      "Steps : 424200, \t Total Gen Loss : 38.88133239746094, \t Total Dis Loss : 0.16525611281394958\n",
      "Steps : 424300, \t Total Gen Loss : 34.23123550415039, \t Total Dis Loss : 3.905910125467926e-05\n",
      "Steps : 424400, \t Total Gen Loss : 35.41712951660156, \t Total Dis Loss : 5.121319645695621e-06\n",
      "Steps : 424500, \t Total Gen Loss : 35.60369110107422, \t Total Dis Loss : 5.6870694606914185e-06\n",
      "Steps : 424600, \t Total Gen Loss : 37.800498962402344, \t Total Dis Loss : 2.126630988641409e-06\n",
      "Steps : 424700, \t Total Gen Loss : 38.355472564697266, \t Total Dis Loss : 1.879800493043149e-07\n",
      "Steps : 424800, \t Total Gen Loss : 35.166656494140625, \t Total Dis Loss : 9.271935255128483e-07\n",
      "Steps : 424900, \t Total Gen Loss : 32.89729309082031, \t Total Dis Loss : 1.7301701518590562e-05\n",
      "Steps : 425000, \t Total Gen Loss : 32.1730842590332, \t Total Dis Loss : 9.864787898550276e-06\n",
      "Steps : 425100, \t Total Gen Loss : 31.426815032958984, \t Total Dis Loss : 9.407579455000814e-06\n",
      "Steps : 425200, \t Total Gen Loss : 32.84049987792969, \t Total Dis Loss : 1.7018979633576237e-05\n",
      "Steps : 425300, \t Total Gen Loss : 37.90215301513672, \t Total Dis Loss : 1.0591101329282537e-07\n",
      "Steps : 425400, \t Total Gen Loss : 41.562435150146484, \t Total Dis Loss : 3.519331812640303e-06\n",
      "Steps : 425500, \t Total Gen Loss : 40.5269775390625, \t Total Dis Loss : 9.845326758295414e-07\n",
      "Steps : 425600, \t Total Gen Loss : 42.019779205322266, \t Total Dis Loss : 2.33875289268326e-06\n",
      "Steps : 425700, \t Total Gen Loss : 37.24288558959961, \t Total Dis Loss : 0.0004143202386330813\n",
      "Steps : 425800, \t Total Gen Loss : 38.49492263793945, \t Total Dis Loss : 7.339120884353179e-07\n",
      "Steps : 425900, \t Total Gen Loss : 37.3184814453125, \t Total Dis Loss : 4.114567957458348e-07\n",
      "Steps : 426000, \t Total Gen Loss : 40.969970703125, \t Total Dis Loss : 2.506490375253634e-07\n",
      "Steps : 426100, \t Total Gen Loss : 38.65007781982422, \t Total Dis Loss : 7.064595024530718e-07\n",
      "Steps : 426200, \t Total Gen Loss : 41.377159118652344, \t Total Dis Loss : 3.383404873602558e-06\n",
      "Steps : 426300, \t Total Gen Loss : 37.875732421875, \t Total Dis Loss : 2.809873080877878e-07\n",
      "Steps : 426400, \t Total Gen Loss : 37.8675537109375, \t Total Dis Loss : 7.17871216693311e-07\n",
      "Steps : 426500, \t Total Gen Loss : 38.09357452392578, \t Total Dis Loss : 3.3848033353933715e-07\n",
      "Steps : 426600, \t Total Gen Loss : 36.27030563354492, \t Total Dis Loss : 1.7215762682099012e-07\n",
      "Steps : 426700, \t Total Gen Loss : 35.59403991699219, \t Total Dis Loss : 4.349411938164849e-06\n",
      "Steps : 426800, \t Total Gen Loss : 32.67158508300781, \t Total Dis Loss : 9.222877110914851e-07\n",
      "Steps : 426900, \t Total Gen Loss : 33.810577392578125, \t Total Dis Loss : 3.962455821238109e-07\n",
      "Steps : 427000, \t Total Gen Loss : 36.24134826660156, \t Total Dis Loss : 1.0920629023303263e-07\n",
      "Steps : 427100, \t Total Gen Loss : 31.841642379760742, \t Total Dis Loss : 8.538183465134352e-06\n",
      "Steps : 427200, \t Total Gen Loss : 37.831092834472656, \t Total Dis Loss : 1.3803854017169215e-05\n",
      "Steps : 427300, \t Total Gen Loss : 33.37558364868164, \t Total Dis Loss : 3.98725705963443e-06\n",
      "Steps : 427400, \t Total Gen Loss : 32.38166427612305, \t Total Dis Loss : 5.361207058740547e-06\n",
      "Steps : 427500, \t Total Gen Loss : 31.717208862304688, \t Total Dis Loss : 1.7168988051707856e-05\n",
      "Time for epoch 76 is 69.20003914833069 sec\n",
      "Steps : 427600, \t Total Gen Loss : 34.89244079589844, \t Total Dis Loss : 2.002358451136388e-05\n",
      "Steps : 427700, \t Total Gen Loss : 34.3938102722168, \t Total Dis Loss : 4.167614406469511e-06\n",
      "Steps : 427800, \t Total Gen Loss : 35.62336349487305, \t Total Dis Loss : 2.996474393057724e-07\n",
      "Steps : 427900, \t Total Gen Loss : 36.992759704589844, \t Total Dis Loss : 1.7427900047550793e-06\n",
      "Steps : 428000, \t Total Gen Loss : 38.583465576171875, \t Total Dis Loss : 1.4571178326150402e-06\n",
      "Steps : 428100, \t Total Gen Loss : 37.94633102416992, \t Total Dis Loss : 2.5047256713151e-06\n",
      "Steps : 428200, \t Total Gen Loss : 36.406822204589844, \t Total Dis Loss : 1.0865112017199863e-05\n",
      "Steps : 428300, \t Total Gen Loss : 37.584129333496094, \t Total Dis Loss : 4.5398419388220645e-06\n",
      "Steps : 428400, \t Total Gen Loss : 34.818328857421875, \t Total Dis Loss : 6.032909368514083e-06\n",
      "Steps : 428500, \t Total Gen Loss : 37.095481872558594, \t Total Dis Loss : 1.9934532247134484e-06\n",
      "Steps : 428600, \t Total Gen Loss : 34.70247268676758, \t Total Dis Loss : 2.258214408357162e-06\n",
      "Steps : 428700, \t Total Gen Loss : 35.25031280517578, \t Total Dis Loss : 1.8644825559022138e-06\n",
      "Steps : 428800, \t Total Gen Loss : 34.31578826904297, \t Total Dis Loss : 4.349305697814998e-07\n",
      "Steps : 428900, \t Total Gen Loss : 37.54289245605469, \t Total Dis Loss : 1.9323017568240175e-06\n",
      "Steps : 429000, \t Total Gen Loss : 38.004173278808594, \t Total Dis Loss : 4.5194963149697287e-07\n",
      "Steps : 429100, \t Total Gen Loss : 35.439659118652344, \t Total Dis Loss : 2.544038579799235e-06\n",
      "Steps : 429200, \t Total Gen Loss : 38.79847717285156, \t Total Dis Loss : 3.705310746227042e-06\n",
      "Steps : 429300, \t Total Gen Loss : 37.66358947753906, \t Total Dis Loss : 3.593571875626367e-07\n",
      "Steps : 429400, \t Total Gen Loss : 36.070709228515625, \t Total Dis Loss : 7.152262924137176e-07\n",
      "Steps : 429500, \t Total Gen Loss : 39.19181442260742, \t Total Dis Loss : 1.5236159356391e-07\n",
      "Steps : 429600, \t Total Gen Loss : 38.04960632324219, \t Total Dis Loss : 1.6620076621620683e-06\n",
      "Steps : 429700, \t Total Gen Loss : 34.13063049316406, \t Total Dis Loss : 3.15852639687364e-06\n",
      "Steps : 429800, \t Total Gen Loss : 37.83634948730469, \t Total Dis Loss : 1.5916672055027448e-05\n",
      "Steps : 429900, \t Total Gen Loss : 41.444087982177734, \t Total Dis Loss : 3.942241164622828e-06\n",
      "Steps : 430000, \t Total Gen Loss : 37.75009536743164, \t Total Dis Loss : 4.120869107282488e-06\n",
      "Steps : 430100, \t Total Gen Loss : 39.60296630859375, \t Total Dis Loss : 1.449849696655292e-05\n",
      "Steps : 430200, \t Total Gen Loss : 40.438446044921875, \t Total Dis Loss : 6.910628371770144e-07\n",
      "Steps : 430300, \t Total Gen Loss : 35.95491027832031, \t Total Dis Loss : 1.6102391953154438e-07\n",
      "Steps : 430400, \t Total Gen Loss : 36.298980712890625, \t Total Dis Loss : 1.3813587429467589e-06\n",
      "Steps : 430500, \t Total Gen Loss : 38.49945831298828, \t Total Dis Loss : 2.060904989775736e-06\n",
      "Steps : 430600, \t Total Gen Loss : 37.317161560058594, \t Total Dis Loss : 1.3236890481493901e-06\n",
      "Steps : 430700, \t Total Gen Loss : 37.786231994628906, \t Total Dis Loss : 5.271417649055365e-07\n",
      "Steps : 430800, \t Total Gen Loss : 35.122554779052734, \t Total Dis Loss : 5.980697181939831e-08\n",
      "Steps : 430900, \t Total Gen Loss : 37.33579635620117, \t Total Dis Loss : 6.6000603737848e-07\n",
      "Steps : 431000, \t Total Gen Loss : 37.37245178222656, \t Total Dis Loss : 1.0613346148602432e-06\n",
      "Steps : 431100, \t Total Gen Loss : 37.15073013305664, \t Total Dis Loss : 2.8163287879579e-07\n",
      "Steps : 431200, \t Total Gen Loss : 38.68553161621094, \t Total Dis Loss : 8.232586310441548e-07\n",
      "Steps : 431300, \t Total Gen Loss : 34.096534729003906, \t Total Dis Loss : 3.590575033740606e-06\n",
      "Steps : 431400, \t Total Gen Loss : 36.41738510131836, \t Total Dis Loss : 2.693441558676568e-07\n",
      "Steps : 431500, \t Total Gen Loss : 38.046478271484375, \t Total Dis Loss : 7.896880447333388e-07\n",
      "Steps : 431600, \t Total Gen Loss : 36.008697509765625, \t Total Dis Loss : 2.7565542950469535e-06\n",
      "Steps : 431700, \t Total Gen Loss : 36.879119873046875, \t Total Dis Loss : 9.014413535624044e-07\n",
      "Steps : 431800, \t Total Gen Loss : 35.57139587402344, \t Total Dis Loss : 5.241169986902605e-08\n",
      "Steps : 431900, \t Total Gen Loss : 39.450042724609375, \t Total Dis Loss : 3.3543658446433255e-06\n",
      "Steps : 432000, \t Total Gen Loss : 38.765960693359375, \t Total Dis Loss : 3.6136279391030257e-07\n",
      "Steps : 432100, \t Total Gen Loss : 36.39002990722656, \t Total Dis Loss : 4.940081339555036e-07\n",
      "Steps : 432200, \t Total Gen Loss : 36.54686737060547, \t Total Dis Loss : 8.251226972788572e-07\n",
      "Steps : 432300, \t Total Gen Loss : 37.57274627685547, \t Total Dis Loss : 2.9356936011026846e-06\n",
      "Steps : 432400, \t Total Gen Loss : 38.19097137451172, \t Total Dis Loss : 7.057132052068482e-07\n",
      "Steps : 432500, \t Total Gen Loss : 34.691558837890625, \t Total Dis Loss : 2.5271341996813135e-07\n",
      "Steps : 432600, \t Total Gen Loss : 37.538169860839844, \t Total Dis Loss : 3.1012490353532485e-07\n",
      "Steps : 432700, \t Total Gen Loss : 35.16265106201172, \t Total Dis Loss : 8.862391496222699e-07\n",
      "Steps : 432800, \t Total Gen Loss : 36.76472091674805, \t Total Dis Loss : 4.286466719349846e-07\n",
      "Steps : 432900, \t Total Gen Loss : 36.403785705566406, \t Total Dis Loss : 1.547911381294398e-07\n",
      "Steps : 433000, \t Total Gen Loss : 33.776126861572266, \t Total Dis Loss : 1.328471626038663e-06\n",
      "Steps : 433100, \t Total Gen Loss : 33.890987396240234, \t Total Dis Loss : 6.59668273783609e-07\n",
      "Time for epoch 77 is 69.00057864189148 sec\n",
      "Steps : 433200, \t Total Gen Loss : 37.66337585449219, \t Total Dis Loss : 4.824548227588821e-07\n",
      "Steps : 433300, \t Total Gen Loss : 37.276611328125, \t Total Dis Loss : 9.197241865877004e-07\n",
      "Steps : 433400, \t Total Gen Loss : 38.269989013671875, \t Total Dis Loss : 6.524537639052141e-07\n",
      "Steps : 433500, \t Total Gen Loss : 37.19324493408203, \t Total Dis Loss : 8.552061444788706e-06\n",
      "Steps : 433600, \t Total Gen Loss : 38.47242736816406, \t Total Dis Loss : 1.737846218929917e-06\n",
      "Steps : 433700, \t Total Gen Loss : 37.582359313964844, \t Total Dis Loss : 2.8526386586236185e-07\n",
      "Steps : 433800, \t Total Gen Loss : 36.91424560546875, \t Total Dis Loss : 3.2078003187052673e-07\n",
      "Steps : 433900, \t Total Gen Loss : 41.11578369140625, \t Total Dis Loss : 7.400565777970769e-07\n",
      "Steps : 434000, \t Total Gen Loss : 35.63742446899414, \t Total Dis Loss : 2.1702892354369396e-06\n",
      "Steps : 434100, \t Total Gen Loss : 34.44580078125, \t Total Dis Loss : 5.438291736936662e-06\n",
      "Steps : 434200, \t Total Gen Loss : 36.43193435668945, \t Total Dis Loss : 2.222378157057392e-07\n",
      "Steps : 434300, \t Total Gen Loss : 32.27808380126953, \t Total Dis Loss : 3.580889597287751e-06\n",
      "Steps : 434400, \t Total Gen Loss : 37.26284408569336, \t Total Dis Loss : 1.0952481943604653e-06\n",
      "Steps : 434500, \t Total Gen Loss : 33.21580505371094, \t Total Dis Loss : 1.5428507822434767e-06\n",
      "Steps : 434600, \t Total Gen Loss : 37.281410217285156, \t Total Dis Loss : 4.556350631901296e-06\n",
      "Steps : 434700, \t Total Gen Loss : 34.38758850097656, \t Total Dis Loss : 1.3876933735446073e-06\n",
      "Steps : 434800, \t Total Gen Loss : 34.36200714111328, \t Total Dis Loss : 2.225258413091069e-06\n",
      "Steps : 434900, \t Total Gen Loss : 33.96944046020508, \t Total Dis Loss : 3.817981166776008e-07\n",
      "Steps : 435000, \t Total Gen Loss : 38.332191467285156, \t Total Dis Loss : 1.0194667083851527e-06\n",
      "Steps : 435100, \t Total Gen Loss : 31.841453552246094, \t Total Dis Loss : 9.386145620737807e-07\n",
      "Steps : 435200, \t Total Gen Loss : 34.116859436035156, \t Total Dis Loss : 1.8376650245954806e-07\n",
      "Steps : 435300, \t Total Gen Loss : 38.514312744140625, \t Total Dis Loss : 7.409117870338378e-07\n",
      "Steps : 435400, \t Total Gen Loss : 35.65816879272461, \t Total Dis Loss : 8.212214197556023e-07\n",
      "Steps : 435500, \t Total Gen Loss : 34.7294921875, \t Total Dis Loss : 9.977089803214767e-07\n",
      "Steps : 435600, \t Total Gen Loss : 32.64125061035156, \t Total Dis Loss : 3.6871338693345024e-07\n",
      "Steps : 435700, \t Total Gen Loss : 37.02605438232422, \t Total Dis Loss : 1.9973549569840543e-06\n",
      "Steps : 435800, \t Total Gen Loss : 36.855735778808594, \t Total Dis Loss : 2.3097852874798264e-07\n",
      "Steps : 435900, \t Total Gen Loss : 34.94634246826172, \t Total Dis Loss : 2.554339971538866e-06\n",
      "Steps : 436000, \t Total Gen Loss : 33.4461784362793, \t Total Dis Loss : 1.4209219898475567e-06\n",
      "Steps : 436100, \t Total Gen Loss : 36.358917236328125, \t Total Dis Loss : 2.6188006359006977e-06\n",
      "Steps : 436200, \t Total Gen Loss : 30.9731388092041, \t Total Dis Loss : 8.714523005437513e-07\n",
      "Steps : 436300, \t Total Gen Loss : 33.28159713745117, \t Total Dis Loss : 1.0089955139847007e-05\n",
      "Steps : 436400, \t Total Gen Loss : 33.994300842285156, \t Total Dis Loss : 1.1604638530116063e-06\n",
      "Steps : 436500, \t Total Gen Loss : 32.698333740234375, \t Total Dis Loss : 2.1439825559355086e-06\n",
      "Steps : 436600, \t Total Gen Loss : 32.496246337890625, \t Total Dis Loss : 2.3756679183861706e-06\n",
      "Steps : 436700, \t Total Gen Loss : 31.718385696411133, \t Total Dis Loss : 2.7044366106565576e-06\n",
      "Steps : 436800, \t Total Gen Loss : 32.24098205566406, \t Total Dis Loss : 1.7530896911921445e-06\n",
      "Steps : 436900, \t Total Gen Loss : 31.892818450927734, \t Total Dis Loss : 2.2776307559979614e-06\n",
      "Steps : 437000, \t Total Gen Loss : 33.221343994140625, \t Total Dis Loss : 4.125400664634071e-06\n",
      "Steps : 437100, \t Total Gen Loss : 33.55805969238281, \t Total Dis Loss : 5.081953531771433e-06\n",
      "Steps : 437200, \t Total Gen Loss : 34.967445373535156, \t Total Dis Loss : 2.1594569261651486e-06\n",
      "Steps : 437300, \t Total Gen Loss : 32.95945739746094, \t Total Dis Loss : 1.573848606994943e-07\n",
      "Steps : 437400, \t Total Gen Loss : 29.402135848999023, \t Total Dis Loss : 3.0432038329308853e-05\n",
      "Steps : 437500, \t Total Gen Loss : 33.32881164550781, \t Total Dis Loss : 6.528299309138674e-06\n",
      "Steps : 437600, \t Total Gen Loss : 30.653764724731445, \t Total Dis Loss : 2.6733790946309455e-05\n",
      "Steps : 437700, \t Total Gen Loss : 32.98612976074219, \t Total Dis Loss : 1.2004054497083416e-06\n",
      "Steps : 437800, \t Total Gen Loss : 37.63390350341797, \t Total Dis Loss : 1.1517612392708543e-06\n",
      "Steps : 437900, \t Total Gen Loss : 41.943355560302734, \t Total Dis Loss : 1.209663605550304e-05\n",
      "Steps : 438000, \t Total Gen Loss : 39.35205841064453, \t Total Dis Loss : 1.035351033351617e-06\n",
      "Steps : 438100, \t Total Gen Loss : 38.685791015625, \t Total Dis Loss : 8.411942872044165e-06\n",
      "Steps : 438200, \t Total Gen Loss : 43.8851432800293, \t Total Dis Loss : 6.101556664361851e-07\n",
      "Steps : 438300, \t Total Gen Loss : 38.33289337158203, \t Total Dis Loss : 3.6518258639262058e-06\n",
      "Steps : 438400, \t Total Gen Loss : 39.96440124511719, \t Total Dis Loss : 1.4772640497540124e-05\n",
      "Steps : 438500, \t Total Gen Loss : 39.86845779418945, \t Total Dis Loss : 1.9828365111607127e-05\n",
      "Steps : 438600, \t Total Gen Loss : 37.073699951171875, \t Total Dis Loss : 2.556074321091728e-07\n",
      "Steps : 438700, \t Total Gen Loss : 43.443241119384766, \t Total Dis Loss : 3.3867283946165117e-06\n",
      "Time for epoch 78 is 69.75964879989624 sec\n",
      "Steps : 438800, \t Total Gen Loss : 44.39972686767578, \t Total Dis Loss : 6.105676675360883e-06\n",
      "Steps : 438900, \t Total Gen Loss : 43.00762176513672, \t Total Dis Loss : 1.5945245195325697e-06\n",
      "Steps : 439000, \t Total Gen Loss : 36.632606506347656, \t Total Dis Loss : 6.653965556324692e-07\n",
      "Steps : 439100, \t Total Gen Loss : 41.328529357910156, \t Total Dis Loss : 1.0396356628916692e-06\n",
      "Steps : 439200, \t Total Gen Loss : 39.928489685058594, \t Total Dis Loss : 5.234680429566652e-06\n",
      "Steps : 439300, \t Total Gen Loss : 42.12506866455078, \t Total Dis Loss : 3.174162884533871e-06\n",
      "Steps : 439400, \t Total Gen Loss : 38.51893615722656, \t Total Dis Loss : 1.0254182143398793e-06\n",
      "Steps : 439500, \t Total Gen Loss : 42.84195327758789, \t Total Dis Loss : 4.0585771785117686e-05\n",
      "Steps : 439600, \t Total Gen Loss : 37.37483596801758, \t Total Dis Loss : 6.136323918326525e-06\n",
      "Steps : 439700, \t Total Gen Loss : 35.93341064453125, \t Total Dis Loss : 7.406089252981474e-07\n",
      "Steps : 439800, \t Total Gen Loss : 36.3502082824707, \t Total Dis Loss : 2.837776264641434e-06\n",
      "Steps : 439900, \t Total Gen Loss : 39.51110076904297, \t Total Dis Loss : 7.402384767374315e-07\n",
      "Steps : 440000, \t Total Gen Loss : 41.99894714355469, \t Total Dis Loss : 7.980414920893963e-06\n",
      "Steps : 440100, \t Total Gen Loss : 39.52299499511719, \t Total Dis Loss : 1.0579313993730466e-06\n",
      "Steps : 440200, \t Total Gen Loss : 42.26472473144531, \t Total Dis Loss : 3.1893516734271543e-06\n",
      "Steps : 440300, \t Total Gen Loss : 37.24274826049805, \t Total Dis Loss : 7.943231139506679e-07\n",
      "Steps : 440400, \t Total Gen Loss : 40.59026336669922, \t Total Dis Loss : 1.4224977348931134e-05\n",
      "Steps : 440500, \t Total Gen Loss : 39.98234558105469, \t Total Dis Loss : 1.5524063201155514e-05\n",
      "Steps : 440600, \t Total Gen Loss : 35.80181884765625, \t Total Dis Loss : 1.8507564618630568e-06\n",
      "Steps : 440700, \t Total Gen Loss : 39.6773681640625, \t Total Dis Loss : 2.4481352056682226e-07\n",
      "Steps : 440800, \t Total Gen Loss : 35.164146423339844, \t Total Dis Loss : 1.1206282124476274e-06\n",
      "Steps : 440900, \t Total Gen Loss : 39.12049865722656, \t Total Dis Loss : 1.872852044471074e-05\n",
      "Steps : 441000, \t Total Gen Loss : 36.85026550292969, \t Total Dis Loss : 2.018878376475186e-06\n",
      "Steps : 441100, \t Total Gen Loss : 36.9769287109375, \t Total Dis Loss : 3.49539313049263e-08\n",
      "Steps : 441200, \t Total Gen Loss : 35.872676849365234, \t Total Dis Loss : 2.6017826115776188e-08\n",
      "Steps : 441300, \t Total Gen Loss : 36.41101837158203, \t Total Dis Loss : 5.107342104793133e-08\n",
      "Steps : 441400, \t Total Gen Loss : 36.31688690185547, \t Total Dis Loss : 1.0504236058750394e-07\n",
      "Steps : 441500, \t Total Gen Loss : 38.74144744873047, \t Total Dis Loss : 2.0810011847061105e-07\n",
      "Steps : 441600, \t Total Gen Loss : 36.54353332519531, \t Total Dis Loss : 2.2535265031820018e-07\n",
      "Steps : 441700, \t Total Gen Loss : 36.16671371459961, \t Total Dis Loss : 4.210422162032046e-07\n",
      "Steps : 441800, \t Total Gen Loss : 38.31629943847656, \t Total Dis Loss : 1.2762606615979166e-07\n",
      "Steps : 441900, \t Total Gen Loss : 38.51747131347656, \t Total Dis Loss : 7.098028618202079e-07\n",
      "Steps : 442000, \t Total Gen Loss : 41.51836395263672, \t Total Dis Loss : 1.3118419417423866e-07\n",
      "Steps : 442100, \t Total Gen Loss : 37.159202575683594, \t Total Dis Loss : 1.492922763191018e-07\n",
      "Steps : 442200, \t Total Gen Loss : 38.41054153442383, \t Total Dis Loss : 4.2476820283354755e-08\n",
      "Steps : 442300, \t Total Gen Loss : 36.023712158203125, \t Total Dis Loss : 3.964539700973546e-07\n",
      "Steps : 442400, \t Total Gen Loss : 34.86940383911133, \t Total Dis Loss : 9.110654985988731e-08\n",
      "Steps : 442500, \t Total Gen Loss : 38.729698181152344, \t Total Dis Loss : 1.8597299344946805e-07\n",
      "Steps : 442600, \t Total Gen Loss : 37.77058410644531, \t Total Dis Loss : 1.8218787545265513e-07\n",
      "Steps : 442700, \t Total Gen Loss : 37.49888229370117, \t Total Dis Loss : 1.1363373175754532e-07\n",
      "Steps : 442800, \t Total Gen Loss : 38.283912658691406, \t Total Dis Loss : 5.707553327738424e-07\n",
      "Steps : 442900, \t Total Gen Loss : 37.226951599121094, \t Total Dis Loss : 6.442250111149406e-08\n",
      "Steps : 443000, \t Total Gen Loss : 33.749488830566406, \t Total Dis Loss : 7.373153493972495e-06\n",
      "Steps : 443100, \t Total Gen Loss : 35.657371520996094, \t Total Dis Loss : 3.785519311350072e-07\n",
      "Steps : 443200, \t Total Gen Loss : 42.209957122802734, \t Total Dis Loss : 3.043954777126601e-08\n",
      "Steps : 443300, \t Total Gen Loss : 39.934417724609375, \t Total Dis Loss : 1.3593389667221345e-06\n",
      "Steps : 443400, \t Total Gen Loss : 39.5645751953125, \t Total Dis Loss : 5.435783805296523e-07\n",
      "Steps : 443500, \t Total Gen Loss : 37.459388732910156, \t Total Dis Loss : 4.116587888347567e-07\n",
      "Steps : 443600, \t Total Gen Loss : 41.038578033447266, \t Total Dis Loss : 1.6839899217302445e-06\n",
      "Steps : 443700, \t Total Gen Loss : 37.229774475097656, \t Total Dis Loss : 4.7573490746799507e-07\n",
      "Steps : 443800, \t Total Gen Loss : 39.61195755004883, \t Total Dis Loss : 1.8115006241714582e-05\n",
      "Steps : 443900, \t Total Gen Loss : 43.305908203125, \t Total Dis Loss : 3.203779499472148e-07\n",
      "Steps : 444000, \t Total Gen Loss : 36.445350646972656, \t Total Dis Loss : 2.185415496569476e-06\n",
      "Steps : 444100, \t Total Gen Loss : 38.55950927734375, \t Total Dis Loss : 2.7913043254557124e-07\n",
      "Steps : 444200, \t Total Gen Loss : 34.35515594482422, \t Total Dis Loss : 2.564152055128943e-07\n",
      "Steps : 444300, \t Total Gen Loss : 36.487789154052734, \t Total Dis Loss : 7.894954023868195e-07\n",
      "Time for epoch 79 is 77.82407879829407 sec\n",
      "Steps : 444400, \t Total Gen Loss : 36.13441467285156, \t Total Dis Loss : 7.568498290311254e-07\n",
      "Steps : 444500, \t Total Gen Loss : 38.6684455871582, \t Total Dis Loss : 4.145917955611367e-06\n",
      "Steps : 444600, \t Total Gen Loss : 38.84736633300781, \t Total Dis Loss : 0.09258471429347992\n",
      "Steps : 444700, \t Total Gen Loss : 32.40096664428711, \t Total Dis Loss : 1.5128629456739873e-05\n",
      "Steps : 444800, \t Total Gen Loss : 38.224327087402344, \t Total Dis Loss : 1.5318163093525072e-07\n",
      "Steps : 444900, \t Total Gen Loss : 32.2103271484375, \t Total Dis Loss : 4.783267286256887e-06\n",
      "Steps : 445000, \t Total Gen Loss : 36.14209747314453, \t Total Dis Loss : 6.439506705646636e-06\n",
      "Steps : 445100, \t Total Gen Loss : 34.67488098144531, \t Total Dis Loss : 0.00029613872175104916\n",
      "Steps : 445200, \t Total Gen Loss : 33.8935546875, \t Total Dis Loss : 4.155866918154061e-06\n",
      "Steps : 445300, \t Total Gen Loss : 34.63530731201172, \t Total Dis Loss : 7.05966249370249e-06\n",
      "Steps : 445400, \t Total Gen Loss : 34.59457778930664, \t Total Dis Loss : 1.1349828810125473e-06\n",
      "Steps : 445500, \t Total Gen Loss : 32.574310302734375, \t Total Dis Loss : 1.5883924788795412e-06\n",
      "Steps : 445600, \t Total Gen Loss : 36.41588592529297, \t Total Dis Loss : 1.4569935729014105e-06\n",
      "Steps : 445700, \t Total Gen Loss : 32.532230377197266, \t Total Dis Loss : 2.1315993308235193e-06\n",
      "Steps : 445800, \t Total Gen Loss : 31.876441955566406, \t Total Dis Loss : 3.3037322282325476e-06\n",
      "Steps : 445900, \t Total Gen Loss : 34.03712844848633, \t Total Dis Loss : 4.0384243220614735e-06\n",
      "Steps : 446000, \t Total Gen Loss : 32.24959182739258, \t Total Dis Loss : 9.387739510202664e-07\n",
      "Steps : 446100, \t Total Gen Loss : 34.53545379638672, \t Total Dis Loss : 3.504475216686842e-06\n",
      "Steps : 446200, \t Total Gen Loss : 36.41315841674805, \t Total Dis Loss : 2.4082739855657564e-06\n",
      "Steps : 446300, \t Total Gen Loss : 33.766334533691406, \t Total Dis Loss : 3.3639248613326345e-06\n",
      "Steps : 446400, \t Total Gen Loss : 37.097991943359375, \t Total Dis Loss : 2.2191263724380406e-06\n",
      "Steps : 446500, \t Total Gen Loss : 39.792049407958984, \t Total Dis Loss : 2.802009112201631e-05\n",
      "Steps : 446600, \t Total Gen Loss : 43.26148986816406, \t Total Dis Loss : 7.609426575072575e-06\n",
      "Steps : 446700, \t Total Gen Loss : 40.46400451660156, \t Total Dis Loss : 6.434828719648067e-06\n",
      "Steps : 446800, \t Total Gen Loss : 41.191524505615234, \t Total Dis Loss : 2.6797888494911604e-06\n",
      "Steps : 446900, \t Total Gen Loss : 41.03453826904297, \t Total Dis Loss : 2.006089971473557e-06\n",
      "Steps : 447000, \t Total Gen Loss : 41.94953918457031, \t Total Dis Loss : 5.636765308736358e-06\n",
      "Steps : 447100, \t Total Gen Loss : 35.68669128417969, \t Total Dis Loss : 1.606630206651971e-07\n",
      "Steps : 447200, \t Total Gen Loss : 40.63182830810547, \t Total Dis Loss : 1.612760485159015e-07\n",
      "Steps : 447300, \t Total Gen Loss : 36.694854736328125, \t Total Dis Loss : 1.5667433217458893e-07\n",
      "Steps : 447400, \t Total Gen Loss : 38.36236572265625, \t Total Dis Loss : 2.305302814420429e-06\n",
      "Steps : 447500, \t Total Gen Loss : 30.24146270751953, \t Total Dis Loss : 0.00015213234291877598\n",
      "Steps : 447600, \t Total Gen Loss : 33.55021286010742, \t Total Dis Loss : 7.753151294309646e-06\n",
      "Steps : 447700, \t Total Gen Loss : 34.37108612060547, \t Total Dis Loss : 5.9674734984582756e-06\n",
      "Steps : 447800, \t Total Gen Loss : 35.786842346191406, \t Total Dis Loss : 2.0171014512015972e-06\n",
      "Steps : 447900, \t Total Gen Loss : 34.16243362426758, \t Total Dis Loss : 6.238306468731025e-06\n",
      "Steps : 448000, \t Total Gen Loss : 35.96541976928711, \t Total Dis Loss : 1.7203938114107586e-06\n",
      "Steps : 448100, \t Total Gen Loss : 39.038246154785156, \t Total Dis Loss : 3.566786404007871e-07\n",
      "Steps : 448200, \t Total Gen Loss : 36.38285446166992, \t Total Dis Loss : 3.3572959523553436e-08\n",
      "Steps : 448300, \t Total Gen Loss : 36.40925216674805, \t Total Dis Loss : 6.507449938908394e-07\n",
      "Steps : 448400, \t Total Gen Loss : 39.943328857421875, \t Total Dis Loss : 1.7589246681382065e-07\n",
      "Steps : 448500, \t Total Gen Loss : 34.559505462646484, \t Total Dis Loss : 2.390045210631797e-06\n",
      "Steps : 448600, \t Total Gen Loss : 35.04054641723633, \t Total Dis Loss : 3.814721367234597e-06\n",
      "Steps : 448700, \t Total Gen Loss : 37.881385803222656, \t Total Dis Loss : 2.1625646695611067e-06\n",
      "Steps : 448800, \t Total Gen Loss : 37.007137298583984, \t Total Dis Loss : 3.7784238884341903e-06\n",
      "Steps : 448900, \t Total Gen Loss : 40.06615447998047, \t Total Dis Loss : 6.79287695675157e-05\n",
      "Steps : 449000, \t Total Gen Loss : 40.44172668457031, \t Total Dis Loss : 2.1526591353904223e-06\n",
      "Steps : 449100, \t Total Gen Loss : 38.69247055053711, \t Total Dis Loss : 5.443073973765422e-07\n",
      "Steps : 449200, \t Total Gen Loss : 35.666969299316406, \t Total Dis Loss : 0.007810900919139385\n",
      "Steps : 449300, \t Total Gen Loss : 38.53175735473633, \t Total Dis Loss : 4.2524367017904297e-05\n",
      "Steps : 449400, \t Total Gen Loss : 36.333160400390625, \t Total Dis Loss : 1.4536788057739614e-06\n",
      "Steps : 449500, \t Total Gen Loss : 37.67236328125, \t Total Dis Loss : 4.771557016169936e-08\n",
      "Steps : 449600, \t Total Gen Loss : 37.387168884277344, \t Total Dis Loss : 3.235773249343765e-07\n",
      "Steps : 449700, \t Total Gen Loss : 35.535438537597656, \t Total Dis Loss : 1.1680735951813404e-06\n",
      "Steps : 449800, \t Total Gen Loss : 37.81999969482422, \t Total Dis Loss : 9.135882237387705e-07\n",
      "Steps : 449900, \t Total Gen Loss : 35.039390563964844, \t Total Dis Loss : 1.2673876881308388e-06\n",
      "Steps : 450000, \t Total Gen Loss : 34.500152587890625, \t Total Dis Loss : 8.405091875829385e-07\n",
      "Time for epoch 80 is 71.65310311317444 sec\n",
      "Steps : 450100, \t Total Gen Loss : 36.18413162231445, \t Total Dis Loss : 3.4152262173847703e-07\n",
      "Steps : 450200, \t Total Gen Loss : 35.77461242675781, \t Total Dis Loss : 1.5855124502195395e-06\n",
      "Steps : 450300, \t Total Gen Loss : 38.905967712402344, \t Total Dis Loss : 1.4007251820657984e-06\n",
      "Steps : 450400, \t Total Gen Loss : 39.55377197265625, \t Total Dis Loss : 2.196770537921111e-06\n",
      "Steps : 450500, \t Total Gen Loss : 40.205753326416016, \t Total Dis Loss : 7.211691013253585e-07\n",
      "Steps : 450600, \t Total Gen Loss : 32.96538543701172, \t Total Dis Loss : 7.037652721919585e-06\n",
      "Steps : 450700, \t Total Gen Loss : 36.53856658935547, \t Total Dis Loss : 6.536575938298483e-07\n",
      "Steps : 450800, \t Total Gen Loss : 30.60611343383789, \t Total Dis Loss : 1.9042166968574747e-05\n",
      "Steps : 450900, \t Total Gen Loss : 28.283143997192383, \t Total Dis Loss : 0.00011531600466696545\n",
      "Steps : 451000, \t Total Gen Loss : 31.388870239257812, \t Total Dis Loss : 2.3420299839926884e-05\n",
      "Steps : 451100, \t Total Gen Loss : 33.35371017456055, \t Total Dis Loss : 3.900530282407999e-05\n",
      "Steps : 451200, \t Total Gen Loss : 36.01025390625, \t Total Dis Loss : 6.181494995871617e-07\n",
      "Steps : 451300, \t Total Gen Loss : 34.64785385131836, \t Total Dis Loss : 9.139503731603327e-07\n",
      "Steps : 451400, \t Total Gen Loss : 38.62178039550781, \t Total Dis Loss : 4.232958588090696e-07\n",
      "Steps : 451500, \t Total Gen Loss : 36.96565246582031, \t Total Dis Loss : 2.2307051494863117e-07\n",
      "Steps : 451600, \t Total Gen Loss : 33.47607421875, \t Total Dis Loss : 1.8264157688463456e-06\n",
      "Steps : 451700, \t Total Gen Loss : 36.4449462890625, \t Total Dis Loss : 2.1545486106333556e-07\n",
      "Steps : 451800, \t Total Gen Loss : 34.86370849609375, \t Total Dis Loss : 4.474308752833167e-06\n",
      "Steps : 451900, \t Total Gen Loss : 38.30919647216797, \t Total Dis Loss : 2.1767446867215767e-07\n",
      "Steps : 452000, \t Total Gen Loss : 35.3518180847168, \t Total Dis Loss : 1.004994828690542e-06\n",
      "Steps : 452100, \t Total Gen Loss : 38.425575256347656, \t Total Dis Loss : 2.475610472174594e-07\n",
      "Steps : 452200, \t Total Gen Loss : 36.39614486694336, \t Total Dis Loss : 1.0885057832865641e-07\n",
      "Steps : 452300, \t Total Gen Loss : 36.80875778198242, \t Total Dis Loss : 1.4835492834208708e-07\n",
      "Steps : 452400, \t Total Gen Loss : 34.43838882446289, \t Total Dis Loss : 1.1414020661959512e-07\n",
      "Steps : 452500, \t Total Gen Loss : 35.727622985839844, \t Total Dis Loss : 1.0741539426817326e-06\n",
      "Steps : 452600, \t Total Gen Loss : 33.76085662841797, \t Total Dis Loss : 3.401757169285702e-07\n",
      "Steps : 452700, \t Total Gen Loss : 38.96199035644531, \t Total Dis Loss : 6.991994894178788e-08\n",
      "Steps : 452800, \t Total Gen Loss : 37.12667465209961, \t Total Dis Loss : 8.111447158398732e-08\n",
      "Steps : 452900, \t Total Gen Loss : 33.755741119384766, \t Total Dis Loss : 1.749570515130472e-06\n",
      "Steps : 453000, \t Total Gen Loss : 37.22991943359375, \t Total Dis Loss : 2.5351792487526836e-07\n",
      "Steps : 453100, \t Total Gen Loss : 37.567909240722656, \t Total Dis Loss : 1.2082814464520197e-06\n",
      "Steps : 453200, \t Total Gen Loss : 37.41394805908203, \t Total Dis Loss : 1.8817661384673556e-06\n",
      "Steps : 453300, \t Total Gen Loss : 38.61669158935547, \t Total Dis Loss : 7.2007077278613e-07\n",
      "Steps : 453400, \t Total Gen Loss : 38.57008361816406, \t Total Dis Loss : 1.005221861305472e-06\n",
      "Steps : 453500, \t Total Gen Loss : 38.293601989746094, \t Total Dis Loss : 1.4497203437713324e-07\n",
      "Steps : 453600, \t Total Gen Loss : 38.35112380981445, \t Total Dis Loss : 1.4000277133163763e-06\n",
      "Steps : 453700, \t Total Gen Loss : 35.19901657104492, \t Total Dis Loss : 1.3252270036900882e-07\n",
      "Steps : 453800, \t Total Gen Loss : 37.585994720458984, \t Total Dis Loss : 2.350616796320537e-06\n",
      "Steps : 453900, \t Total Gen Loss : 37.630760192871094, \t Total Dis Loss : 8.539550435671117e-07\n",
      "Steps : 454000, \t Total Gen Loss : 38.17497253417969, \t Total Dis Loss : 3.9557528452860424e-07\n",
      "Steps : 454100, \t Total Gen Loss : 36.355953216552734, \t Total Dis Loss : 4.646350930670451e-07\n",
      "Steps : 454200, \t Total Gen Loss : 39.266387939453125, \t Total Dis Loss : 5.226703478911077e-07\n",
      "Steps : 454300, \t Total Gen Loss : 37.63566207885742, \t Total Dis Loss : 5.337071797839599e-07\n",
      "Steps : 454400, \t Total Gen Loss : 35.90480422973633, \t Total Dis Loss : 1.7888756929096417e-06\n",
      "Steps : 454500, \t Total Gen Loss : 37.319358825683594, \t Total Dis Loss : 1.0690217777664657e-06\n",
      "Steps : 454600, \t Total Gen Loss : 38.77739334106445, \t Total Dis Loss : 8.602887646702584e-07\n",
      "Steps : 454700, \t Total Gen Loss : 36.272926330566406, \t Total Dis Loss : 3.285946661435446e-07\n",
      "Steps : 454800, \t Total Gen Loss : 38.703887939453125, \t Total Dis Loss : 1.0003416406334509e-07\n",
      "Steps : 454900, \t Total Gen Loss : 35.755767822265625, \t Total Dis Loss : 1.8852402305924443e-08\n",
      "Steps : 455000, \t Total Gen Loss : 35.928714752197266, \t Total Dis Loss : 3.3822709610831225e-07\n",
      "Steps : 455100, \t Total Gen Loss : 36.1661376953125, \t Total Dis Loss : 1.3651934978042846e-06\n",
      "Steps : 455200, \t Total Gen Loss : 36.53791427612305, \t Total Dis Loss : 2.0058203631379e-07\n",
      "Steps : 455300, \t Total Gen Loss : 37.827396392822266, \t Total Dis Loss : 9.913019312079996e-07\n",
      "Steps : 455400, \t Total Gen Loss : 35.58061981201172, \t Total Dis Loss : 7.004560984569252e-07\n",
      "Steps : 455500, \t Total Gen Loss : 36.872501373291016, \t Total Dis Loss : 3.1615311257837675e-08\n",
      "Steps : 455600, \t Total Gen Loss : 38.236175537109375, \t Total Dis Loss : 1.5703061251315376e-07\n",
      "Time for epoch 81 is 69.453693151474 sec\n",
      "Steps : 455700, \t Total Gen Loss : 35.36273193359375, \t Total Dis Loss : 3.082009527588525e-07\n",
      "Steps : 455800, \t Total Gen Loss : 36.19059753417969, \t Total Dis Loss : 5.161114131624345e-07\n",
      "Steps : 455900, \t Total Gen Loss : 35.42898178100586, \t Total Dis Loss : 1.2284827732855774e-07\n",
      "Steps : 456000, \t Total Gen Loss : 34.50153350830078, \t Total Dis Loss : 4.002283162662934e-07\n",
      "Steps : 456100, \t Total Gen Loss : 34.65444564819336, \t Total Dis Loss : 1.1758274922613055e-06\n",
      "Steps : 456200, \t Total Gen Loss : 39.510284423828125, \t Total Dis Loss : 3.593151632230729e-06\n",
      "Steps : 456300, \t Total Gen Loss : 40.35166931152344, \t Total Dis Loss : 7.443280765073723e-07\n",
      "Steps : 456400, \t Total Gen Loss : 36.208412170410156, \t Total Dis Loss : 4.753779947463954e-08\n",
      "Steps : 456500, \t Total Gen Loss : 34.79417419433594, \t Total Dis Loss : 8.853738791003707e-07\n",
      "Steps : 456600, \t Total Gen Loss : 40.03166198730469, \t Total Dis Loss : 1.516803649792564e-06\n",
      "Steps : 456700, \t Total Gen Loss : 36.86069107055664, \t Total Dis Loss : 3.445527596568354e-08\n",
      "Steps : 456800, \t Total Gen Loss : 38.818302154541016, \t Total Dis Loss : 4.1771764358600194e-07\n",
      "Steps : 456900, \t Total Gen Loss : 33.89119338989258, \t Total Dis Loss : 1.050162154569989e-05\n",
      "Steps : 457000, \t Total Gen Loss : 36.56697463989258, \t Total Dis Loss : 3.42580960932537e-06\n",
      "Steps : 457100, \t Total Gen Loss : 38.42129135131836, \t Total Dis Loss : 1.8716987142397556e-06\n",
      "Steps : 457200, \t Total Gen Loss : 40.35264205932617, \t Total Dis Loss : 1.00963643490104e-06\n",
      "Steps : 457300, \t Total Gen Loss : 44.18238067626953, \t Total Dis Loss : 6.737384978805494e-07\n",
      "Steps : 457400, \t Total Gen Loss : 41.36311340332031, \t Total Dis Loss : 1.0047549039882142e-05\n",
      "Steps : 457500, \t Total Gen Loss : 40.40538024902344, \t Total Dis Loss : 4.649038146453677e-06\n",
      "Steps : 457600, \t Total Gen Loss : 39.27655029296875, \t Total Dis Loss : 1.5917249740482475e-08\n",
      "Steps : 457700, \t Total Gen Loss : 39.978153228759766, \t Total Dis Loss : 0.0002038422244368121\n",
      "Steps : 457800, \t Total Gen Loss : 38.1225700378418, \t Total Dis Loss : 1.4589514485408017e-08\n",
      "Steps : 457900, \t Total Gen Loss : 37.50294494628906, \t Total Dis Loss : 3.73502160755379e-07\n",
      "Steps : 458000, \t Total Gen Loss : 32.90376281738281, \t Total Dis Loss : 6.8682220444316044e-06\n",
      "Steps : 458100, \t Total Gen Loss : 34.2529296875, \t Total Dis Loss : 5.562069418374449e-07\n",
      "Steps : 458200, \t Total Gen Loss : 36.13090515136719, \t Total Dis Loss : 1.8049721575152944e-06\n",
      "Steps : 458300, \t Total Gen Loss : 35.91343688964844, \t Total Dis Loss : 6.779394880140899e-06\n",
      "Steps : 458400, \t Total Gen Loss : 36.23134994506836, \t Total Dis Loss : 1.1126462595711928e-06\n",
      "Steps : 458500, \t Total Gen Loss : 36.270362854003906, \t Total Dis Loss : 1.3407081951299915e-06\n",
      "Steps : 458600, \t Total Gen Loss : 38.20514678955078, \t Total Dis Loss : 2.749406121438369e-06\n",
      "Steps : 458700, \t Total Gen Loss : 36.808414459228516, \t Total Dis Loss : 1.0118331374542322e-05\n",
      "Steps : 458800, \t Total Gen Loss : 39.395225524902344, \t Total Dis Loss : 3.1294262043957133e-06\n",
      "Steps : 458900, \t Total Gen Loss : 34.74634552001953, \t Total Dis Loss : 2.8416654913598904e-06\n",
      "Steps : 459000, \t Total Gen Loss : 40.836856842041016, \t Total Dis Loss : 2.773879714368377e-06\n",
      "Steps : 459100, \t Total Gen Loss : 36.44108963012695, \t Total Dis Loss : 4.301530680095311e-06\n",
      "Steps : 459200, \t Total Gen Loss : 34.91795349121094, \t Total Dis Loss : 3.4147042242693715e-06\n",
      "Steps : 459300, \t Total Gen Loss : 34.98273468017578, \t Total Dis Loss : 2.8214455596753396e-05\n",
      "Steps : 459400, \t Total Gen Loss : 39.14527130126953, \t Total Dis Loss : 1.1340648597979452e-06\n",
      "Steps : 459500, \t Total Gen Loss : 36.7294807434082, \t Total Dis Loss : 2.1368216494010994e-06\n",
      "Steps : 459600, \t Total Gen Loss : 35.69586944580078, \t Total Dis Loss : 7.5627722253557295e-06\n",
      "Steps : 459700, \t Total Gen Loss : 37.49915313720703, \t Total Dis Loss : 8.120407528622309e-07\n",
      "Steps : 459800, \t Total Gen Loss : 35.95243835449219, \t Total Dis Loss : 2.300525238752016e-06\n",
      "Steps : 459900, \t Total Gen Loss : 35.67536544799805, \t Total Dis Loss : 4.164558959018905e-06\n",
      "Steps : 460000, \t Total Gen Loss : 31.14527130126953, \t Total Dis Loss : 0.006059224251657724\n",
      "Steps : 460100, \t Total Gen Loss : 32.08653259277344, \t Total Dis Loss : 5.325231086317217e-06\n",
      "Steps : 460200, \t Total Gen Loss : 39.33608627319336, \t Total Dis Loss : 1.1083024986646706e-07\n",
      "Steps : 460300, \t Total Gen Loss : 32.52804946899414, \t Total Dis Loss : 5.785535108771e-07\n",
      "Steps : 460400, \t Total Gen Loss : 36.24787521362305, \t Total Dis Loss : 8.419248842983507e-06\n",
      "Steps : 460500, \t Total Gen Loss : 33.25197219848633, \t Total Dis Loss : 1.7237136944459053e-06\n",
      "Steps : 460600, \t Total Gen Loss : 34.025264739990234, \t Total Dis Loss : 3.828899025393184e-06\n",
      "Steps : 460700, \t Total Gen Loss : 35.30270767211914, \t Total Dis Loss : 4.042359250888694e-06\n",
      "Steps : 460800, \t Total Gen Loss : 35.67361831665039, \t Total Dis Loss : 1.0384543713826133e-07\n",
      "Steps : 460900, \t Total Gen Loss : 34.4405517578125, \t Total Dis Loss : 6.810247441535466e-07\n",
      "Steps : 461000, \t Total Gen Loss : 33.36529541015625, \t Total Dis Loss : 6.745489713466668e-07\n",
      "Steps : 461100, \t Total Gen Loss : 35.802635192871094, \t Total Dis Loss : 2.8906792977068108e-06\n",
      "Steps : 461200, \t Total Gen Loss : 38.65961456298828, \t Total Dis Loss : 7.205171641544439e-07\n",
      "Time for epoch 82 is 69.45451426506042 sec\n",
      "Steps : 461300, \t Total Gen Loss : 33.03173065185547, \t Total Dis Loss : 1.282542996250413e-07\n",
      "Steps : 461400, \t Total Gen Loss : 36.89516067504883, \t Total Dis Loss : 1.8687077272261376e-06\n",
      "Steps : 461500, \t Total Gen Loss : 33.55251693725586, \t Total Dis Loss : 2.6411244107293896e-05\n",
      "Steps : 461600, \t Total Gen Loss : 31.54889678955078, \t Total Dis Loss : 1.7673708498477936e-05\n",
      "Steps : 461700, \t Total Gen Loss : 41.24258804321289, \t Total Dis Loss : 7.498107947867538e-07\n",
      "Steps : 461800, \t Total Gen Loss : 32.41350173950195, \t Total Dis Loss : 3.306832923044567e-06\n",
      "Steps : 461900, \t Total Gen Loss : 34.34061050415039, \t Total Dis Loss : 3.4150218652939657e-07\n",
      "Steps : 462000, \t Total Gen Loss : 33.506526947021484, \t Total Dis Loss : 1.0469676681168494e-06\n",
      "Steps : 462100, \t Total Gen Loss : 36.02415084838867, \t Total Dis Loss : 8.952548569141072e-07\n",
      "Steps : 462200, \t Total Gen Loss : 37.20441436767578, \t Total Dis Loss : 3.674883373605553e-06\n",
      "Steps : 462300, \t Total Gen Loss : 36.36334991455078, \t Total Dis Loss : 8.486707514521186e-08\n",
      "Steps : 462400, \t Total Gen Loss : 36.03511428833008, \t Total Dis Loss : 3.3646321639935195e-07\n",
      "Steps : 462500, \t Total Gen Loss : 33.89295959472656, \t Total Dis Loss : 1.5856170421102433e-06\n",
      "Steps : 462600, \t Total Gen Loss : 32.997474670410156, \t Total Dis Loss : 5.57630028197309e-06\n",
      "Steps : 462700, \t Total Gen Loss : 33.428985595703125, \t Total Dis Loss : 3.941407157981303e-06\n",
      "Steps : 462800, \t Total Gen Loss : 33.969364166259766, \t Total Dis Loss : 2.7811679501610342e-06\n",
      "Steps : 462900, \t Total Gen Loss : 33.09138488769531, \t Total Dis Loss : 3.663635652628727e-05\n",
      "Steps : 463000, \t Total Gen Loss : 33.529319763183594, \t Total Dis Loss : 1.3696532732865307e-05\n",
      "Steps : 463100, \t Total Gen Loss : 32.92481994628906, \t Total Dis Loss : 1.3779403161606751e-05\n",
      "Steps : 463200, \t Total Gen Loss : 30.036012649536133, \t Total Dis Loss : 4.0154741327569354e-06\n",
      "Steps : 463300, \t Total Gen Loss : 35.435707092285156, \t Total Dis Loss : 6.42177326426463e-07\n",
      "Steps : 463400, \t Total Gen Loss : 36.17012023925781, \t Total Dis Loss : 3.0966882604843704e-06\n",
      "Steps : 463500, \t Total Gen Loss : 37.03559875488281, \t Total Dis Loss : 2.5854466002783738e-05\n",
      "Steps : 463600, \t Total Gen Loss : 34.95995330810547, \t Total Dis Loss : 2.8286426640988793e-06\n",
      "Steps : 463700, \t Total Gen Loss : 35.544395446777344, \t Total Dis Loss : 1.3158154388293042e-06\n",
      "Steps : 463800, \t Total Gen Loss : 37.1031379699707, \t Total Dis Loss : 3.363508085385547e-06\n",
      "Steps : 463900, \t Total Gen Loss : 36.17081069946289, \t Total Dis Loss : 4.6207132982090116e-06\n",
      "Steps : 464000, \t Total Gen Loss : 40.06336212158203, \t Total Dis Loss : 3.894086148648057e-06\n",
      "Steps : 464100, \t Total Gen Loss : 37.52867126464844, \t Total Dis Loss : 5.586382485489594e-07\n",
      "Steps : 464200, \t Total Gen Loss : 38.49335479736328, \t Total Dis Loss : 2.1085606931592338e-06\n",
      "Steps : 464300, \t Total Gen Loss : 33.04962158203125, \t Total Dis Loss : 6.775881956855301e-06\n",
      "Steps : 464400, \t Total Gen Loss : 36.651092529296875, \t Total Dis Loss : 8.531331332051195e-06\n",
      "Steps : 464500, \t Total Gen Loss : 34.80183029174805, \t Total Dis Loss : 3.3936575505322253e-07\n",
      "Steps : 464600, \t Total Gen Loss : 36.3234748840332, \t Total Dis Loss : 1.4255019777920097e-06\n",
      "Steps : 464700, \t Total Gen Loss : 31.69148826599121, \t Total Dis Loss : 1.766285549820168e-06\n",
      "Steps : 464800, \t Total Gen Loss : 34.37860870361328, \t Total Dis Loss : 3.550690507836407e-06\n",
      "Steps : 464900, \t Total Gen Loss : 37.11985778808594, \t Total Dis Loss : 3.108211330982158e-06\n",
      "Steps : 465000, \t Total Gen Loss : 36.76482391357422, \t Total Dis Loss : 8.376052278435964e-07\n",
      "Steps : 465100, \t Total Gen Loss : 35.008052825927734, \t Total Dis Loss : 1.775257914005124e-07\n",
      "Steps : 465200, \t Total Gen Loss : 38.8691520690918, \t Total Dis Loss : 2.483172067968553e-07\n",
      "Steps : 465300, \t Total Gen Loss : 37.640533447265625, \t Total Dis Loss : 6.343532845676236e-07\n",
      "Steps : 465400, \t Total Gen Loss : 35.321929931640625, \t Total Dis Loss : 2.1591270069620805e-06\n",
      "Steps : 465500, \t Total Gen Loss : 33.706573486328125, \t Total Dis Loss : 1.7837493260230985e-07\n",
      "Steps : 465600, \t Total Gen Loss : 34.82118225097656, \t Total Dis Loss : 2.232039150840137e-06\n",
      "Steps : 465700, \t Total Gen Loss : 31.76644515991211, \t Total Dis Loss : 1.8689172520680586e-07\n",
      "Steps : 465800, \t Total Gen Loss : 32.863929748535156, \t Total Dis Loss : 5.691454703082854e-07\n",
      "Steps : 465900, \t Total Gen Loss : 33.02446746826172, \t Total Dis Loss : 4.599986880293727e-07\n",
      "Steps : 466000, \t Total Gen Loss : 32.361671447753906, \t Total Dis Loss : 4.113890099688433e-07\n",
      "Steps : 466100, \t Total Gen Loss : 35.2637825012207, \t Total Dis Loss : 5.310794222168624e-07\n",
      "Steps : 466200, \t Total Gen Loss : 37.01152038574219, \t Total Dis Loss : 1.7215147636306938e-06\n",
      "Steps : 466300, \t Total Gen Loss : 34.38128662109375, \t Total Dis Loss : 1.209383526656893e-07\n",
      "Steps : 466400, \t Total Gen Loss : 36.408199310302734, \t Total Dis Loss : 3.6060799857295933e-07\n",
      "Steps : 466500, \t Total Gen Loss : 34.824867248535156, \t Total Dis Loss : 1.2678035545832245e-06\n",
      "Steps : 466600, \t Total Gen Loss : 37.1541862487793, \t Total Dis Loss : 1.0366879905632231e-06\n",
      "Steps : 466700, \t Total Gen Loss : 35.0224609375, \t Total Dis Loss : 1.4180651533024502e-06\n",
      "Steps : 466800, \t Total Gen Loss : 34.23088836669922, \t Total Dis Loss : 1.1703768620918709e-07\n",
      "Time for epoch 83 is 71.15964913368225 sec\n",
      "Steps : 466900, \t Total Gen Loss : 36.26864242553711, \t Total Dis Loss : 7.514498747696052e-07\n",
      "Steps : 467000, \t Total Gen Loss : 33.220977783203125, \t Total Dis Loss : 2.1195753561187303e-06\n",
      "Steps : 467100, \t Total Gen Loss : 36.716712951660156, \t Total Dis Loss : 2.561665496614296e-06\n",
      "Steps : 467200, \t Total Gen Loss : 33.62544250488281, \t Total Dis Loss : 3.242709226469742e-06\n",
      "Steps : 467300, \t Total Gen Loss : 36.3148078918457, \t Total Dis Loss : 2.0386241885717027e-06\n",
      "Steps : 467400, \t Total Gen Loss : 35.968421936035156, \t Total Dis Loss : 1.7936386029759888e-06\n",
      "Steps : 467500, \t Total Gen Loss : 37.2164306640625, \t Total Dis Loss : 1.968890501302667e-06\n",
      "Steps : 467600, \t Total Gen Loss : 34.49201583862305, \t Total Dis Loss : 3.6418928175407927e-06\n",
      "Steps : 467700, \t Total Gen Loss : 39.67368698120117, \t Total Dis Loss : 1.382283798534445e-08\n",
      "Steps : 467800, \t Total Gen Loss : 41.165645599365234, \t Total Dis Loss : 4.2463166209927294e-06\n",
      "Steps : 467900, \t Total Gen Loss : 40.05438995361328, \t Total Dis Loss : 7.170928739697047e-08\n",
      "Steps : 468000, \t Total Gen Loss : 40.850608825683594, \t Total Dis Loss : 1.5711520973127335e-07\n",
      "Steps : 468100, \t Total Gen Loss : 32.68953323364258, \t Total Dis Loss : 3.867088253173279e-06\n",
      "Steps : 468200, \t Total Gen Loss : 32.55484390258789, \t Total Dis Loss : 1.5685682228649966e-05\n",
      "Steps : 468300, \t Total Gen Loss : 31.077064514160156, \t Total Dis Loss : 1.3408408449322451e-05\n",
      "Steps : 468400, \t Total Gen Loss : 32.52833557128906, \t Total Dis Loss : 4.585934220813215e-06\n",
      "Steps : 468500, \t Total Gen Loss : 33.05470275878906, \t Total Dis Loss : 4.875361810263712e-06\n",
      "Steps : 468600, \t Total Gen Loss : 35.617835998535156, \t Total Dis Loss : 9.161980415228754e-06\n",
      "Steps : 468700, \t Total Gen Loss : 34.549774169921875, \t Total Dis Loss : 8.387952220800798e-06\n",
      "Steps : 468800, \t Total Gen Loss : 33.31953430175781, \t Total Dis Loss : 1.5002608051872812e-05\n",
      "Steps : 468900, \t Total Gen Loss : 32.54045867919922, \t Total Dis Loss : 2.048639908025507e-05\n",
      "Steps : 469000, \t Total Gen Loss : 35.22075271606445, \t Total Dis Loss : 7.239357273647329e-06\n",
      "Steps : 469100, \t Total Gen Loss : 34.77157974243164, \t Total Dis Loss : 9.053997928276658e-06\n",
      "Steps : 469200, \t Total Gen Loss : 35.53498077392578, \t Total Dis Loss : 1.7797683540266007e-06\n",
      "Steps : 469300, \t Total Gen Loss : 34.41141128540039, \t Total Dis Loss : 2.8842188726230233e-07\n",
      "Steps : 469400, \t Total Gen Loss : 33.24822998046875, \t Total Dis Loss : 3.345122365772113e-07\n",
      "Steps : 469500, \t Total Gen Loss : 32.32149124145508, \t Total Dis Loss : 5.212553787714569e-06\n",
      "Steps : 469600, \t Total Gen Loss : 34.647743225097656, \t Total Dis Loss : 3.3807341424108017e-06\n",
      "Steps : 469700, \t Total Gen Loss : 36.31348419189453, \t Total Dis Loss : 3.488435709186888e-07\n",
      "Steps : 469800, \t Total Gen Loss : 34.95306396484375, \t Total Dis Loss : 3.3739743230398744e-06\n",
      "Steps : 469900, \t Total Gen Loss : 33.57126998901367, \t Total Dis Loss : 2.2727906525688013e-06\n",
      "Steps : 470000, \t Total Gen Loss : 33.10698699951172, \t Total Dis Loss : 2.374826863160706e-06\n",
      "Steps : 470100, \t Total Gen Loss : 39.87940216064453, \t Total Dis Loss : 5.904050226490654e-07\n",
      "Steps : 470200, \t Total Gen Loss : 37.87021255493164, \t Total Dis Loss : 1.6140886316406977e-07\n",
      "Steps : 470300, \t Total Gen Loss : 39.16181182861328, \t Total Dis Loss : 6.022666099170237e-08\n",
      "Steps : 470400, \t Total Gen Loss : 39.173828125, \t Total Dis Loss : 3.0758837255007165e-08\n",
      "Steps : 470500, \t Total Gen Loss : 36.85345458984375, \t Total Dis Loss : 2.0783264176316152e-07\n",
      "Steps : 470600, \t Total Gen Loss : 36.95692443847656, \t Total Dis Loss : 8.135786089269459e-08\n",
      "Steps : 470700, \t Total Gen Loss : 37.222572326660156, \t Total Dis Loss : 3.7556850429609767e-07\n",
      "Steps : 470800, \t Total Gen Loss : 35.044349670410156, \t Total Dis Loss : 3.1508236020272307e-08\n",
      "Steps : 470900, \t Total Gen Loss : 37.60385513305664, \t Total Dis Loss : 6.319850172076258e-07\n",
      "Steps : 471000, \t Total Gen Loss : 38.86418533325195, \t Total Dis Loss : 1.4077207310947415e-07\n",
      "Steps : 471100, \t Total Gen Loss : 38.17503356933594, \t Total Dis Loss : 1.4087515864957822e-06\n",
      "Steps : 471200, \t Total Gen Loss : 36.26385498046875, \t Total Dis Loss : 1.8598325368657243e-07\n",
      "Steps : 471300, \t Total Gen Loss : 37.807884216308594, \t Total Dis Loss : 2.143862332104618e-08\n",
      "Steps : 471400, \t Total Gen Loss : 38.615474700927734, \t Total Dis Loss : 2.802516689826007e-07\n",
      "Steps : 471500, \t Total Gen Loss : 36.19636154174805, \t Total Dis Loss : 3.834924768852943e-07\n",
      "Steps : 471600, \t Total Gen Loss : 32.51955032348633, \t Total Dis Loss : 3.1510478493146366e-07\n",
      "Steps : 471700, \t Total Gen Loss : 38.38393783569336, \t Total Dis Loss : 3.1238480005413294e-05\n",
      "Steps : 471800, \t Total Gen Loss : 45.51565933227539, \t Total Dis Loss : 1.69111808645539e-05\n",
      "Steps : 471900, \t Total Gen Loss : 43.746009826660156, \t Total Dis Loss : 2.8627823667193297e-06\n",
      "Steps : 472000, \t Total Gen Loss : 43.40838623046875, \t Total Dis Loss : 3.48045591636037e-06\n",
      "Steps : 472100, \t Total Gen Loss : 43.34673309326172, \t Total Dis Loss : 3.266966450610198e-05\n",
      "Steps : 472200, \t Total Gen Loss : 38.366722106933594, \t Total Dis Loss : 1.3057716614639503e-06\n",
      "Steps : 472300, \t Total Gen Loss : 39.8681640625, \t Total Dis Loss : 1.7971677152672783e-05\n",
      "Steps : 472400, \t Total Gen Loss : 40.86281967163086, \t Total Dis Loss : 4.622522737918189e-06\n",
      "Steps : 472500, \t Total Gen Loss : 39.59913635253906, \t Total Dis Loss : 1.3661883713211864e-05\n",
      "Time for epoch 84 is 74.15406250953674 sec\n",
      "Steps : 472600, \t Total Gen Loss : 34.79070281982422, \t Total Dis Loss : 5.267943834041944e-06\n",
      "Steps : 472700, \t Total Gen Loss : 36.88099670410156, \t Total Dis Loss : 4.0350703784497455e-05\n",
      "Steps : 472800, \t Total Gen Loss : 36.56566619873047, \t Total Dis Loss : 1.867566788860131e-05\n",
      "Steps : 472900, \t Total Gen Loss : 34.96461868286133, \t Total Dis Loss : 4.0199392969952896e-05\n",
      "Steps : 473000, \t Total Gen Loss : 38.52702331542969, \t Total Dis Loss : 5.891156888537807e-06\n",
      "Steps : 473100, \t Total Gen Loss : 40.19474411010742, \t Total Dis Loss : 2.5398230718565173e-06\n",
      "Steps : 473200, \t Total Gen Loss : 36.88062286376953, \t Total Dis Loss : 1.350534489574784e-06\n",
      "Steps : 473300, \t Total Gen Loss : 38.216949462890625, \t Total Dis Loss : 2.9953162083984353e-06\n",
      "Steps : 473400, \t Total Gen Loss : 38.649993896484375, \t Total Dis Loss : 1.1348755833751056e-05\n",
      "Steps : 473500, \t Total Gen Loss : 38.680267333984375, \t Total Dis Loss : 7.82639835961163e-06\n",
      "Steps : 473600, \t Total Gen Loss : 35.85755157470703, \t Total Dis Loss : 2.2935812467039796e-06\n",
      "Steps : 473700, \t Total Gen Loss : 37.575870513916016, \t Total Dis Loss : 1.6938041653702385e-06\n",
      "Steps : 473800, \t Total Gen Loss : 35.99244689941406, \t Total Dis Loss : 1.389822841701971e-06\n",
      "Steps : 473900, \t Total Gen Loss : 36.913543701171875, \t Total Dis Loss : 1.5943812741170404e-06\n",
      "Steps : 474000, \t Total Gen Loss : 34.74969482421875, \t Total Dis Loss : 0.09764451533555984\n",
      "Steps : 474100, \t Total Gen Loss : 41.40602111816406, \t Total Dis Loss : 1.568736115586944e-05\n",
      "Steps : 474200, \t Total Gen Loss : 41.867427825927734, \t Total Dis Loss : 5.4869960877113044e-05\n",
      "Steps : 474300, \t Total Gen Loss : 37.29485321044922, \t Total Dis Loss : 9.333109119324945e-06\n",
      "Steps : 474400, \t Total Gen Loss : 37.235984802246094, \t Total Dis Loss : 4.4923590394319035e-06\n",
      "Steps : 474500, \t Total Gen Loss : 35.015380859375, \t Total Dis Loss : 3.7616009649354964e-06\n",
      "Steps : 474600, \t Total Gen Loss : 36.93983840942383, \t Total Dis Loss : 5.255563883110881e-05\n",
      "Steps : 474700, \t Total Gen Loss : 38.202999114990234, \t Total Dis Loss : 9.160192348645069e-06\n",
      "Steps : 474800, \t Total Gen Loss : 37.227088928222656, \t Total Dis Loss : 9.988103784053237e-07\n",
      "Steps : 474900, \t Total Gen Loss : 37.33851623535156, \t Total Dis Loss : 1.9304832221678225e-06\n",
      "Steps : 475000, \t Total Gen Loss : 38.48457336425781, \t Total Dis Loss : 9.692300864116987e-07\n",
      "Steps : 475100, \t Total Gen Loss : 35.76263427734375, \t Total Dis Loss : 1.1641960554698016e-05\n",
      "Steps : 475200, \t Total Gen Loss : 38.1754150390625, \t Total Dis Loss : 1.3828979490426718e-06\n",
      "Steps : 475300, \t Total Gen Loss : 36.931663513183594, \t Total Dis Loss : 3.8803782445029356e-06\n",
      "Steps : 475400, \t Total Gen Loss : 39.26255416870117, \t Total Dis Loss : 1.6367030184483156e-06\n",
      "Steps : 475500, \t Total Gen Loss : 37.612483978271484, \t Total Dis Loss : 1.2956893442606088e-05\n",
      "Steps : 475600, \t Total Gen Loss : 37.1217155456543, \t Total Dis Loss : 2.7889532248082105e-06\n",
      "Steps : 475700, \t Total Gen Loss : 37.64417266845703, \t Total Dis Loss : 7.198279377007566e-07\n",
      "Steps : 475800, \t Total Gen Loss : 36.901466369628906, \t Total Dis Loss : 3.300931894045789e-06\n",
      "Steps : 475900, \t Total Gen Loss : 41.44508743286133, \t Total Dis Loss : 1.6838430383359082e-05\n",
      "Steps : 476000, \t Total Gen Loss : 36.145263671875, \t Total Dis Loss : 4.464815447136061e-06\n",
      "Steps : 476100, \t Total Gen Loss : 39.572269439697266, \t Total Dis Loss : 3.463814266524423e-07\n",
      "Steps : 476200, \t Total Gen Loss : 37.868656158447266, \t Total Dis Loss : 3.021471854935953e-07\n",
      "Steps : 476300, \t Total Gen Loss : 34.82229232788086, \t Total Dis Loss : 9.762108675204217e-06\n",
      "Steps : 476400, \t Total Gen Loss : 36.24201965332031, \t Total Dis Loss : 3.3340479603793938e-06\n",
      "Steps : 476500, \t Total Gen Loss : 36.4276123046875, \t Total Dis Loss : 2.2301237549982034e-06\n",
      "Steps : 476600, \t Total Gen Loss : 40.32162857055664, \t Total Dis Loss : 2.1587873106909683e-06\n",
      "Steps : 476700, \t Total Gen Loss : 35.308311462402344, \t Total Dis Loss : 2.8282984203542583e-06\n",
      "Steps : 476800, \t Total Gen Loss : 37.94171905517578, \t Total Dis Loss : 1.1812812772404868e-06\n",
      "Steps : 476900, \t Total Gen Loss : 38.67656707763672, \t Total Dis Loss : 4.73511903464896e-07\n",
      "Steps : 477000, \t Total Gen Loss : 35.46833801269531, \t Total Dis Loss : 6.124244919192279e-06\n",
      "Steps : 477100, \t Total Gen Loss : 42.331295013427734, \t Total Dis Loss : 2.2255298972595483e-06\n",
      "Steps : 477200, \t Total Gen Loss : 36.57326889038086, \t Total Dis Loss : 6.415871212084312e-07\n",
      "Steps : 477300, \t Total Gen Loss : 36.60542297363281, \t Total Dis Loss : 3.1544345802103635e-06\n",
      "Steps : 477400, \t Total Gen Loss : 31.325340270996094, \t Total Dis Loss : 2.3297499865293503e-05\n",
      "Steps : 477500, \t Total Gen Loss : 34.059303283691406, \t Total Dis Loss : 3.540404577506706e-05\n",
      "Steps : 477600, \t Total Gen Loss : 31.56159210205078, \t Total Dis Loss : 4.1113089537248015e-06\n",
      "Steps : 477700, \t Total Gen Loss : 35.79010772705078, \t Total Dis Loss : 3.816923310751008e-07\n",
      "Steps : 477800, \t Total Gen Loss : 33.95948028564453, \t Total Dis Loss : 7.290314897545613e-06\n",
      "Steps : 477900, \t Total Gen Loss : 33.640655517578125, \t Total Dis Loss : 2.672559639904648e-05\n",
      "Steps : 478000, \t Total Gen Loss : 30.614582061767578, \t Total Dis Loss : 0.00015724175318609923\n",
      "Steps : 478100, \t Total Gen Loss : 33.546974182128906, \t Total Dis Loss : 9.238710845238529e-06\n",
      "Time for epoch 85 is 77.61399793624878 sec\n",
      "Steps : 478200, \t Total Gen Loss : 30.223419189453125, \t Total Dis Loss : 1.76749217644101e-05\n",
      "Steps : 478300, \t Total Gen Loss : 31.979976654052734, \t Total Dis Loss : 3.2728819405747345e-06\n",
      "Steps : 478400, \t Total Gen Loss : 34.879981994628906, \t Total Dis Loss : 5.623960532830097e-07\n",
      "Steps : 478500, \t Total Gen Loss : 32.68114471435547, \t Total Dis Loss : 1.840463482949417e-05\n",
      "Steps : 478600, \t Total Gen Loss : 31.672122955322266, \t Total Dis Loss : 1.3789874174108263e-05\n",
      "Steps : 478700, \t Total Gen Loss : 34.810211181640625, \t Total Dis Loss : 2.4969916921691038e-05\n",
      "Steps : 478800, \t Total Gen Loss : 35.82307434082031, \t Total Dis Loss : 9.834127467911458e-07\n",
      "Steps : 478900, \t Total Gen Loss : 30.91366958618164, \t Total Dis Loss : 1.6119448673634906e-06\n",
      "Steps : 479000, \t Total Gen Loss : 34.15462875366211, \t Total Dis Loss : 2.7488251362228766e-05\n",
      "Steps : 479100, \t Total Gen Loss : 38.22279739379883, \t Total Dis Loss : 7.414353149215458e-06\n",
      "Steps : 479200, \t Total Gen Loss : 37.31171417236328, \t Total Dis Loss : 7.079201225224097e-08\n",
      "Steps : 479300, \t Total Gen Loss : 36.67461395263672, \t Total Dis Loss : 5.429187410754821e-08\n",
      "Steps : 479400, \t Total Gen Loss : 33.76502227783203, \t Total Dis Loss : 6.749501721969864e-07\n",
      "Steps : 479500, \t Total Gen Loss : 35.461891174316406, \t Total Dis Loss : 2.25999187364323e-07\n",
      "Steps : 479600, \t Total Gen Loss : 34.66985321044922, \t Total Dis Loss : 5.422123194875894e-06\n",
      "Steps : 479700, \t Total Gen Loss : 34.852745056152344, \t Total Dis Loss : 3.3666091781014984e-07\n",
      "Steps : 479800, \t Total Gen Loss : 31.912473678588867, \t Total Dis Loss : 1.287769919144921e-06\n",
      "Steps : 479900, \t Total Gen Loss : 29.911319732666016, \t Total Dis Loss : 4.353858457761817e-06\n",
      "Steps : 480000, \t Total Gen Loss : 34.01906967163086, \t Total Dis Loss : 5.2637715270975605e-05\n",
      "Steps : 480100, \t Total Gen Loss : 32.98103332519531, \t Total Dis Loss : 3.1762669095769525e-05\n",
      "Steps : 480200, \t Total Gen Loss : 32.895225524902344, \t Total Dis Loss : 1.7395104805473238e-06\n",
      "Steps : 480300, \t Total Gen Loss : 32.402313232421875, \t Total Dis Loss : 2.2477994207292795e-05\n",
      "Steps : 480400, \t Total Gen Loss : 34.84086227416992, \t Total Dis Loss : 2.9438027922878973e-05\n",
      "Steps : 480500, \t Total Gen Loss : 29.24386978149414, \t Total Dis Loss : 2.4293045498779975e-05\n",
      "Steps : 480600, \t Total Gen Loss : 37.31163787841797, \t Total Dis Loss : 7.541225954810216e-08\n",
      "Steps : 480700, \t Total Gen Loss : 36.32695388793945, \t Total Dis Loss : 1.9749954560666083e-08\n",
      "Steps : 480800, \t Total Gen Loss : 41.35280227661133, \t Total Dis Loss : 5.976682587061077e-06\n",
      "Steps : 480900, \t Total Gen Loss : 34.34156036376953, \t Total Dis Loss : 1.753882543198415e-06\n",
      "Steps : 481000, \t Total Gen Loss : 34.836978912353516, \t Total Dis Loss : 6.963607575016795e-07\n",
      "Steps : 481100, \t Total Gen Loss : 35.7999267578125, \t Total Dis Loss : 1.5632398572051898e-05\n",
      "Steps : 481200, \t Total Gen Loss : 35.47895812988281, \t Total Dis Loss : 7.898149306129199e-06\n",
      "Steps : 481300, \t Total Gen Loss : 32.78742980957031, \t Total Dis Loss : 5.307854735292494e-06\n",
      "Steps : 481400, \t Total Gen Loss : 36.81122589111328, \t Total Dis Loss : 5.198279268370243e-06\n",
      "Steps : 481500, \t Total Gen Loss : 35.46009826660156, \t Total Dis Loss : 2.9526217986131087e-06\n",
      "Steps : 481600, \t Total Gen Loss : 33.21480941772461, \t Total Dis Loss : 5.585164217336569e-06\n",
      "Steps : 481700, \t Total Gen Loss : 36.81049346923828, \t Total Dis Loss : 1.1007887223968282e-05\n",
      "Steps : 481800, \t Total Gen Loss : 30.939748764038086, \t Total Dis Loss : 3.137150770271546e-06\n",
      "Steps : 481900, \t Total Gen Loss : 33.37297821044922, \t Total Dis Loss : 2.179296103577144e-07\n",
      "Steps : 482000, \t Total Gen Loss : 35.87379455566406, \t Total Dis Loss : 4.020168944407487e-06\n",
      "Steps : 482100, \t Total Gen Loss : 33.98264694213867, \t Total Dis Loss : 1.186638769468118e-06\n",
      "Steps : 482200, \t Total Gen Loss : 32.298583984375, \t Total Dis Loss : 8.615872957307147e-07\n",
      "Steps : 482300, \t Total Gen Loss : 33.3858528137207, \t Total Dis Loss : 1.0310802281310316e-05\n",
      "Steps : 482400, \t Total Gen Loss : 32.53207778930664, \t Total Dis Loss : 2.5266519969591172e-06\n",
      "Steps : 482500, \t Total Gen Loss : 36.45292663574219, \t Total Dis Loss : 2.2250242182053626e-06\n",
      "Steps : 482600, \t Total Gen Loss : 36.151344299316406, \t Total Dis Loss : 1.5264763533195946e-06\n",
      "Steps : 482700, \t Total Gen Loss : 32.21051788330078, \t Total Dis Loss : 1.1373823554094997e-06\n",
      "Steps : 482800, \t Total Gen Loss : 31.87797737121582, \t Total Dis Loss : 5.842703103553504e-05\n",
      "Steps : 482900, \t Total Gen Loss : 31.615406036376953, \t Total Dis Loss : 0.00011364780948497355\n",
      "Steps : 483000, \t Total Gen Loss : 29.862552642822266, \t Total Dis Loss : 9.499718544248026e-06\n",
      "Steps : 483100, \t Total Gen Loss : 30.052490234375, \t Total Dis Loss : 7.67153687775135e-06\n",
      "Steps : 483200, \t Total Gen Loss : 33.87971496582031, \t Total Dis Loss : 3.911023304681294e-05\n",
      "Steps : 483300, \t Total Gen Loss : 32.00484085083008, \t Total Dis Loss : 6.724035756633384e-06\n",
      "Steps : 483400, \t Total Gen Loss : 30.126850128173828, \t Total Dis Loss : 3.554902650648728e-05\n",
      "Steps : 483500, \t Total Gen Loss : 29.98488426208496, \t Total Dis Loss : 2.8623575417441316e-05\n",
      "Steps : 483600, \t Total Gen Loss : 31.467557907104492, \t Total Dis Loss : 4.163272024015896e-06\n",
      "Steps : 483700, \t Total Gen Loss : 30.106731414794922, \t Total Dis Loss : 1.9162385797244497e-05\n",
      "Time for epoch 86 is 81.40151190757751 sec\n",
      "Steps : 483800, \t Total Gen Loss : 32.51824951171875, \t Total Dis Loss : 1.348069326922996e-05\n",
      "Steps : 483900, \t Total Gen Loss : 33.07588195800781, \t Total Dis Loss : 7.106275916157756e-06\n",
      "Steps : 484000, \t Total Gen Loss : 34.87838363647461, \t Total Dis Loss : 1.5901138183949115e-08\n",
      "Steps : 484100, \t Total Gen Loss : 36.596614837646484, \t Total Dis Loss : 3.533751424811271e-08\n",
      "Steps : 484200, \t Total Gen Loss : 33.65068817138672, \t Total Dis Loss : 2.272755637022783e-06\n",
      "Steps : 484300, \t Total Gen Loss : 34.114288330078125, \t Total Dis Loss : 1.7504197558082524e-07\n",
      "Steps : 484400, \t Total Gen Loss : 37.17046356201172, \t Total Dis Loss : 1.8951021729662898e-06\n",
      "Steps : 484500, \t Total Gen Loss : 35.965518951416016, \t Total Dis Loss : 3.485592969809659e-06\n",
      "Steps : 484600, \t Total Gen Loss : 32.38946533203125, \t Total Dis Loss : 3.9525702959508635e-06\n",
      "Steps : 484700, \t Total Gen Loss : 34.566734313964844, \t Total Dis Loss : 3.7636416436725995e-06\n",
      "Steps : 484800, \t Total Gen Loss : 35.87055587768555, \t Total Dis Loss : 2.743193135756883e-06\n",
      "Steps : 484900, \t Total Gen Loss : 35.64055633544922, \t Total Dis Loss : 4.973787781636929e-07\n",
      "Steps : 485000, \t Total Gen Loss : 33.11829376220703, \t Total Dis Loss : 2.3572911231894977e-06\n",
      "Steps : 485100, \t Total Gen Loss : 35.10570526123047, \t Total Dis Loss : 3.131716752591274e-08\n",
      "Steps : 485200, \t Total Gen Loss : 38.11761474609375, \t Total Dis Loss : 5.053374252383946e-07\n",
      "Steps : 485300, \t Total Gen Loss : 36.58201217651367, \t Total Dis Loss : 2.1888314449824975e-07\n",
      "Steps : 485400, \t Total Gen Loss : 33.71443557739258, \t Total Dis Loss : 7.114541222108528e-05\n",
      "Steps : 485500, \t Total Gen Loss : 37.570621490478516, \t Total Dis Loss : 3.125898274447536e-06\n",
      "Steps : 485600, \t Total Gen Loss : 34.22444534301758, \t Total Dis Loss : 1.546716816847038e-06\n",
      "Steps : 485700, \t Total Gen Loss : 38.36076354980469, \t Total Dis Loss : 5.619955700808532e-08\n",
      "Steps : 485800, \t Total Gen Loss : 36.558319091796875, \t Total Dis Loss : 2.52639188147441e-06\n",
      "Steps : 485900, \t Total Gen Loss : 35.94965362548828, \t Total Dis Loss : 8.898138759150243e-08\n",
      "Steps : 486000, \t Total Gen Loss : 34.981689453125, \t Total Dis Loss : 1.3560204479290405e-06\n",
      "Steps : 486100, \t Total Gen Loss : 33.37189483642578, \t Total Dis Loss : 3.0690941912325798e-06\n",
      "Steps : 486200, \t Total Gen Loss : 37.10936737060547, \t Total Dis Loss : 4.2244531073265534e-07\n",
      "Steps : 486300, \t Total Gen Loss : 38.40727233886719, \t Total Dis Loss : 1.4549716979672667e-05\n",
      "Steps : 486400, \t Total Gen Loss : 36.02537536621094, \t Total Dis Loss : 7.565053238067776e-06\n",
      "Steps : 486500, \t Total Gen Loss : 34.57316589355469, \t Total Dis Loss : 7.139872195693897e-06\n",
      "Steps : 486600, \t Total Gen Loss : 34.96589660644531, \t Total Dis Loss : 1.7306781955994666e-05\n",
      "Steps : 486700, \t Total Gen Loss : 32.61570739746094, \t Total Dis Loss : 2.4524946638848633e-05\n",
      "Steps : 486800, \t Total Gen Loss : 31.719982147216797, \t Total Dis Loss : 1.2987673471798189e-05\n",
      "Steps : 486900, \t Total Gen Loss : 32.41252899169922, \t Total Dis Loss : 0.0001743960310705006\n",
      "Steps : 487000, \t Total Gen Loss : 36.73627471923828, \t Total Dis Loss : 7.40215546102263e-06\n",
      "Steps : 487100, \t Total Gen Loss : 36.96209716796875, \t Total Dis Loss : 2.774016820694669e-07\n",
      "Steps : 487200, \t Total Gen Loss : 33.785362243652344, \t Total Dis Loss : 1.3353493955037266e-07\n",
      "Steps : 487300, \t Total Gen Loss : 45.973941802978516, \t Total Dis Loss : 3.925144483218901e-06\n",
      "Steps : 487400, \t Total Gen Loss : 43.839115142822266, \t Total Dis Loss : 1.5240561879181769e-05\n",
      "Steps : 487500, \t Total Gen Loss : 40.8218879699707, \t Total Dis Loss : 2.8941078653588193e-06\n",
      "Steps : 487600, \t Total Gen Loss : 42.56679153442383, \t Total Dis Loss : 4.176747552264715e-06\n",
      "Steps : 487700, \t Total Gen Loss : 38.98014450073242, \t Total Dis Loss : 8.746869571041316e-05\n",
      "Steps : 487800, \t Total Gen Loss : 37.450401306152344, \t Total Dis Loss : 4.000455646746559e-07\n",
      "Steps : 487900, \t Total Gen Loss : 38.63083267211914, \t Total Dis Loss : 2.1831282026596455e-07\n",
      "Steps : 488000, \t Total Gen Loss : 37.8319206237793, \t Total Dis Loss : 1.4202208298286223e-08\n",
      "Steps : 488100, \t Total Gen Loss : 37.62395477294922, \t Total Dis Loss : 7.78196920236951e-07\n",
      "Steps : 488200, \t Total Gen Loss : 37.05121612548828, \t Total Dis Loss : 5.442591941573482e-07\n",
      "Steps : 488300, \t Total Gen Loss : 37.07322692871094, \t Total Dis Loss : 6.130904694146011e-06\n",
      "Steps : 488400, \t Total Gen Loss : 33.40936279296875, \t Total Dis Loss : 3.954127976157906e-07\n",
      "Steps : 488500, \t Total Gen Loss : 37.63930892944336, \t Total Dis Loss : 1.995839966184576e-06\n",
      "Steps : 488600, \t Total Gen Loss : 32.05012893676758, \t Total Dis Loss : 1.2426836519807694e-06\n",
      "Steps : 488700, \t Total Gen Loss : 36.22374725341797, \t Total Dis Loss : 1.2246030109963613e-06\n",
      "Steps : 488800, \t Total Gen Loss : 35.783363342285156, \t Total Dis Loss : 2.4153385311365128e-05\n",
      "Steps : 488900, \t Total Gen Loss : 35.69499969482422, \t Total Dis Loss : 1.7910192582348827e-06\n",
      "Steps : 489000, \t Total Gen Loss : 35.64619445800781, \t Total Dis Loss : 1.3775502338830847e-06\n",
      "Steps : 489100, \t Total Gen Loss : 40.344215393066406, \t Total Dis Loss : 1.4012440487931599e-06\n",
      "Steps : 489200, \t Total Gen Loss : 35.27073669433594, \t Total Dis Loss : 5.598880761681357e-06\n",
      "Steps : 489300, \t Total Gen Loss : 37.33084487915039, \t Total Dis Loss : 3.392555527170771e-06\n",
      "Time for epoch 87 is 80.46778559684753 sec\n",
      "Steps : 489400, \t Total Gen Loss : 37.99163055419922, \t Total Dis Loss : 2.0686920834123157e-06\n",
      "Steps : 489500, \t Total Gen Loss : 41.123756408691406, \t Total Dis Loss : 1.1494848877191544e-06\n",
      "Steps : 489600, \t Total Gen Loss : 37.681060791015625, \t Total Dis Loss : 3.584021897040657e-07\n",
      "Steps : 489700, \t Total Gen Loss : 35.78703689575195, \t Total Dis Loss : 1.3962562661617994e-06\n",
      "Steps : 489800, \t Total Gen Loss : 32.456172943115234, \t Total Dis Loss : 4.81615143144154e-06\n",
      "Steps : 489900, \t Total Gen Loss : 31.112266540527344, \t Total Dis Loss : 1.8846356397261843e-05\n",
      "Steps : 490000, \t Total Gen Loss : 35.23875045776367, \t Total Dis Loss : 1.158447389570938e-06\n",
      "Steps : 490100, \t Total Gen Loss : 39.327476501464844, \t Total Dis Loss : 8.790207175479736e-06\n",
      "Steps : 490200, \t Total Gen Loss : 31.96886444091797, \t Total Dis Loss : 1.1823171917058062e-05\n",
      "Steps : 490300, \t Total Gen Loss : 33.535133361816406, \t Total Dis Loss : 1.6997069906210527e-05\n",
      "Steps : 490400, \t Total Gen Loss : 33.01020812988281, \t Total Dis Loss : 3.7157299175305525e-06\n",
      "Steps : 490500, \t Total Gen Loss : 31.176528930664062, \t Total Dis Loss : 5.790041177533567e-05\n",
      "Steps : 490600, \t Total Gen Loss : 30.6688232421875, \t Total Dis Loss : 3.141015622531995e-05\n",
      "Steps : 490700, \t Total Gen Loss : 35.551692962646484, \t Total Dis Loss : 3.031507560535829e-07\n",
      "Steps : 490800, \t Total Gen Loss : 38.50550842285156, \t Total Dis Loss : 3.009980446222471e-06\n",
      "Steps : 490900, \t Total Gen Loss : 38.0189323425293, \t Total Dis Loss : 7.519397513533477e-07\n",
      "Steps : 491000, \t Total Gen Loss : 37.33467102050781, \t Total Dis Loss : 1.3244871865936148e-07\n",
      "Steps : 491100, \t Total Gen Loss : 39.804542541503906, \t Total Dis Loss : 5.952600758973858e-07\n",
      "Steps : 491200, \t Total Gen Loss : 35.03988265991211, \t Total Dis Loss : 2.872746449611441e-07\n",
      "Steps : 491300, \t Total Gen Loss : 39.0412483215332, \t Total Dis Loss : 1.5470942571482738e-07\n",
      "Steps : 491400, \t Total Gen Loss : 37.46792221069336, \t Total Dis Loss : 1.257449042668668e-07\n",
      "Steps : 491500, \t Total Gen Loss : 38.44150161743164, \t Total Dis Loss : 3.1362432650894334e-08\n",
      "Steps : 491600, \t Total Gen Loss : 38.771026611328125, \t Total Dis Loss : 3.987212267020368e-07\n",
      "Steps : 491700, \t Total Gen Loss : 40.242095947265625, \t Total Dis Loss : 4.4720289338329167e-07\n",
      "Steps : 491800, \t Total Gen Loss : 37.214874267578125, \t Total Dis Loss : 3.6759036703415404e-08\n",
      "Steps : 491900, \t Total Gen Loss : 36.56835174560547, \t Total Dis Loss : 4.3631450807879446e-07\n",
      "Steps : 492000, \t Total Gen Loss : 38.36946105957031, \t Total Dis Loss : 3.4895336398221843e-07\n",
      "Steps : 492100, \t Total Gen Loss : 37.12034225463867, \t Total Dis Loss : 1.2611282329544338e-07\n",
      "Steps : 492200, \t Total Gen Loss : 36.637298583984375, \t Total Dis Loss : 4.0542192891734885e-07\n",
      "Steps : 492300, \t Total Gen Loss : 31.734609603881836, \t Total Dis Loss : 0.0010257118847221136\n",
      "Steps : 492400, \t Total Gen Loss : 35.223052978515625, \t Total Dis Loss : 1.170831524177629e-06\n",
      "Steps : 492500, \t Total Gen Loss : 39.65802764892578, \t Total Dis Loss : 4.162945543839669e-08\n",
      "Steps : 492600, \t Total Gen Loss : 37.170692443847656, \t Total Dis Loss : 5.350509013624105e-07\n",
      "Steps : 492700, \t Total Gen Loss : 36.01656723022461, \t Total Dis Loss : 3.443203695496777e-07\n",
      "Steps : 492800, \t Total Gen Loss : 38.03208923339844, \t Total Dis Loss : 2.6851679990613775e-07\n",
      "Steps : 492900, \t Total Gen Loss : 35.90442657470703, \t Total Dis Loss : 1.9938288176035712e-07\n",
      "Steps : 493000, \t Total Gen Loss : 38.50670623779297, \t Total Dis Loss : 9.370343718728691e-07\n",
      "Steps : 493100, \t Total Gen Loss : 37.90658950805664, \t Total Dis Loss : 1.1891419404719272e-08\n",
      "Steps : 493200, \t Total Gen Loss : 33.36559295654297, \t Total Dis Loss : 6.300024324445985e-07\n",
      "Steps : 493300, \t Total Gen Loss : 31.827564239501953, \t Total Dis Loss : 1.2940333363076206e-05\n",
      "Steps : 493400, \t Total Gen Loss : 32.8952751159668, \t Total Dis Loss : 1.0715777534642257e-05\n",
      "Steps : 493500, \t Total Gen Loss : 33.39126968383789, \t Total Dis Loss : 1.1517535313032568e-05\n",
      "Steps : 493600, \t Total Gen Loss : 31.883699417114258, \t Total Dis Loss : 7.2528364398749545e-06\n",
      "Steps : 493700, \t Total Gen Loss : 36.49095153808594, \t Total Dis Loss : 5.047483864473179e-05\n",
      "Steps : 493800, \t Total Gen Loss : 38.636566162109375, \t Total Dis Loss : 1.2842417618230684e-06\n",
      "Steps : 493900, \t Total Gen Loss : 38.11981964111328, \t Total Dis Loss : 7.576387588414946e-07\n",
      "Steps : 494000, \t Total Gen Loss : 38.9000129699707, \t Total Dis Loss : 1.392026433677529e-06\n",
      "Steps : 494100, \t Total Gen Loss : 35.678977966308594, \t Total Dis Loss : 1.2358835874692886e-06\n",
      "Steps : 494200, \t Total Gen Loss : 39.78034591674805, \t Total Dis Loss : 2.1071109586046077e-06\n",
      "Steps : 494300, \t Total Gen Loss : 40.34291076660156, \t Total Dis Loss : 4.1232786429645785e-07\n",
      "Steps : 494400, \t Total Gen Loss : 39.40398025512695, \t Total Dis Loss : 8.226614340856031e-07\n",
      "Steps : 494500, \t Total Gen Loss : 38.12598419189453, \t Total Dis Loss : 3.1012086765258573e-07\n",
      "Steps : 494600, \t Total Gen Loss : 37.337158203125, \t Total Dis Loss : 4.3602639721029846e-07\n",
      "Steps : 494700, \t Total Gen Loss : 39.83201599121094, \t Total Dis Loss : 2.4078281057882123e-06\n",
      "Steps : 494800, \t Total Gen Loss : 38.772926330566406, \t Total Dis Loss : 1.648655711505853e-06\n",
      "Steps : 494900, \t Total Gen Loss : 38.54204559326172, \t Total Dis Loss : 2.882725539166131e-06\n",
      "Steps : 495000, \t Total Gen Loss : 38.17294692993164, \t Total Dis Loss : 4.316228114475962e-06\n",
      "Time for epoch 88 is 78.08935785293579 sec\n",
      "Steps : 495100, \t Total Gen Loss : 38.50947570800781, \t Total Dis Loss : 2.786272489174735e-06\n",
      "Steps : 495200, \t Total Gen Loss : 37.885215759277344, \t Total Dis Loss : 7.836925419724139e-07\n",
      "Steps : 495300, \t Total Gen Loss : 40.433536529541016, \t Total Dis Loss : 3.1688166473031742e-06\n",
      "Steps : 495400, \t Total Gen Loss : 36.38981246948242, \t Total Dis Loss : 4.62980779047939e-07\n",
      "Steps : 495500, \t Total Gen Loss : 36.298309326171875, \t Total Dis Loss : 4.076296136190649e-07\n",
      "Steps : 495600, \t Total Gen Loss : 37.89387512207031, \t Total Dis Loss : 5.934810474172991e-07\n",
      "Steps : 495700, \t Total Gen Loss : 41.336952209472656, \t Total Dis Loss : 6.282856475081644e-07\n",
      "Steps : 495800, \t Total Gen Loss : 34.93214416503906, \t Total Dis Loss : 1.0034485740106902e-06\n",
      "Steps : 495900, \t Total Gen Loss : 39.18698501586914, \t Total Dis Loss : 1.0422058949188795e-06\n",
      "Steps : 496000, \t Total Gen Loss : 39.60821533203125, \t Total Dis Loss : 2.5551598810125142e-06\n",
      "Steps : 496100, \t Total Gen Loss : 38.50017547607422, \t Total Dis Loss : 2.391657574207784e-07\n",
      "Steps : 496200, \t Total Gen Loss : 37.56777572631836, \t Total Dis Loss : 4.402096749345219e-07\n",
      "Steps : 496300, \t Total Gen Loss : 37.70177459716797, \t Total Dis Loss : 2.601919675271347e-07\n",
      "Steps : 496400, \t Total Gen Loss : 41.49501037597656, \t Total Dis Loss : 1.069322593139077e-06\n",
      "Steps : 496500, \t Total Gen Loss : 39.58331298828125, \t Total Dis Loss : 1.1893906730620074e-06\n",
      "Steps : 496600, \t Total Gen Loss : 35.444435119628906, \t Total Dis Loss : 5.414292445493629e-07\n",
      "Steps : 496700, \t Total Gen Loss : 40.19617462158203, \t Total Dis Loss : 1.6025671811803477e-06\n",
      "Steps : 496800, \t Total Gen Loss : 38.06348419189453, \t Total Dis Loss : 5.075619355920935e-07\n",
      "Steps : 496900, \t Total Gen Loss : 34.086517333984375, \t Total Dis Loss : 7.899661795818247e-06\n",
      "Steps : 497000, \t Total Gen Loss : 32.92549133300781, \t Total Dis Loss : 7.159592882999277e-07\n",
      "Steps : 497100, \t Total Gen Loss : 37.261741638183594, \t Total Dis Loss : 7.458898210188636e-08\n",
      "Steps : 497200, \t Total Gen Loss : 38.583946228027344, \t Total Dis Loss : 1.6473736650368664e-06\n",
      "Steps : 497300, \t Total Gen Loss : 40.96281433105469, \t Total Dis Loss : 1.36305459363939e-07\n",
      "Steps : 497400, \t Total Gen Loss : 40.1671142578125, \t Total Dis Loss : 6.333814894787793e-08\n",
      "Steps : 497500, \t Total Gen Loss : 37.01040267944336, \t Total Dis Loss : 9.98299810817116e-07\n",
      "Steps : 497600, \t Total Gen Loss : 36.58837127685547, \t Total Dis Loss : 8.701702427060809e-06\n",
      "Steps : 497700, \t Total Gen Loss : 41.22772216796875, \t Total Dis Loss : 6.0714637584169395e-06\n",
      "Steps : 497800, \t Total Gen Loss : 35.37276077270508, \t Total Dis Loss : 2.246703843411524e-06\n",
      "Steps : 497900, \t Total Gen Loss : 39.29010772705078, \t Total Dis Loss : 2.1625446606776677e-06\n",
      "Steps : 498000, \t Total Gen Loss : 35.23257064819336, \t Total Dis Loss : 5.639459232043009e-06\n",
      "Steps : 498100, \t Total Gen Loss : 39.201908111572266, \t Total Dis Loss : 6.135879715429837e-08\n",
      "Steps : 498200, \t Total Gen Loss : 39.5200309753418, \t Total Dis Loss : 2.0535280782496557e-06\n",
      "Steps : 498300, \t Total Gen Loss : 38.671630859375, \t Total Dis Loss : 2.699466108424531e-07\n",
      "Steps : 498400, \t Total Gen Loss : 39.26854705810547, \t Total Dis Loss : 4.3326508603058755e-06\n",
      "Steps : 498500, \t Total Gen Loss : 39.86446762084961, \t Total Dis Loss : 2.224716126875137e-07\n",
      "Steps : 498600, \t Total Gen Loss : 40.25587463378906, \t Total Dis Loss : 5.808427886222489e-06\n",
      "Steps : 498700, \t Total Gen Loss : 41.41712188720703, \t Total Dis Loss : 1.338474589829275e-06\n",
      "Steps : 498800, \t Total Gen Loss : 38.925758361816406, \t Total Dis Loss : 2.248109012725763e-06\n",
      "Steps : 498900, \t Total Gen Loss : 37.91011047363281, \t Total Dis Loss : 2.3956185941642616e-06\n",
      "Steps : 499000, \t Total Gen Loss : 37.250831604003906, \t Total Dis Loss : 1.9705394151969813e-06\n",
      "Steps : 499100, \t Total Gen Loss : 40.09808349609375, \t Total Dis Loss : 1.1197053936484735e-05\n",
      "Steps : 499200, \t Total Gen Loss : 38.52415466308594, \t Total Dis Loss : 9.237457561539486e-06\n",
      "Steps : 499300, \t Total Gen Loss : 35.550071716308594, \t Total Dis Loss : 4.297879513615044e-06\n",
      "Steps : 499400, \t Total Gen Loss : 35.46778106689453, \t Total Dis Loss : 8.195634109142702e-07\n",
      "Steps : 499500, \t Total Gen Loss : 38.98666763305664, \t Total Dis Loss : 1.4112578128333553e-06\n",
      "Steps : 499600, \t Total Gen Loss : 35.76409149169922, \t Total Dis Loss : 1.3062238394923043e-05\n",
      "Steps : 499700, \t Total Gen Loss : 39.17445755004883, \t Total Dis Loss : 4.1211501411453355e-06\n",
      "Steps : 499800, \t Total Gen Loss : 41.29896545410156, \t Total Dis Loss : 8.655963029013947e-06\n",
      "Steps : 499900, \t Total Gen Loss : 41.514652252197266, \t Total Dis Loss : 1.384622464684071e-05\n",
      "Steps : 500000, \t Total Gen Loss : 41.54304504394531, \t Total Dis Loss : 1.1932644156331662e-05\n",
      "Steps : 500100, \t Total Gen Loss : 37.4984130859375, \t Total Dis Loss : 5.147978754393989e-07\n",
      "Steps : 500200, \t Total Gen Loss : 36.36390686035156, \t Total Dis Loss : 2.64121172222076e-05\n",
      "Steps : 500300, \t Total Gen Loss : 39.33413314819336, \t Total Dis Loss : 2.1074488358863164e-06\n",
      "Steps : 500400, \t Total Gen Loss : 38.946754455566406, \t Total Dis Loss : 9.743425835040398e-07\n",
      "Steps : 500500, \t Total Gen Loss : 40.56212615966797, \t Total Dis Loss : 2.937882754849852e-06\n",
      "Steps : 500600, \t Total Gen Loss : 38.392852783203125, \t Total Dis Loss : 3.716711944434792e-06\n",
      "Time for epoch 89 is 79.4615752696991 sec\n",
      "Steps : 500700, \t Total Gen Loss : 39.51218032836914, \t Total Dis Loss : 2.949342388092191e-07\n",
      "Steps : 500800, \t Total Gen Loss : 33.316131591796875, \t Total Dis Loss : 4.420581717567984e-06\n",
      "Steps : 500900, \t Total Gen Loss : 35.69606018066406, \t Total Dis Loss : 6.154390916890407e-07\n",
      "Steps : 501000, \t Total Gen Loss : 34.78021240234375, \t Total Dis Loss : 1.2165678242581635e-07\n",
      "Steps : 501100, \t Total Gen Loss : 37.77379608154297, \t Total Dis Loss : 1.8907575167759205e-06\n",
      "Steps : 501200, \t Total Gen Loss : 40.74415969848633, \t Total Dis Loss : 2.266173396492377e-06\n",
      "Steps : 501300, \t Total Gen Loss : 37.450321197509766, \t Total Dis Loss : 3.406431460462045e-06\n",
      "Steps : 501400, \t Total Gen Loss : 36.211090087890625, \t Total Dis Loss : 5.044863655712106e-07\n",
      "Steps : 501500, \t Total Gen Loss : 36.404624938964844, \t Total Dis Loss : 8.113437388601596e-07\n",
      "Steps : 501600, \t Total Gen Loss : 44.39292526245117, \t Total Dis Loss : 6.766927981516346e-06\n",
      "Steps : 501700, \t Total Gen Loss : 36.11981201171875, \t Total Dis Loss : 4.5451241021510214e-05\n",
      "Steps : 501800, \t Total Gen Loss : 41.86977005004883, \t Total Dis Loss : 8.985190106614027e-06\n",
      "Steps : 501900, \t Total Gen Loss : 38.27948760986328, \t Total Dis Loss : 1.519265566685135e-07\n",
      "Steps : 502000, \t Total Gen Loss : 40.734214782714844, \t Total Dis Loss : 3.2250954973278567e-07\n",
      "Steps : 502100, \t Total Gen Loss : 34.947601318359375, \t Total Dis Loss : 6.698162451357348e-06\n",
      "Steps : 502200, \t Total Gen Loss : 35.826961517333984, \t Total Dis Loss : 4.058103877468966e-05\n",
      "Steps : 502300, \t Total Gen Loss : 35.801246643066406, \t Total Dis Loss : 1.8241358702653088e-05\n",
      "Steps : 502400, \t Total Gen Loss : 33.601463317871094, \t Total Dis Loss : 1.0084697350976057e-05\n",
      "Steps : 502500, \t Total Gen Loss : 34.30699920654297, \t Total Dis Loss : 1.6822212273837067e-05\n",
      "Steps : 502600, \t Total Gen Loss : 31.00016975402832, \t Total Dis Loss : 7.64854939916404e-06\n",
      "Steps : 502700, \t Total Gen Loss : 34.440589904785156, \t Total Dis Loss : 2.579120064183371e-06\n",
      "Steps : 502800, \t Total Gen Loss : 35.90919876098633, \t Total Dis Loss : 8.29072632768657e-06\n",
      "Steps : 502900, \t Total Gen Loss : 34.61384582519531, \t Total Dis Loss : 0.00011029953748220578\n",
      "Steps : 503000, \t Total Gen Loss : 37.54804992675781, \t Total Dis Loss : 7.758089282106084e-07\n",
      "Steps : 503100, \t Total Gen Loss : 36.31523895263672, \t Total Dis Loss : 0.00010951394506264478\n",
      "Steps : 503200, \t Total Gen Loss : 31.906753540039062, \t Total Dis Loss : 5.4228667067945935e-06\n",
      "Steps : 503300, \t Total Gen Loss : 34.90418243408203, \t Total Dis Loss : 6.682670209556818e-05\n",
      "Steps : 503400, \t Total Gen Loss : 34.838829040527344, \t Total Dis Loss : 4.284252099751029e-06\n",
      "Steps : 503500, \t Total Gen Loss : 32.464988708496094, \t Total Dis Loss : 1.6956888657659874e-06\n",
      "Steps : 503600, \t Total Gen Loss : 36.80573272705078, \t Total Dis Loss : 8.462058758595958e-06\n",
      "Steps : 503700, \t Total Gen Loss : 33.90501022338867, \t Total Dis Loss : 2.15017244045157e-05\n",
      "Steps : 503800, \t Total Gen Loss : 38.388099670410156, \t Total Dis Loss : 6.032573310221778e-06\n",
      "Steps : 503900, \t Total Gen Loss : 39.995849609375, \t Total Dis Loss : 2.045337851086515e-06\n",
      "Steps : 504000, \t Total Gen Loss : 39.99954605102539, \t Total Dis Loss : 3.097787839578814e-06\n",
      "Steps : 504100, \t Total Gen Loss : 35.98468780517578, \t Total Dis Loss : 2.2026906663086265e-06\n",
      "Steps : 504200, \t Total Gen Loss : 37.6620979309082, \t Total Dis Loss : 2.1605114852718543e-06\n",
      "Steps : 504300, \t Total Gen Loss : 38.056087493896484, \t Total Dis Loss : 1.8936586911877384e-06\n",
      "Steps : 504400, \t Total Gen Loss : 39.826324462890625, \t Total Dis Loss : 4.885513590124901e-06\n",
      "Steps : 504500, \t Total Gen Loss : 37.76852798461914, \t Total Dis Loss : 5.221155561230262e-07\n",
      "Steps : 504600, \t Total Gen Loss : 42.739009857177734, \t Total Dis Loss : 4.96827351526008e-06\n",
      "Steps : 504700, \t Total Gen Loss : 42.55913543701172, \t Total Dis Loss : 4.248364348313771e-06\n",
      "Steps : 504800, \t Total Gen Loss : 36.23590087890625, \t Total Dis Loss : 3.1937752282829024e-06\n",
      "Steps : 504900, \t Total Gen Loss : 37.790428161621094, \t Total Dis Loss : 5.958332167210756e-06\n",
      "Steps : 505000, \t Total Gen Loss : 37.06651306152344, \t Total Dis Loss : 4.578696461976506e-06\n",
      "Steps : 505100, \t Total Gen Loss : 38.78608322143555, \t Total Dis Loss : 5.2371883612067904e-06\n",
      "Steps : 505200, \t Total Gen Loss : 38.84931945800781, \t Total Dis Loss : 6.13461315879249e-06\n",
      "Steps : 505300, \t Total Gen Loss : 38.69925308227539, \t Total Dis Loss : 9.910418157232925e-07\n",
      "Steps : 505400, \t Total Gen Loss : 43.30908203125, \t Total Dis Loss : 1.4169590940582566e-06\n",
      "Steps : 505500, \t Total Gen Loss : 41.54368591308594, \t Total Dis Loss : 7.645731443517434e-07\n",
      "Steps : 505600, \t Total Gen Loss : 37.240413665771484, \t Total Dis Loss : 3.6680069115391234e-06\n",
      "Steps : 505700, \t Total Gen Loss : 37.70807647705078, \t Total Dis Loss : 1.2238009730936028e-05\n",
      "Steps : 505800, \t Total Gen Loss : 38.6485595703125, \t Total Dis Loss : 6.22542256678571e-06\n",
      "Steps : 505900, \t Total Gen Loss : 38.8087043762207, \t Total Dis Loss : 5.854754817846697e-06\n",
      "Steps : 506000, \t Total Gen Loss : 39.45062255859375, \t Total Dis Loss : 2.822795977408532e-06\n",
      "Steps : 506100, \t Total Gen Loss : 39.2252197265625, \t Total Dis Loss : 4.2187218696199125e-07\n",
      "Steps : 506200, \t Total Gen Loss : 38.88565444946289, \t Total Dis Loss : 2.5132671908068005e-06\n",
      "Time for epoch 90 is 81.03088331222534 sec\n",
      "Steps : 506300, \t Total Gen Loss : 41.252933502197266, \t Total Dis Loss : 5.818697445647558e-06\n",
      "Steps : 506400, \t Total Gen Loss : 39.57892990112305, \t Total Dis Loss : 1.5886969777056947e-05\n",
      "Steps : 506500, \t Total Gen Loss : 37.50843811035156, \t Total Dis Loss : 5.19583090863307e-06\n",
      "Steps : 506600, \t Total Gen Loss : 37.331504821777344, \t Total Dis Loss : 6.7550472522270866e-06\n",
      "Steps : 506700, \t Total Gen Loss : 38.8237190246582, \t Total Dis Loss : 1.0884052699111635e-06\n",
      "Steps : 506800, \t Total Gen Loss : 40.10851287841797, \t Total Dis Loss : 1.6258010873571038e-05\n",
      "Steps : 506900, \t Total Gen Loss : 39.00939178466797, \t Total Dis Loss : 0.00010085250687552616\n",
      "Steps : 507000, \t Total Gen Loss : 41.490238189697266, \t Total Dis Loss : 8.132518814818468e-06\n",
      "Steps : 507100, \t Total Gen Loss : 45.508811950683594, \t Total Dis Loss : 1.7123413272202015e-05\n",
      "Steps : 507200, \t Total Gen Loss : 40.47071075439453, \t Total Dis Loss : 1.1393837667128537e-05\n",
      "Steps : 507300, \t Total Gen Loss : 45.258060455322266, \t Total Dis Loss : 9.514357770967763e-06\n",
      "Steps : 507400, \t Total Gen Loss : 44.39765167236328, \t Total Dis Loss : 1.5905932286841562e-06\n",
      "Steps : 507500, \t Total Gen Loss : 45.28525161743164, \t Total Dis Loss : 4.8754292947705835e-05\n",
      "Steps : 507600, \t Total Gen Loss : 43.68852615356445, \t Total Dis Loss : 5.6400960602331907e-05\n",
      "Steps : 507700, \t Total Gen Loss : 39.55767059326172, \t Total Dis Loss : 3.145320533803897e-06\n",
      "Steps : 507800, \t Total Gen Loss : 36.236053466796875, \t Total Dis Loss : 1.4462563058259548e-06\n",
      "Steps : 507900, \t Total Gen Loss : 38.09014892578125, \t Total Dis Loss : 2.778774899070413e-08\n",
      "Steps : 508000, \t Total Gen Loss : 33.98918151855469, \t Total Dis Loss : 2.585708216429339e-06\n",
      "Steps : 508100, \t Total Gen Loss : 35.55694580078125, \t Total Dis Loss : 4.72492189373952e-07\n",
      "Steps : 508200, \t Total Gen Loss : 35.84569549560547, \t Total Dis Loss : 5.4953297023985215e-08\n",
      "Steps : 508300, \t Total Gen Loss : 37.64669418334961, \t Total Dis Loss : 0.0014302519848570228\n",
      "Steps : 508400, \t Total Gen Loss : 37.424015045166016, \t Total Dis Loss : 3.121412106565913e-08\n",
      "Steps : 508500, \t Total Gen Loss : 38.48036193847656, \t Total Dis Loss : 7.01350231224751e-08\n",
      "Steps : 508600, \t Total Gen Loss : 42.401100158691406, \t Total Dis Loss : 2.926536240011046e-07\n",
      "Steps : 508700, \t Total Gen Loss : 39.73037338256836, \t Total Dis Loss : 4.943590852235502e-07\n",
      "Steps : 508800, \t Total Gen Loss : 36.48651123046875, \t Total Dis Loss : 5.619357068553654e-08\n",
      "Steps : 508900, \t Total Gen Loss : 37.52961730957031, \t Total Dis Loss : 7.291728820746357e-07\n",
      "Steps : 509000, \t Total Gen Loss : 39.22297286987305, \t Total Dis Loss : 5.979639354336541e-06\n",
      "Steps : 509100, \t Total Gen Loss : 36.650325775146484, \t Total Dis Loss : 2.7817836780741345e-06\n",
      "Steps : 509200, \t Total Gen Loss : 38.962589263916016, \t Total Dis Loss : 1.2383395642245887e-07\n",
      "Steps : 509300, \t Total Gen Loss : 38.524169921875, \t Total Dis Loss : 1.1229891470065922e-06\n",
      "Steps : 509400, \t Total Gen Loss : 39.61170196533203, \t Total Dis Loss : 3.142432092317904e-08\n",
      "Steps : 509500, \t Total Gen Loss : 38.099308013916016, \t Total Dis Loss : 2.548995894358086e-07\n",
      "Steps : 509600, \t Total Gen Loss : 40.15238952636719, \t Total Dis Loss : 1.2475473987194619e-08\n",
      "Steps : 509700, \t Total Gen Loss : 39.00069046020508, \t Total Dis Loss : 2.6824507131095743e-06\n",
      "Steps : 509800, \t Total Gen Loss : 36.18523406982422, \t Total Dis Loss : 1.66021175118658e-06\n",
      "Steps : 509900, \t Total Gen Loss : 38.36505889892578, \t Total Dis Loss : 2.2100191472418373e-06\n",
      "Steps : 510000, \t Total Gen Loss : 34.90801239013672, \t Total Dis Loss : 2.017791757680243e-06\n",
      "Steps : 510100, \t Total Gen Loss : 36.139102935791016, \t Total Dis Loss : 8.614513717475347e-06\n",
      "Steps : 510200, \t Total Gen Loss : 32.64691162109375, \t Total Dis Loss : 1.3966222240924253e-06\n",
      "Steps : 510300, \t Total Gen Loss : 33.671852111816406, \t Total Dis Loss : 1.428554924132186e-06\n",
      "Steps : 510400, \t Total Gen Loss : 32.80540084838867, \t Total Dis Loss : 1.7393532516507548e-06\n",
      "Steps : 510500, \t Total Gen Loss : 35.938568115234375, \t Total Dis Loss : 1.729003997752443e-05\n",
      "Steps : 510600, \t Total Gen Loss : 35.41670608520508, \t Total Dis Loss : 3.1086847229744308e-06\n",
      "Steps : 510700, \t Total Gen Loss : 37.8636474609375, \t Total Dis Loss : 1.8190064565715147e-06\n",
      "Steps : 510800, \t Total Gen Loss : 33.26854705810547, \t Total Dis Loss : 1.1209919648536015e-05\n",
      "Steps : 510900, \t Total Gen Loss : 33.22075653076172, \t Total Dis Loss : 1.8456665202393197e-05\n",
      "Steps : 511000, \t Total Gen Loss : 35.09265899658203, \t Total Dis Loss : 2.020866122620646e-05\n",
      "Steps : 511100, \t Total Gen Loss : 33.7064208984375, \t Total Dis Loss : 9.865501851891167e-06\n",
      "Steps : 511200, \t Total Gen Loss : 33.41425323486328, \t Total Dis Loss : 5.125185452925507e-06\n",
      "Steps : 511300, \t Total Gen Loss : 39.12049865722656, \t Total Dis Loss : 5.881827746634372e-06\n",
      "Steps : 511400, \t Total Gen Loss : 36.75669860839844, \t Total Dis Loss : 7.566747626697179e-06\n",
      "Steps : 511500, \t Total Gen Loss : 37.159122467041016, \t Total Dis Loss : 6.784424840589054e-06\n",
      "Steps : 511600, \t Total Gen Loss : 32.14373016357422, \t Total Dis Loss : 4.8812794375407975e-06\n",
      "Steps : 511700, \t Total Gen Loss : 38.355934143066406, \t Total Dis Loss : 1.1432333621996804e-06\n",
      "Steps : 511800, \t Total Gen Loss : 33.68380355834961, \t Total Dis Loss : 1.1703658856276888e-05\n",
      "Time for epoch 91 is 82.34078979492188 sec\n",
      "Steps : 511900, \t Total Gen Loss : 35.581298828125, \t Total Dis Loss : 1.91971685126191e-06\n",
      "Steps : 512000, \t Total Gen Loss : 36.19025802612305, \t Total Dis Loss : 4.082253326487262e-06\n",
      "Steps : 512100, \t Total Gen Loss : 38.46273422241211, \t Total Dis Loss : 1.1503269888635259e-05\n",
      "Steps : 512200, \t Total Gen Loss : 37.981666564941406, \t Total Dis Loss : 1.250811806130514e-06\n",
      "Steps : 512300, \t Total Gen Loss : 39.042320251464844, \t Total Dis Loss : 2.774140284600435e-06\n",
      "Steps : 512400, \t Total Gen Loss : 41.18024444580078, \t Total Dis Loss : 1.5798061212990433e-05\n",
      "Steps : 512500, \t Total Gen Loss : 42.29206085205078, \t Total Dis Loss : 8.743089097151824e-07\n",
      "Steps : 512600, \t Total Gen Loss : 39.17123794555664, \t Total Dis Loss : 6.094252057664562e-06\n",
      "Steps : 512700, \t Total Gen Loss : 38.21653366088867, \t Total Dis Loss : 1.3979916730022524e-05\n",
      "Steps : 512800, \t Total Gen Loss : 38.26696014404297, \t Total Dis Loss : 5.006923856853973e-06\n",
      "Steps : 512900, \t Total Gen Loss : 41.75688552856445, \t Total Dis Loss : 2.8287258828640915e-06\n",
      "Steps : 513000, \t Total Gen Loss : 39.304630279541016, \t Total Dis Loss : 1.229478357345215e-06\n",
      "Steps : 513100, \t Total Gen Loss : 39.113407135009766, \t Total Dis Loss : 3.5949642551713623e-06\n",
      "Steps : 513200, \t Total Gen Loss : 39.232078552246094, \t Total Dis Loss : 1.2937689461978152e-06\n",
      "Steps : 513300, \t Total Gen Loss : 38.172508239746094, \t Total Dis Loss : 2.8544591259560548e-06\n",
      "Steps : 513400, \t Total Gen Loss : 38.0325927734375, \t Total Dis Loss : 1.5132542330320575e-07\n",
      "Steps : 513500, \t Total Gen Loss : 36.168548583984375, \t Total Dis Loss : 1.606766090844758e-05\n",
      "Steps : 513600, \t Total Gen Loss : 39.48461151123047, \t Total Dis Loss : 2.598138053144794e-06\n",
      "Steps : 513700, \t Total Gen Loss : 34.849365234375, \t Total Dis Loss : 7.586141236970434e-06\n",
      "Steps : 513800, \t Total Gen Loss : 38.240760803222656, \t Total Dis Loss : 5.273139322525822e-06\n",
      "Steps : 513900, \t Total Gen Loss : 38.700477600097656, \t Total Dis Loss : 7.609860404045321e-06\n",
      "Steps : 514000, \t Total Gen Loss : 36.059364318847656, \t Total Dis Loss : 4.346997957327403e-05\n",
      "Steps : 514100, \t Total Gen Loss : 37.69245147705078, \t Total Dis Loss : 7.447653842973523e-06\n",
      "Steps : 514200, \t Total Gen Loss : 34.971187591552734, \t Total Dis Loss : 2.7225423764321022e-05\n",
      "Steps : 514300, \t Total Gen Loss : 36.83476257324219, \t Total Dis Loss : 2.0647727069444954e-05\n",
      "Steps : 514400, \t Total Gen Loss : 34.7910270690918, \t Total Dis Loss : 1.74999404407572e-05\n",
      "Steps : 514500, \t Total Gen Loss : 35.82068634033203, \t Total Dis Loss : 1.8558257579570636e-05\n",
      "Steps : 514600, \t Total Gen Loss : 30.436508178710938, \t Total Dis Loss : 0.004933258984237909\n",
      "Steps : 514700, \t Total Gen Loss : 36.204166412353516, \t Total Dis Loss : 1.560640612296993e-06\n",
      "Steps : 514800, \t Total Gen Loss : 35.11316680908203, \t Total Dis Loss : 6.874779046484036e-06\n",
      "Steps : 514900, \t Total Gen Loss : 35.834381103515625, \t Total Dis Loss : 2.7955984478467144e-06\n",
      "Steps : 515000, \t Total Gen Loss : 35.910675048828125, \t Total Dis Loss : 1.0811891115736216e-05\n",
      "Steps : 515100, \t Total Gen Loss : 37.26213836669922, \t Total Dis Loss : 5.865258572157472e-06\n",
      "Steps : 515200, \t Total Gen Loss : 38.3848762512207, \t Total Dis Loss : 1.1017511951649794e-06\n",
      "Steps : 515300, \t Total Gen Loss : 36.97015380859375, \t Total Dis Loss : 8.032354514853068e-08\n",
      "Steps : 515400, \t Total Gen Loss : 35.10601043701172, \t Total Dis Loss : 6.628054052271182e-07\n",
      "Steps : 515500, \t Total Gen Loss : 33.626129150390625, \t Total Dis Loss : 5.188529030419886e-05\n",
      "Steps : 515600, \t Total Gen Loss : 37.0784797668457, \t Total Dis Loss : 2.9624054150190204e-05\n",
      "Steps : 515700, \t Total Gen Loss : 34.003143310546875, \t Total Dis Loss : 2.5732504582265392e-05\n",
      "Steps : 515800, \t Total Gen Loss : 31.343769073486328, \t Total Dis Loss : 0.00040606778929941356\n",
      "Steps : 515900, \t Total Gen Loss : 36.80389404296875, \t Total Dis Loss : 2.031665098911617e-05\n",
      "Steps : 516000, \t Total Gen Loss : 30.28380584716797, \t Total Dis Loss : 6.9974827056285e-05\n",
      "Steps : 516100, \t Total Gen Loss : 35.13975524902344, \t Total Dis Loss : 3.9537291740998626e-05\n",
      "Steps : 516200, \t Total Gen Loss : 35.09724426269531, \t Total Dis Loss : 2.437409602862317e-05\n",
      "Steps : 516300, \t Total Gen Loss : 33.84810256958008, \t Total Dis Loss : 1.0608329830574803e-05\n",
      "Steps : 516400, \t Total Gen Loss : 36.89569091796875, \t Total Dis Loss : 2.296837919857353e-05\n",
      "Steps : 516500, \t Total Gen Loss : 37.40119552612305, \t Total Dis Loss : 4.4571324906428345e-06\n",
      "Steps : 516600, \t Total Gen Loss : 35.721534729003906, \t Total Dis Loss : 4.51909409093787e-06\n",
      "Steps : 516700, \t Total Gen Loss : 39.62705612182617, \t Total Dis Loss : 1.4209648497853777e-06\n",
      "Steps : 516800, \t Total Gen Loss : 42.51203155517578, \t Total Dis Loss : 1.6693598809069954e-05\n",
      "Steps : 516900, \t Total Gen Loss : 40.19142150878906, \t Total Dis Loss : 3.5400324122747406e-05\n",
      "Steps : 517000, \t Total Gen Loss : 37.15694046020508, \t Total Dis Loss : 3.9656907802054775e-07\n",
      "Steps : 517100, \t Total Gen Loss : 36.88477325439453, \t Total Dis Loss : 1.977303554667742e-06\n",
      "Steps : 517200, \t Total Gen Loss : 36.0190544128418, \t Total Dis Loss : 1.8352038750890642e-06\n",
      "Steps : 517300, \t Total Gen Loss : 36.67552185058594, \t Total Dis Loss : 6.878119620523648e-06\n",
      "Steps : 517400, \t Total Gen Loss : 34.913291931152344, \t Total Dis Loss : 2.7745084025809774e-06\n",
      "Steps : 517500, \t Total Gen Loss : 39.74878692626953, \t Total Dis Loss : 5.542379312828416e-06\n",
      "Time for epoch 92 is 72.98184704780579 sec\n",
      "Steps : 517600, \t Total Gen Loss : 33.41710662841797, \t Total Dis Loss : 3.7752167827420635e-06\n",
      "Steps : 517700, \t Total Gen Loss : 37.98259735107422, \t Total Dis Loss : 4.674019237427274e-06\n",
      "Steps : 517800, \t Total Gen Loss : 37.312255859375, \t Total Dis Loss : 6.111555649113143e-06\n",
      "Steps : 517900, \t Total Gen Loss : 33.04298400878906, \t Total Dis Loss : 3.7655822779925074e-06\n",
      "Steps : 518000, \t Total Gen Loss : 33.649169921875, \t Total Dis Loss : 2.0254028640920296e-05\n",
      "Steps : 518100, \t Total Gen Loss : 35.79399490356445, \t Total Dis Loss : 7.000865025474923e-06\n",
      "Steps : 518200, \t Total Gen Loss : 37.10521697998047, \t Total Dis Loss : 7.946563528093975e-06\n",
      "Steps : 518300, \t Total Gen Loss : 34.76835250854492, \t Total Dis Loss : 2.846943971235305e-06\n",
      "Steps : 518400, \t Total Gen Loss : 36.80079650878906, \t Total Dis Loss : 1.3793676771456376e-05\n",
      "Steps : 518500, \t Total Gen Loss : 35.82426834106445, \t Total Dis Loss : 5.417585271061398e-06\n",
      "Steps : 518600, \t Total Gen Loss : 32.47913360595703, \t Total Dis Loss : 1.7161488358397037e-05\n",
      "Steps : 518700, \t Total Gen Loss : 34.543521881103516, \t Total Dis Loss : 4.957417331752367e-06\n",
      "Steps : 518800, \t Total Gen Loss : 34.40351104736328, \t Total Dis Loss : 3.5575283163780114e-06\n",
      "Steps : 518900, \t Total Gen Loss : 36.63292694091797, \t Total Dis Loss : 7.589289907627972e-07\n",
      "Steps : 519000, \t Total Gen Loss : 41.90558624267578, \t Total Dis Loss : 7.710003728789161e-07\n",
      "Steps : 519100, \t Total Gen Loss : 35.46268081665039, \t Total Dis Loss : 3.3179480851686094e-06\n",
      "Steps : 519200, \t Total Gen Loss : 36.645591735839844, \t Total Dis Loss : 3.5239017961430363e-06\n",
      "Steps : 519300, \t Total Gen Loss : 38.69676971435547, \t Total Dis Loss : 8.246303877967875e-06\n",
      "Steps : 519400, \t Total Gen Loss : 40.456512451171875, \t Total Dis Loss : 6.745556220266735e-07\n",
      "Steps : 519500, \t Total Gen Loss : 32.89344787597656, \t Total Dis Loss : 0.00015862559666857123\n",
      "Steps : 519600, \t Total Gen Loss : 35.158447265625, \t Total Dis Loss : 5.253251856629504e-06\n",
      "Steps : 519700, \t Total Gen Loss : 40.07198715209961, \t Total Dis Loss : 1.282467110286234e-06\n",
      "Steps : 519800, \t Total Gen Loss : 34.74629211425781, \t Total Dis Loss : 1.083043002836348e-06\n",
      "Steps : 519900, \t Total Gen Loss : 38.490081787109375, \t Total Dis Loss : 4.737870767712593e-06\n",
      "Steps : 520000, \t Total Gen Loss : 35.51433563232422, \t Total Dis Loss : 2.486759740349953e-07\n",
      "Steps : 520100, \t Total Gen Loss : 36.017459869384766, \t Total Dis Loss : 1.113796997742611e-06\n",
      "Steps : 520200, \t Total Gen Loss : 35.18543243408203, \t Total Dis Loss : 1.546653402328957e-05\n",
      "Steps : 520300, \t Total Gen Loss : 37.89183044433594, \t Total Dis Loss : 7.893028737271379e-07\n",
      "Steps : 520400, \t Total Gen Loss : 38.487918853759766, \t Total Dis Loss : 6.114370080467779e-06\n",
      "Steps : 520500, \t Total Gen Loss : 34.89804458618164, \t Total Dis Loss : 3.978096356149763e-05\n",
      "Steps : 520600, \t Total Gen Loss : 35.29552459716797, \t Total Dis Loss : 1.2066574299751665e-06\n",
      "Steps : 520700, \t Total Gen Loss : 34.62815475463867, \t Total Dis Loss : 1.8941435655506211e-06\n",
      "Steps : 520800, \t Total Gen Loss : 34.852970123291016, \t Total Dis Loss : 7.222126328088052e-07\n",
      "Steps : 520900, \t Total Gen Loss : 32.610809326171875, \t Total Dis Loss : 1.505524505773792e-05\n",
      "Steps : 521000, \t Total Gen Loss : 31.692108154296875, \t Total Dis Loss : 3.66193899026257e-06\n",
      "Steps : 521100, \t Total Gen Loss : 35.92962646484375, \t Total Dis Loss : 1.3207505844547995e-06\n",
      "Steps : 521200, \t Total Gen Loss : 36.93815231323242, \t Total Dis Loss : 6.597225592486211e-07\n",
      "Steps : 521300, \t Total Gen Loss : 38.896629333496094, \t Total Dis Loss : 9.087211196856515e-07\n",
      "Steps : 521400, \t Total Gen Loss : 37.52220916748047, \t Total Dis Loss : 6.746290182491066e-06\n",
      "Steps : 521500, \t Total Gen Loss : 35.30509948730469, \t Total Dis Loss : 3.3646595056779915e-06\n",
      "Steps : 521600, \t Total Gen Loss : 36.90199279785156, \t Total Dis Loss : 1.4490087778540328e-06\n",
      "Steps : 521700, \t Total Gen Loss : 36.69731903076172, \t Total Dis Loss : 7.880150224082172e-05\n",
      "Steps : 521800, \t Total Gen Loss : 38.48552703857422, \t Total Dis Loss : 2.0147945178905502e-05\n",
      "Steps : 521900, \t Total Gen Loss : 36.52817153930664, \t Total Dis Loss : 1.524258368590381e-05\n",
      "Steps : 522000, \t Total Gen Loss : 37.778865814208984, \t Total Dis Loss : 7.207536782516399e-06\n",
      "Steps : 522100, \t Total Gen Loss : 33.61084747314453, \t Total Dis Loss : 2.1871281205676496e-05\n",
      "Steps : 522200, \t Total Gen Loss : 32.765052795410156, \t Total Dis Loss : 6.140633195172995e-05\n",
      "Steps : 522300, \t Total Gen Loss : 36.84662628173828, \t Total Dis Loss : 5.198770668357611e-05\n",
      "Steps : 522400, \t Total Gen Loss : 34.56161117553711, \t Total Dis Loss : 0.00021785407443530858\n",
      "Steps : 522500, \t Total Gen Loss : 35.23278045654297, \t Total Dis Loss : 4.923268903667122e-08\n",
      "Steps : 522600, \t Total Gen Loss : 37.895179748535156, \t Total Dis Loss : 5.244276962912409e-06\n",
      "Steps : 522700, \t Total Gen Loss : 37.614830017089844, \t Total Dis Loss : 5.44460647233791e-07\n",
      "Steps : 522800, \t Total Gen Loss : 38.01557159423828, \t Total Dis Loss : 4.011366399936378e-06\n",
      "Steps : 522900, \t Total Gen Loss : 39.97541427612305, \t Total Dis Loss : 1.596788479218958e-07\n",
      "Steps : 523000, \t Total Gen Loss : 42.88774108886719, \t Total Dis Loss : 2.0968798253306886e-06\n",
      "Steps : 523100, \t Total Gen Loss : 36.185577392578125, \t Total Dis Loss : 0.0003165282541885972\n",
      "Time for epoch 93 is 73.77587580680847 sec\n",
      "Steps : 523200, \t Total Gen Loss : 37.89614486694336, \t Total Dis Loss : 6.700259973513312e-07\n",
      "Steps : 523300, \t Total Gen Loss : 38.63972473144531, \t Total Dis Loss : 2.1433129404613283e-07\n",
      "Steps : 523400, \t Total Gen Loss : 37.15103530883789, \t Total Dis Loss : 1.1566449984456995e-06\n",
      "Steps : 523500, \t Total Gen Loss : 37.456626892089844, \t Total Dis Loss : 4.289645971766731e-08\n",
      "Steps : 523600, \t Total Gen Loss : 40.07025146484375, \t Total Dis Loss : 2.604634119052207e-06\n",
      "Steps : 523700, \t Total Gen Loss : 38.85536193847656, \t Total Dis Loss : 4.67035192741605e-07\n",
      "Steps : 523800, \t Total Gen Loss : 38.29554748535156, \t Total Dis Loss : 7.789064966345904e-07\n",
      "Steps : 523900, \t Total Gen Loss : 39.18571090698242, \t Total Dis Loss : 1.1884150126206805e-06\n",
      "Steps : 524000, \t Total Gen Loss : 38.108211517333984, \t Total Dis Loss : 2.8253657546883915e-06\n",
      "Steps : 524100, \t Total Gen Loss : 40.10102081298828, \t Total Dis Loss : 1.037903416545305e-07\n",
      "Steps : 524200, \t Total Gen Loss : 34.93916702270508, \t Total Dis Loss : 3.2371274301112862e-06\n",
      "Steps : 524300, \t Total Gen Loss : 37.495513916015625, \t Total Dis Loss : 3.7229975191621634e-07\n",
      "Steps : 524400, \t Total Gen Loss : 36.23604202270508, \t Total Dis Loss : 9.241710472451814e-07\n",
      "Steps : 524500, \t Total Gen Loss : 38.71403503417969, \t Total Dis Loss : 1.6731726191210328e-06\n",
      "Steps : 524600, \t Total Gen Loss : 38.76703643798828, \t Total Dis Loss : 7.929631351544231e-08\n",
      "Steps : 524700, \t Total Gen Loss : 37.91729736328125, \t Total Dis Loss : 3.569181217244477e-07\n",
      "Steps : 524800, \t Total Gen Loss : 40.51310348510742, \t Total Dis Loss : 2.876592475331563e-07\n",
      "Steps : 524900, \t Total Gen Loss : 34.973876953125, \t Total Dis Loss : 5.352129619495827e-07\n",
      "Steps : 525000, \t Total Gen Loss : 39.943756103515625, \t Total Dis Loss : 5.872061137779383e-06\n",
      "Steps : 525100, \t Total Gen Loss : 34.47207260131836, \t Total Dis Loss : 3.1877291348791914e-06\n",
      "Steps : 525200, \t Total Gen Loss : 40.406829833984375, \t Total Dis Loss : 2.245187488369993e-06\n",
      "Steps : 525300, \t Total Gen Loss : 38.23161315917969, \t Total Dis Loss : 5.114190116728423e-06\n",
      "Steps : 525400, \t Total Gen Loss : 31.308792114257812, \t Total Dis Loss : 8.623951725894585e-05\n",
      "Steps : 525500, \t Total Gen Loss : 34.86932373046875, \t Total Dis Loss : 4.0071616240311414e-05\n",
      "Steps : 525600, \t Total Gen Loss : 30.712600708007812, \t Total Dis Loss : 7.905279926490039e-05\n",
      "Steps : 525700, \t Total Gen Loss : 34.17633819580078, \t Total Dis Loss : 4.451970016816631e-05\n",
      "Steps : 525800, \t Total Gen Loss : 37.60740661621094, \t Total Dis Loss : 1.4208734683052171e-05\n",
      "Steps : 525900, \t Total Gen Loss : 34.6612663269043, \t Total Dis Loss : 4.5469314500223845e-06\n",
      "Steps : 526000, \t Total Gen Loss : 29.398754119873047, \t Total Dis Loss : 8.171929039235692e-06\n",
      "Steps : 526100, \t Total Gen Loss : 33.68267059326172, \t Total Dis Loss : 3.0737832275917754e-05\n",
      "Steps : 526200, \t Total Gen Loss : 34.30589294433594, \t Total Dis Loss : 5.924668130319333e-06\n",
      "Steps : 526300, \t Total Gen Loss : 34.84687805175781, \t Total Dis Loss : 1.2125368584747775e-06\n",
      "Steps : 526400, \t Total Gen Loss : 35.982357025146484, \t Total Dis Loss : 3.581954615583527e-06\n",
      "Steps : 526500, \t Total Gen Loss : 33.49020767211914, \t Total Dis Loss : 1.1143497431476135e-05\n",
      "Steps : 526600, \t Total Gen Loss : 35.35468673706055, \t Total Dis Loss : 3.790659945934749e-07\n",
      "Steps : 526700, \t Total Gen Loss : 37.487693786621094, \t Total Dis Loss : 1.3926458450441714e-05\n",
      "Steps : 526800, \t Total Gen Loss : 35.8663330078125, \t Total Dis Loss : 6.156938979984261e-06\n",
      "Steps : 526900, \t Total Gen Loss : 41.191490173339844, \t Total Dis Loss : 1.1338189551679534e-06\n",
      "Steps : 527000, \t Total Gen Loss : 33.35121536254883, \t Total Dis Loss : 1.1264937711530365e-06\n",
      "Steps : 527100, \t Total Gen Loss : 37.65873336791992, \t Total Dis Loss : 1.004855903374846e-06\n",
      "Steps : 527200, \t Total Gen Loss : 32.983848571777344, \t Total Dis Loss : 4.287853982987144e-07\n",
      "Steps : 527300, \t Total Gen Loss : 34.085662841796875, \t Total Dis Loss : 1.620213697606232e-05\n",
      "Steps : 527400, \t Total Gen Loss : 34.85282897949219, \t Total Dis Loss : 5.290481112751877e-06\n",
      "Steps : 527500, \t Total Gen Loss : 33.68792724609375, \t Total Dis Loss : 9.883513484965079e-06\n",
      "Steps : 527600, \t Total Gen Loss : 35.798553466796875, \t Total Dis Loss : 5.26637222719728e-07\n",
      "Steps : 527700, \t Total Gen Loss : 38.77191925048828, \t Total Dis Loss : 4.84521751786815e-06\n",
      "Steps : 527800, \t Total Gen Loss : 34.79512405395508, \t Total Dis Loss : 5.4139791245688684e-06\n",
      "Steps : 527900, \t Total Gen Loss : 37.80339050292969, \t Total Dis Loss : 6.100317023083335e-06\n",
      "Steps : 528000, \t Total Gen Loss : 35.38609313964844, \t Total Dis Loss : 1.0235125955659896e-06\n",
      "Steps : 528100, \t Total Gen Loss : 33.564247131347656, \t Total Dis Loss : 1.6131348274939228e-06\n",
      "Steps : 528200, \t Total Gen Loss : 38.333824157714844, \t Total Dis Loss : 4.173289653408574e-06\n",
      "Steps : 528300, \t Total Gen Loss : 36.41045379638672, \t Total Dis Loss : 2.679648559933412e-06\n",
      "Steps : 528400, \t Total Gen Loss : 34.77243423461914, \t Total Dis Loss : 5.7293013924208935e-06\n",
      "Steps : 528500, \t Total Gen Loss : 38.130802154541016, \t Total Dis Loss : 4.231737420923309e-06\n",
      "Steps : 528600, \t Total Gen Loss : 32.02108383178711, \t Total Dis Loss : 9.798064866117784e-07\n",
      "Steps : 528700, \t Total Gen Loss : 33.447879791259766, \t Total Dis Loss : 6.68489738018252e-05\n",
      "Time for epoch 94 is 71.64926934242249 sec\n",
      "Steps : 528800, \t Total Gen Loss : 35.80156326293945, \t Total Dis Loss : 2.719009353313595e-05\n",
      "Steps : 528900, \t Total Gen Loss : 34.300498962402344, \t Total Dis Loss : 4.096774773643119e-06\n",
      "Steps : 529000, \t Total Gen Loss : 31.47640609741211, \t Total Dis Loss : 6.94406026013894e-06\n",
      "Steps : 529100, \t Total Gen Loss : 32.665672302246094, \t Total Dis Loss : 2.2631984393228777e-05\n",
      "Steps : 529200, \t Total Gen Loss : 34.372032165527344, \t Total Dis Loss : 1.406223418598529e-05\n",
      "Steps : 529300, \t Total Gen Loss : 33.748722076416016, \t Total Dis Loss : 1.792034163372591e-05\n",
      "Steps : 529400, \t Total Gen Loss : 38.98877716064453, \t Total Dis Loss : 2.5758919946383685e-05\n",
      "Steps : 529500, \t Total Gen Loss : 34.26283264160156, \t Total Dis Loss : 9.365580808662344e-07\n",
      "Steps : 529600, \t Total Gen Loss : 38.31018829345703, \t Total Dis Loss : 1.7039163822119008e-06\n",
      "Steps : 529700, \t Total Gen Loss : 41.06689453125, \t Total Dis Loss : 1.5292877719730313e-07\n",
      "Steps : 529800, \t Total Gen Loss : 35.985897064208984, \t Total Dis Loss : 2.5661040581326233e-06\n",
      "Steps : 529900, \t Total Gen Loss : 37.84776306152344, \t Total Dis Loss : 1.8190668527040543e-07\n",
      "Steps : 530000, \t Total Gen Loss : 34.423702239990234, \t Total Dis Loss : 5.102781415189384e-07\n",
      "Steps : 530100, \t Total Gen Loss : 38.629974365234375, \t Total Dis Loss : 9.71159082041595e-08\n",
      "Steps : 530200, \t Total Gen Loss : 33.15309143066406, \t Total Dis Loss : 6.665554792562034e-07\n",
      "Steps : 530300, \t Total Gen Loss : 36.144718170166016, \t Total Dis Loss : 1.6728442915336927e-06\n",
      "Steps : 530400, \t Total Gen Loss : 40.433868408203125, \t Total Dis Loss : 1.5109962987480685e-06\n",
      "Steps : 530500, \t Total Gen Loss : 29.275657653808594, \t Total Dis Loss : 0.0022926803212612867\n",
      "Steps : 530600, \t Total Gen Loss : 34.95225524902344, \t Total Dis Loss : 4.8405236157123e-06\n",
      "Steps : 530700, \t Total Gen Loss : 35.696876525878906, \t Total Dis Loss : 4.266725682100514e-06\n",
      "Steps : 530800, \t Total Gen Loss : 38.80176544189453, \t Total Dis Loss : 3.7619345221173717e-06\n",
      "Steps : 530900, \t Total Gen Loss : 38.28627014160156, \t Total Dis Loss : 8.24760900286492e-06\n",
      "Steps : 531000, \t Total Gen Loss : 40.12315368652344, \t Total Dis Loss : 1.3355111150303856e-06\n",
      "Steps : 531100, \t Total Gen Loss : 37.67192840576172, \t Total Dis Loss : 3.303911455532216e-07\n",
      "Steps : 531200, \t Total Gen Loss : 37.58320617675781, \t Total Dis Loss : 1.1975232609984232e-06\n",
      "Steps : 531300, \t Total Gen Loss : 37.490020751953125, \t Total Dis Loss : 2.881830369005911e-06\n",
      "Steps : 531400, \t Total Gen Loss : 37.51087188720703, \t Total Dis Loss : 1.5753803381812759e-06\n",
      "Steps : 531500, \t Total Gen Loss : 37.9397087097168, \t Total Dis Loss : 2.139015487045981e-05\n",
      "Steps : 531600, \t Total Gen Loss : 37.97587203979492, \t Total Dis Loss : 5.936477464274503e-06\n",
      "Steps : 531700, \t Total Gen Loss : 36.627532958984375, \t Total Dis Loss : 3.990644017903833e-06\n",
      "Steps : 531800, \t Total Gen Loss : 34.41399002075195, \t Total Dis Loss : 9.945704732672311e-06\n",
      "Steps : 531900, \t Total Gen Loss : 35.210166931152344, \t Total Dis Loss : 6.290737474046182e-06\n",
      "Steps : 532000, \t Total Gen Loss : 35.55113983154297, \t Total Dis Loss : 2.120050794474082e-06\n",
      "Steps : 532100, \t Total Gen Loss : 38.104225158691406, \t Total Dis Loss : 1.2166862688900437e-05\n",
      "Steps : 532200, \t Total Gen Loss : 35.73221969604492, \t Total Dis Loss : 1.2180407793493941e-05\n",
      "Steps : 532300, \t Total Gen Loss : 37.71292495727539, \t Total Dis Loss : 1.1236486898269504e-05\n",
      "Steps : 532400, \t Total Gen Loss : 35.92694854736328, \t Total Dis Loss : 5.987471467960859e-06\n",
      "Steps : 532500, \t Total Gen Loss : 34.52585220336914, \t Total Dis Loss : 2.704810185605311e-06\n",
      "Steps : 532600, \t Total Gen Loss : 36.75425720214844, \t Total Dis Loss : 7.658518370590173e-06\n",
      "Steps : 532700, \t Total Gen Loss : 32.348575592041016, \t Total Dis Loss : 0.00045391323510557413\n",
      "Steps : 532800, \t Total Gen Loss : 37.89133834838867, \t Total Dis Loss : 1.1147371026254405e-07\n",
      "Steps : 532900, \t Total Gen Loss : 30.703824996948242, \t Total Dis Loss : 1.1087670827691909e-05\n",
      "Steps : 533000, \t Total Gen Loss : 37.055091857910156, \t Total Dis Loss : 2.3075231183611322e-06\n",
      "Steps : 533100, \t Total Gen Loss : 36.05013656616211, \t Total Dis Loss : 3.7084289942868054e-05\n",
      "Steps : 533200, \t Total Gen Loss : 34.0815544128418, \t Total Dis Loss : 1.9104647435597144e-05\n",
      "Steps : 533300, \t Total Gen Loss : 35.257606506347656, \t Total Dis Loss : 2.5026827643159777e-05\n",
      "Steps : 533400, \t Total Gen Loss : 34.954952239990234, \t Total Dis Loss : 3.087025470449589e-05\n",
      "Steps : 533500, \t Total Gen Loss : 39.06122589111328, \t Total Dis Loss : 9.22456820262596e-07\n",
      "Steps : 533600, \t Total Gen Loss : 35.65587615966797, \t Total Dis Loss : 7.13951976649696e-06\n",
      "Steps : 533700, \t Total Gen Loss : 37.93134689331055, \t Total Dis Loss : 7.340947263401176e-07\n",
      "Steps : 533800, \t Total Gen Loss : 38.160247802734375, \t Total Dis Loss : 2.7709807000064757e-06\n",
      "Steps : 533900, \t Total Gen Loss : 36.900169372558594, \t Total Dis Loss : 1.4616698535974137e-06\n",
      "Steps : 534000, \t Total Gen Loss : 38.403079986572266, \t Total Dis Loss : 1.984434220503317e-06\n",
      "Steps : 534100, \t Total Gen Loss : 37.154685974121094, \t Total Dis Loss : 1.589439193594444e-06\n",
      "Steps : 534200, \t Total Gen Loss : 38.19147491455078, \t Total Dis Loss : 1.8429615238346742e-06\n",
      "Steps : 534300, \t Total Gen Loss : 37.806907653808594, \t Total Dis Loss : 5.176513241167413e-06\n",
      "Time for epoch 95 is 69.93745112419128 sec\n",
      "Steps : 534400, \t Total Gen Loss : 35.534603118896484, \t Total Dis Loss : 2.0603183656930923e-05\n",
      "Steps : 534500, \t Total Gen Loss : 33.07054138183594, \t Total Dis Loss : 4.9980833864538e-05\n",
      "Steps : 534600, \t Total Gen Loss : 37.14982986450195, \t Total Dis Loss : 2.3549013349111192e-05\n",
      "Steps : 534700, \t Total Gen Loss : 31.652450561523438, \t Total Dis Loss : 1.9575454643927515e-05\n",
      "Steps : 534800, \t Total Gen Loss : 33.27275848388672, \t Total Dis Loss : 1.5512458730881917e-06\n",
      "Steps : 534900, \t Total Gen Loss : 38.699790954589844, \t Total Dis Loss : 1.938034017712198e-07\n",
      "Steps : 535000, \t Total Gen Loss : 35.05609893798828, \t Total Dis Loss : 9.986559916796978e-07\n",
      "Steps : 535100, \t Total Gen Loss : 32.41363525390625, \t Total Dis Loss : 0.0002815934712998569\n",
      "Steps : 535200, \t Total Gen Loss : 33.41619110107422, \t Total Dis Loss : 2.464692988723982e-05\n",
      "Steps : 535300, \t Total Gen Loss : 34.29380798339844, \t Total Dis Loss : 3.5689538435690338e-06\n",
      "Steps : 535400, \t Total Gen Loss : 34.71208953857422, \t Total Dis Loss : 1.0801623830047902e-05\n",
      "Steps : 535500, \t Total Gen Loss : 34.647613525390625, \t Total Dis Loss : 2.310717491127434e-06\n",
      "Steps : 535600, \t Total Gen Loss : 37.369850158691406, \t Total Dis Loss : 4.223219889354368e-07\n",
      "Steps : 535700, \t Total Gen Loss : 39.369178771972656, \t Total Dis Loss : 3.592448410927318e-05\n",
      "Steps : 535800, \t Total Gen Loss : 33.19664764404297, \t Total Dis Loss : 1.7926419104696834e-07\n",
      "Steps : 535900, \t Total Gen Loss : 26.422908782958984, \t Total Dis Loss : 0.00043587436084635556\n",
      "Steps : 536000, \t Total Gen Loss : 35.302223205566406, \t Total Dis Loss : 1.236373918800382e-06\n",
      "Steps : 536100, \t Total Gen Loss : 33.75617980957031, \t Total Dis Loss : 2.4110349841066636e-05\n",
      "Steps : 536200, \t Total Gen Loss : 33.6445426940918, \t Total Dis Loss : 9.796194717637263e-07\n",
      "Steps : 536300, \t Total Gen Loss : 36.91395950317383, \t Total Dis Loss : 4.690650894190185e-05\n",
      "Steps : 536400, \t Total Gen Loss : 30.42323875427246, \t Total Dis Loss : 0.0004313823883421719\n",
      "Steps : 536500, \t Total Gen Loss : 32.24775695800781, \t Total Dis Loss : 0.00028062937781214714\n",
      "Steps : 536600, \t Total Gen Loss : 32.62285614013672, \t Total Dis Loss : 2.068666071863845e-05\n",
      "Steps : 536700, \t Total Gen Loss : 33.42987060546875, \t Total Dis Loss : 2.29023426072672e-05\n",
      "Steps : 536800, \t Total Gen Loss : 33.014442443847656, \t Total Dis Loss : 4.751994856633246e-05\n",
      "Steps : 536900, \t Total Gen Loss : 32.208866119384766, \t Total Dis Loss : 3.4945767311000964e-06\n",
      "Steps : 537000, \t Total Gen Loss : 31.59486198425293, \t Total Dis Loss : 1.2865795724792406e-05\n",
      "Steps : 537100, \t Total Gen Loss : 37.2895622253418, \t Total Dis Loss : 2.8149448553449474e-05\n",
      "Steps : 537200, \t Total Gen Loss : 37.195980072021484, \t Total Dis Loss : 1.0216564987786114e-05\n",
      "Steps : 537300, \t Total Gen Loss : 36.58334732055664, \t Total Dis Loss : 4.336289748607669e-06\n",
      "Steps : 537400, \t Total Gen Loss : 35.58638381958008, \t Total Dis Loss : 7.549942893092521e-07\n",
      "Steps : 537500, \t Total Gen Loss : 36.873233795166016, \t Total Dis Loss : 9.000625595945166e-07\n",
      "Steps : 537600, \t Total Gen Loss : 34.60794448852539, \t Total Dis Loss : 1.1619582437560894e-05\n",
      "Steps : 537700, \t Total Gen Loss : 33.7457275390625, \t Total Dis Loss : 7.194211775640724e-06\n",
      "Steps : 537800, \t Total Gen Loss : 33.5853157043457, \t Total Dis Loss : 1.0640064829203766e-05\n",
      "Steps : 537900, \t Total Gen Loss : 34.57073211669922, \t Total Dis Loss : 7.538180852861842e-06\n",
      "Steps : 538000, \t Total Gen Loss : 36.82265090942383, \t Total Dis Loss : 7.570446882709803e-07\n",
      "Steps : 538100, \t Total Gen Loss : 33.114906311035156, \t Total Dis Loss : 3.1229985779646086e-06\n",
      "Steps : 538200, \t Total Gen Loss : 36.627960205078125, \t Total Dis Loss : 7.561689585600107e-07\n",
      "Steps : 538300, \t Total Gen Loss : 36.06077575683594, \t Total Dis Loss : 2.1978888753437786e-07\n",
      "Steps : 538400, \t Total Gen Loss : 33.3259162902832, \t Total Dis Loss : 1.0348992873332463e-05\n",
      "Steps : 538500, \t Total Gen Loss : 34.60523223876953, \t Total Dis Loss : 3.85205839847913e-06\n",
      "Steps : 538600, \t Total Gen Loss : 35.08650207519531, \t Total Dis Loss : 3.6419428397493903e-06\n",
      "Steps : 538700, \t Total Gen Loss : 35.59012222290039, \t Total Dis Loss : 4.045666355523281e-05\n",
      "Steps : 538800, \t Total Gen Loss : 37.73775863647461, \t Total Dis Loss : 1.8834922229871154e-05\n",
      "Steps : 538900, \t Total Gen Loss : 32.704124450683594, \t Total Dis Loss : 4.465734036784852e-06\n",
      "Steps : 539000, \t Total Gen Loss : 36.13551330566406, \t Total Dis Loss : 8.316779712913558e-06\n",
      "Steps : 539100, \t Total Gen Loss : 33.977840423583984, \t Total Dis Loss : 2.3860451619839296e-05\n",
      "Steps : 539200, \t Total Gen Loss : 32.111305236816406, \t Total Dis Loss : 1.0041542282124283e-06\n",
      "Steps : 539300, \t Total Gen Loss : 34.265628814697266, \t Total Dis Loss : 1.9063808167629759e-06\n",
      "Steps : 539400, \t Total Gen Loss : 33.20100402832031, \t Total Dis Loss : 2.4797761852823896e-06\n",
      "Steps : 539500, \t Total Gen Loss : 33.41120910644531, \t Total Dis Loss : 8.149277164193336e-06\n",
      "Steps : 539600, \t Total Gen Loss : 35.52173614501953, \t Total Dis Loss : 1.8740433915809263e-06\n",
      "Steps : 539700, \t Total Gen Loss : 32.99274444580078, \t Total Dis Loss : 4.356629233370768e-06\n",
      "Steps : 539800, \t Total Gen Loss : 34.806251525878906, \t Total Dis Loss : 4.22666926169768e-06\n",
      "Steps : 539900, \t Total Gen Loss : 38.61798095703125, \t Total Dis Loss : 3.9376386666845065e-06\n",
      "Steps : 540000, \t Total Gen Loss : 34.25312805175781, \t Total Dis Loss : 7.10063375208847e-07\n",
      "Time for epoch 96 is 73.2005615234375 sec\n",
      "Steps : 540100, \t Total Gen Loss : 38.4875373840332, \t Total Dis Loss : 1.9428325970238802e-07\n",
      "Steps : 540200, \t Total Gen Loss : 35.234588623046875, \t Total Dis Loss : 4.0538787970945123e-07\n",
      "Steps : 540300, \t Total Gen Loss : 38.95427703857422, \t Total Dis Loss : 1.192045715470158e-06\n",
      "Steps : 540400, \t Total Gen Loss : 38.43714141845703, \t Total Dis Loss : 4.3825400553032523e-07\n",
      "Steps : 540500, \t Total Gen Loss : 34.11844253540039, \t Total Dis Loss : 5.802568011858966e-06\n",
      "Steps : 540600, \t Total Gen Loss : 32.33269500732422, \t Total Dis Loss : 2.4987562937894836e-05\n",
      "Steps : 540700, \t Total Gen Loss : 37.966880798339844, \t Total Dis Loss : 4.460751370061189e-06\n",
      "Steps : 540800, \t Total Gen Loss : 38.265472412109375, \t Total Dis Loss : 1.8838717323887977e-06\n",
      "Steps : 540900, \t Total Gen Loss : 38.661163330078125, \t Total Dis Loss : 1.1788793017331045e-06\n",
      "Steps : 541000, \t Total Gen Loss : 36.81104278564453, \t Total Dis Loss : 3.5181994917365955e-07\n",
      "Steps : 541100, \t Total Gen Loss : 39.8994140625, \t Total Dis Loss : 8.080028806034534e-07\n",
      "Steps : 541200, \t Total Gen Loss : 35.51398849487305, \t Total Dis Loss : 7.06374862602388e-07\n",
      "Steps : 541300, \t Total Gen Loss : 33.75883483886719, \t Total Dis Loss : 5.643217377837573e-07\n",
      "Steps : 541400, \t Total Gen Loss : 36.23971176147461, \t Total Dis Loss : 6.590848755649859e-08\n",
      "Steps : 541500, \t Total Gen Loss : 35.55883026123047, \t Total Dis Loss : 5.174582611289225e-07\n",
      "Steps : 541600, \t Total Gen Loss : 38.611392974853516, \t Total Dis Loss : 2.8568928200911614e-07\n",
      "Steps : 541700, \t Total Gen Loss : 34.65582275390625, \t Total Dis Loss : 1.2922497205636319e-07\n",
      "Steps : 541800, \t Total Gen Loss : 36.57378005981445, \t Total Dis Loss : 4.382958707083162e-08\n",
      "Steps : 541900, \t Total Gen Loss : 40.888423919677734, \t Total Dis Loss : 5.914285594599278e-08\n",
      "Steps : 542000, \t Total Gen Loss : 39.80309295654297, \t Total Dis Loss : 2.935436498319177e-07\n",
      "Steps : 542100, \t Total Gen Loss : 35.210052490234375, \t Total Dis Loss : 2.6313551870771335e-07\n",
      "Steps : 542200, \t Total Gen Loss : 38.41039276123047, \t Total Dis Loss : 1.7471752755682246e-07\n",
      "Steps : 542300, \t Total Gen Loss : 39.007118225097656, \t Total Dis Loss : 1.7995164114381623e-07\n",
      "Steps : 542400, \t Total Gen Loss : 37.52899169921875, \t Total Dis Loss : 1.2173097729828442e-06\n",
      "Steps : 542500, \t Total Gen Loss : 35.60664367675781, \t Total Dis Loss : 4.163580342719797e-07\n",
      "Steps : 542600, \t Total Gen Loss : 38.13401412963867, \t Total Dis Loss : 2.811064803154295e-07\n",
      "Steps : 542700, \t Total Gen Loss : 38.75975799560547, \t Total Dis Loss : 6.034091484252713e-07\n",
      "Steps : 542800, \t Total Gen Loss : 34.34070587158203, \t Total Dis Loss : 3.2692064451111946e-06\n",
      "Steps : 542900, \t Total Gen Loss : 38.598968505859375, \t Total Dis Loss : 2.533252825287491e-07\n",
      "Steps : 543000, \t Total Gen Loss : 40.96138381958008, \t Total Dis Loss : 1.2315673814100592e-07\n",
      "Steps : 543100, \t Total Gen Loss : 37.91999435424805, \t Total Dis Loss : 5.477502327266848e-07\n",
      "Steps : 543200, \t Total Gen Loss : 36.97330093383789, \t Total Dis Loss : 1.795019812789178e-07\n",
      "Steps : 543300, \t Total Gen Loss : 34.500450134277344, \t Total Dis Loss : 7.053540684864856e-08\n",
      "Steps : 543400, \t Total Gen Loss : 39.29559326171875, \t Total Dis Loss : 3.999765567641589e-07\n",
      "Steps : 543500, \t Total Gen Loss : 38.3298225402832, \t Total Dis Loss : 4.331029401782871e-08\n",
      "Steps : 543600, \t Total Gen Loss : 31.525550842285156, \t Total Dis Loss : 6.007015258546744e-07\n",
      "Steps : 543700, \t Total Gen Loss : 38.21382522583008, \t Total Dis Loss : 1.850819216997479e-06\n",
      "Steps : 543800, \t Total Gen Loss : 40.2523193359375, \t Total Dis Loss : 2.15975774153776e-06\n",
      "Steps : 543900, \t Total Gen Loss : 41.80580139160156, \t Total Dis Loss : 2.9409044145722874e-05\n",
      "Steps : 544000, \t Total Gen Loss : 35.21510696411133, \t Total Dis Loss : 4.120828634768259e-06\n",
      "Steps : 544100, \t Total Gen Loss : 35.80327606201172, \t Total Dis Loss : 5.232611783867469e-07\n",
      "Steps : 544200, \t Total Gen Loss : 38.216949462890625, \t Total Dis Loss : 1.3137660062056966e-06\n",
      "Steps : 544300, \t Total Gen Loss : 34.62846374511719, \t Total Dis Loss : 8.018149674171582e-05\n",
      "Steps : 544400, \t Total Gen Loss : 35.30806350708008, \t Total Dis Loss : 1.910522996695363e-06\n",
      "Steps : 544500, \t Total Gen Loss : 36.03321075439453, \t Total Dis Loss : 2.6718764274846762e-05\n",
      "Steps : 544600, \t Total Gen Loss : 38.300025939941406, \t Total Dis Loss : 6.448659860325279e-06\n",
      "Steps : 544700, \t Total Gen Loss : 36.95429229736328, \t Total Dis Loss : 3.315976186968328e-07\n",
      "Steps : 544800, \t Total Gen Loss : 36.90113067626953, \t Total Dis Loss : 1.1747566759368056e-06\n",
      "Steps : 544900, \t Total Gen Loss : 36.11517333984375, \t Total Dis Loss : 1.73503082123716e-07\n",
      "Steps : 545000, \t Total Gen Loss : 39.692657470703125, \t Total Dis Loss : 9.161194469697875e-08\n",
      "Steps : 545100, \t Total Gen Loss : 41.02558135986328, \t Total Dis Loss : 3.039478997379774e-07\n",
      "Steps : 545200, \t Total Gen Loss : 44.75670623779297, \t Total Dis Loss : 0.0003130141703877598\n",
      "Steps : 545300, \t Total Gen Loss : 39.5942497253418, \t Total Dis Loss : 1.489635934603939e-07\n",
      "Steps : 545400, \t Total Gen Loss : 39.35706329345703, \t Total Dis Loss : 8.437899623459089e-08\n",
      "Steps : 545500, \t Total Gen Loss : 41.90983963012695, \t Total Dis Loss : 2.1568381214365218e-07\n",
      "Steps : 545600, \t Total Gen Loss : 39.15913391113281, \t Total Dis Loss : 6.648205896908621e-08\n",
      "Time for epoch 97 is 71.60242795944214 sec\n",
      "Steps : 545700, \t Total Gen Loss : 41.362152099609375, \t Total Dis Loss : 6.506225957991774e-08\n",
      "Steps : 545800, \t Total Gen Loss : 40.61051559448242, \t Total Dis Loss : 1.3417669642024066e-08\n",
      "Steps : 545900, \t Total Gen Loss : 39.002201080322266, \t Total Dis Loss : 6.402098620128527e-07\n",
      "Steps : 546000, \t Total Gen Loss : 41.98774337768555, \t Total Dis Loss : 2.356531467739842e-07\n",
      "Steps : 546100, \t Total Gen Loss : 41.13846969604492, \t Total Dis Loss : 1.568979257626779e-08\n",
      "Steps : 546200, \t Total Gen Loss : 40.805179595947266, \t Total Dis Loss : 4.972784495294036e-08\n",
      "Steps : 546300, \t Total Gen Loss : 37.581993103027344, \t Total Dis Loss : 4.386943430745305e-08\n",
      "Steps : 546400, \t Total Gen Loss : 39.98004913330078, \t Total Dis Loss : 1.0813060669079277e-07\n",
      "Steps : 546500, \t Total Gen Loss : 42.090248107910156, \t Total Dis Loss : 4.0758926900252845e-08\n",
      "Steps : 546600, \t Total Gen Loss : 39.19285202026367, \t Total Dis Loss : 5.9343260971900236e-08\n",
      "Steps : 546700, \t Total Gen Loss : 38.07389450073242, \t Total Dis Loss : 2.7534898094927485e-07\n",
      "Steps : 546800, \t Total Gen Loss : 40.81163787841797, \t Total Dis Loss : 2.0555523860821268e-07\n",
      "Steps : 546900, \t Total Gen Loss : 40.748992919921875, \t Total Dis Loss : 1.2903538504360768e-07\n",
      "Steps : 547000, \t Total Gen Loss : 44.51486587524414, \t Total Dis Loss : 1.6418809423157654e-07\n",
      "Steps : 547100, \t Total Gen Loss : 40.73319625854492, \t Total Dis Loss : 2.6606887360003384e-08\n",
      "Steps : 547200, \t Total Gen Loss : 39.91304016113281, \t Total Dis Loss : 1.651769707677886e-07\n",
      "Steps : 547300, \t Total Gen Loss : 43.66292953491211, \t Total Dis Loss : 1.0074759160261237e-07\n",
      "Steps : 547400, \t Total Gen Loss : 40.283103942871094, \t Total Dis Loss : 8.482805924359127e-08\n",
      "Steps : 547500, \t Total Gen Loss : 40.47328186035156, \t Total Dis Loss : 1.589170750548874e-07\n",
      "Steps : 547600, \t Total Gen Loss : 39.57524871826172, \t Total Dis Loss : 2.809252208635371e-08\n",
      "Steps : 547700, \t Total Gen Loss : 42.08673858642578, \t Total Dis Loss : 1.054648635090416e-07\n",
      "Steps : 547800, \t Total Gen Loss : 35.87504577636719, \t Total Dis Loss : 1.0894672897165947e-07\n",
      "Steps : 547900, \t Total Gen Loss : 41.65597915649414, \t Total Dis Loss : 1.5795286856246094e-08\n",
      "Steps : 548000, \t Total Gen Loss : 39.6804313659668, \t Total Dis Loss : 1.1733367699662267e-07\n",
      "Steps : 548100, \t Total Gen Loss : 37.648311614990234, \t Total Dis Loss : 5.126088922224881e-07\n",
      "Steps : 548200, \t Total Gen Loss : 35.80502700805664, \t Total Dis Loss : 4.818593879463151e-07\n",
      "Steps : 548300, \t Total Gen Loss : 38.85081481933594, \t Total Dis Loss : 2.1189617882555467e-07\n",
      "Steps : 548400, \t Total Gen Loss : 38.39708709716797, \t Total Dis Loss : 1.733823893346198e-07\n",
      "Steps : 548500, \t Total Gen Loss : 38.812408447265625, \t Total Dis Loss : 6.96952469070311e-08\n",
      "Steps : 548600, \t Total Gen Loss : 47.90730667114258, \t Total Dis Loss : 7.543626452388708e-07\n",
      "Steps : 548700, \t Total Gen Loss : 43.721466064453125, \t Total Dis Loss : 7.21553590210533e-07\n",
      "Steps : 548800, \t Total Gen Loss : 41.265045166015625, \t Total Dis Loss : 3.3004030797201267e-07\n",
      "Steps : 548900, \t Total Gen Loss : 40.297760009765625, \t Total Dis Loss : 2.5161577354992914e-07\n",
      "Steps : 549000, \t Total Gen Loss : 46.21733093261719, \t Total Dis Loss : 1.4630503031298758e-08\n",
      "Steps : 549100, \t Total Gen Loss : 42.63838195800781, \t Total Dis Loss : 1.5002559905497037e-07\n",
      "Steps : 549200, \t Total Gen Loss : 47.29685592651367, \t Total Dis Loss : 5.073584361525718e-07\n",
      "Steps : 549300, \t Total Gen Loss : 44.698402404785156, \t Total Dis Loss : 6.458590178226586e-07\n",
      "Steps : 549400, \t Total Gen Loss : 44.015323638916016, \t Total Dis Loss : 3.880324584315531e-06\n",
      "Steps : 549500, \t Total Gen Loss : 43.07068634033203, \t Total Dis Loss : 7.328761739699985e-07\n",
      "Steps : 549600, \t Total Gen Loss : 43.889854431152344, \t Total Dis Loss : 1.1382231832612888e-06\n",
      "Steps : 549700, \t Total Gen Loss : 32.89448165893555, \t Total Dis Loss : 3.979850475843705e-07\n",
      "Steps : 549800, \t Total Gen Loss : 36.859928131103516, \t Total Dis Loss : 2.6703560251917224e-07\n",
      "Steps : 549900, \t Total Gen Loss : 34.320159912109375, \t Total Dis Loss : 3.215489778085612e-05\n",
      "Steps : 550000, \t Total Gen Loss : 36.83441925048828, \t Total Dis Loss : 4.434743914316641e-07\n",
      "Steps : 550100, \t Total Gen Loss : 37.85533142089844, \t Total Dis Loss : 1.6485323328652157e-07\n",
      "Steps : 550200, \t Total Gen Loss : 35.343841552734375, \t Total Dis Loss : 2.7840003440360306e-06\n",
      "Steps : 550300, \t Total Gen Loss : 37.55156707763672, \t Total Dis Loss : 9.751223615239724e-07\n",
      "Steps : 550400, \t Total Gen Loss : 33.61363983154297, \t Total Dis Loss : 2.184886625400395e-06\n",
      "Steps : 550500, \t Total Gen Loss : 35.1898193359375, \t Total Dis Loss : 3.2080051823868416e-06\n",
      "Steps : 550600, \t Total Gen Loss : 35.86845397949219, \t Total Dis Loss : 6.603265205740172e-07\n",
      "Steps : 550700, \t Total Gen Loss : 36.697723388671875, \t Total Dis Loss : 4.078546282926254e-07\n",
      "Steps : 550800, \t Total Gen Loss : 37.38636779785156, \t Total Dis Loss : 5.865749486133609e-08\n",
      "Steps : 550900, \t Total Gen Loss : 37.97035217285156, \t Total Dis Loss : 1.0232467317905503e-08\n",
      "Steps : 551000, \t Total Gen Loss : 35.32819747924805, \t Total Dis Loss : 4.522644871940429e-07\n",
      "Steps : 551100, \t Total Gen Loss : 36.60352325439453, \t Total Dis Loss : 2.0341973083759513e-07\n",
      "Steps : 551200, \t Total Gen Loss : 35.13374710083008, \t Total Dis Loss : 9.482324117016105e-07\n",
      "Time for epoch 98 is 72.2740044593811 sec\n",
      "Steps : 551300, \t Total Gen Loss : 35.8762092590332, \t Total Dis Loss : 1.0685914730856894e-06\n",
      "Steps : 551400, \t Total Gen Loss : 36.442840576171875, \t Total Dis Loss : 6.964574481571617e-07\n",
      "Steps : 551500, \t Total Gen Loss : 35.25401306152344, \t Total Dis Loss : 1.2086077276762808e-06\n",
      "Steps : 551600, \t Total Gen Loss : 37.84260559082031, \t Total Dis Loss : 1.3685631756743533e-06\n",
      "Steps : 551700, \t Total Gen Loss : 38.095184326171875, \t Total Dis Loss : 1.1616150459303753e-06\n",
      "Steps : 551800, \t Total Gen Loss : 33.98960494995117, \t Total Dis Loss : 3.7766358218505047e-06\n",
      "Steps : 551900, \t Total Gen Loss : 36.21105194091797, \t Total Dis Loss : 1.0664414276106982e-06\n",
      "Steps : 552000, \t Total Gen Loss : 34.96113967895508, \t Total Dis Loss : 8.897357872683642e-08\n",
      "Steps : 552100, \t Total Gen Loss : 35.19696807861328, \t Total Dis Loss : 1.3295127700985176e-06\n",
      "Steps : 552200, \t Total Gen Loss : 42.763763427734375, \t Total Dis Loss : 1.9056932387684355e-07\n",
      "Steps : 552300, \t Total Gen Loss : 36.36033630371094, \t Total Dis Loss : 3.181259671691805e-05\n",
      "Steps : 552400, \t Total Gen Loss : 35.5458869934082, \t Total Dis Loss : 3.5843748946717824e-07\n",
      "Steps : 552500, \t Total Gen Loss : 36.68155288696289, \t Total Dis Loss : 2.7812961889139842e-06\n",
      "Steps : 552600, \t Total Gen Loss : 39.15388488769531, \t Total Dis Loss : 2.0370152924442664e-06\n",
      "Steps : 552700, \t Total Gen Loss : 36.52450942993164, \t Total Dis Loss : 7.773400625410432e-07\n",
      "Steps : 552800, \t Total Gen Loss : 33.53333282470703, \t Total Dis Loss : 1.6283221384583157e-06\n",
      "Steps : 552900, \t Total Gen Loss : 35.73215103149414, \t Total Dis Loss : 4.321169058130181e-07\n",
      "Steps : 553000, \t Total Gen Loss : 31.838598251342773, \t Total Dis Loss : 1.3172584090170858e-07\n",
      "Steps : 553100, \t Total Gen Loss : 39.356224060058594, \t Total Dis Loss : 1.6281269381579477e-06\n",
      "Steps : 553200, \t Total Gen Loss : 36.97711181640625, \t Total Dis Loss : 8.091187737591099e-07\n",
      "Steps : 553300, \t Total Gen Loss : 37.32667541503906, \t Total Dis Loss : 1.640290065552108e-06\n",
      "Steps : 553400, \t Total Gen Loss : 38.790164947509766, \t Total Dis Loss : 5.965571290289517e-07\n",
      "Steps : 553500, \t Total Gen Loss : 35.964759826660156, \t Total Dis Loss : 1.166703259514179e-05\n",
      "Steps : 553600, \t Total Gen Loss : 34.194908142089844, \t Total Dis Loss : 3.106103179106867e-07\n",
      "Steps : 553700, \t Total Gen Loss : 39.345672607421875, \t Total Dis Loss : 1.5987409085482795e-07\n",
      "Steps : 553800, \t Total Gen Loss : 37.14236831665039, \t Total Dis Loss : 5.718121087738837e-07\n",
      "Steps : 553900, \t Total Gen Loss : 37.722469329833984, \t Total Dis Loss : 7.186023367466987e-07\n",
      "Steps : 554000, \t Total Gen Loss : 41.053131103515625, \t Total Dis Loss : 9.450785398712469e-08\n",
      "Steps : 554100, \t Total Gen Loss : 36.24264907836914, \t Total Dis Loss : 3.3133275678665086e-07\n",
      "Steps : 554200, \t Total Gen Loss : 29.814414978027344, \t Total Dis Loss : 6.437076081056148e-05\n",
      "Steps : 554300, \t Total Gen Loss : 38.361427307128906, \t Total Dis Loss : 4.640176484826952e-06\n",
      "Steps : 554400, \t Total Gen Loss : 37.20188522338867, \t Total Dis Loss : 1.2392438293318264e-06\n",
      "Steps : 554500, \t Total Gen Loss : 34.15320587158203, \t Total Dis Loss : 0.004525091033428907\n",
      "Steps : 554600, \t Total Gen Loss : 36.962738037109375, \t Total Dis Loss : 8.080109523689316e-07\n",
      "Steps : 554700, \t Total Gen Loss : 33.817935943603516, \t Total Dis Loss : 4.450148196610826e-07\n",
      "Steps : 554800, \t Total Gen Loss : 34.60392761230469, \t Total Dis Loss : 6.939613740541972e-06\n",
      "Steps : 554900, \t Total Gen Loss : 35.79280090332031, \t Total Dis Loss : 2.303767359990161e-06\n",
      "Steps : 555000, \t Total Gen Loss : 36.518733978271484, \t Total Dis Loss : 7.956689842103515e-06\n",
      "Steps : 555100, \t Total Gen Loss : 35.32783126831055, \t Total Dis Loss : 9.976109822673607e-07\n",
      "Steps : 555200, \t Total Gen Loss : 35.503204345703125, \t Total Dis Loss : 3.366925011505373e-05\n",
      "Steps : 555300, \t Total Gen Loss : 35.59723663330078, \t Total Dis Loss : 3.506027042021742e-07\n",
      "Steps : 555400, \t Total Gen Loss : 37.95465850830078, \t Total Dis Loss : 8.73084331942664e-07\n",
      "Steps : 555500, \t Total Gen Loss : 37.65065002441406, \t Total Dis Loss : 3.9621693304070504e-07\n",
      "Steps : 555600, \t Total Gen Loss : 38.56010437011719, \t Total Dis Loss : 5.137209882377647e-06\n",
      "Steps : 555700, \t Total Gen Loss : 36.36682891845703, \t Total Dis Loss : 3.2020048479353136e-07\n",
      "Steps : 555800, \t Total Gen Loss : 41.298377990722656, \t Total Dis Loss : 1.850856847340765e-06\n",
      "Steps : 555900, \t Total Gen Loss : 34.605072021484375, \t Total Dis Loss : 1.5575169527437538e-06\n",
      "Steps : 556000, \t Total Gen Loss : 35.560638427734375, \t Total Dis Loss : 1.6456109506179928e-06\n",
      "Steps : 556100, \t Total Gen Loss : 37.73796844482422, \t Total Dis Loss : 3.970751549786655e-07\n",
      "Steps : 556200, \t Total Gen Loss : 38.550437927246094, \t Total Dis Loss : 2.2426409032050287e-06\n",
      "Steps : 556300, \t Total Gen Loss : 37.19619369506836, \t Total Dis Loss : 1.0347538648147747e-07\n",
      "Steps : 556400, \t Total Gen Loss : 37.27158737182617, \t Total Dis Loss : 1.773451003828086e-06\n",
      "Steps : 556500, \t Total Gen Loss : 35.692832946777344, \t Total Dis Loss : 2.3424435369179264e-07\n",
      "Steps : 556600, \t Total Gen Loss : 39.65641784667969, \t Total Dis Loss : 2.0551314605654625e-07\n",
      "Steps : 556700, \t Total Gen Loss : 38.17086410522461, \t Total Dis Loss : 2.2528277554556553e-07\n",
      "Steps : 556800, \t Total Gen Loss : 37.78739929199219, \t Total Dis Loss : 9.93490289147303e-07\n",
      "Time for epoch 99 is 74.39669728279114 sec\n",
      "Steps : 556900, \t Total Gen Loss : 32.057281494140625, \t Total Dis Loss : 3.226027729397174e-06\n",
      "Steps : 557000, \t Total Gen Loss : 34.70915985107422, \t Total Dis Loss : 4.929120223096106e-06\n",
      "Steps : 557100, \t Total Gen Loss : 32.66744613647461, \t Total Dis Loss : 3.49514039044152e-06\n",
      "Steps : 557200, \t Total Gen Loss : 35.70439147949219, \t Total Dis Loss : 4.8657866500434466e-06\n",
      "Steps : 557300, \t Total Gen Loss : 34.49553680419922, \t Total Dis Loss : 5.043962119088974e-06\n",
      "Steps : 557400, \t Total Gen Loss : 32.71861267089844, \t Total Dis Loss : 5.629121005767956e-06\n",
      "Steps : 557500, \t Total Gen Loss : 32.93607711791992, \t Total Dis Loss : 2.993691396113718e-06\n",
      "Steps : 557600, \t Total Gen Loss : 34.055824279785156, \t Total Dis Loss : 3.7324489312595688e-06\n",
      "Steps : 557700, \t Total Gen Loss : 31.321969985961914, \t Total Dis Loss : 3.0748287827009335e-06\n",
      "Steps : 557800, \t Total Gen Loss : 32.943359375, \t Total Dis Loss : 0.00011948461178690195\n",
      "Steps : 557900, \t Total Gen Loss : 30.779294967651367, \t Total Dis Loss : 2.7443795261206105e-05\n",
      "Steps : 558000, \t Total Gen Loss : 34.54973602294922, \t Total Dis Loss : 5.334182333172066e-06\n",
      "Steps : 558100, \t Total Gen Loss : 30.243831634521484, \t Total Dis Loss : 2.6716412321547978e-05\n",
      "Steps : 558200, \t Total Gen Loss : 34.74851608276367, \t Total Dis Loss : 4.4522570533445105e-05\n",
      "Steps : 558300, \t Total Gen Loss : 33.78440856933594, \t Total Dis Loss : 9.135971595242154e-06\n",
      "Steps : 558400, \t Total Gen Loss : 33.31626892089844, \t Total Dis Loss : 7.2837538027670234e-06\n",
      "Steps : 558500, \t Total Gen Loss : 33.326377868652344, \t Total Dis Loss : 2.368364221183583e-06\n",
      "Steps : 558600, \t Total Gen Loss : 35.45018768310547, \t Total Dis Loss : 2.64245318248868e-06\n",
      "Steps : 558700, \t Total Gen Loss : 34.072723388671875, \t Total Dis Loss : 4.524340965872398e-06\n",
      "Steps : 558800, \t Total Gen Loss : 33.98256301879883, \t Total Dis Loss : 7.106103112164419e-06\n",
      "Steps : 558900, \t Total Gen Loss : 31.814085006713867, \t Total Dis Loss : 9.541644431010354e-06\n",
      "Steps : 559000, \t Total Gen Loss : 34.64548110961914, \t Total Dis Loss : 5.247851277090376e-06\n",
      "Steps : 559100, \t Total Gen Loss : 39.2674560546875, \t Total Dis Loss : 2.0702616438939003e-06\n",
      "Steps : 559200, \t Total Gen Loss : 36.702980041503906, \t Total Dis Loss : 5.040225232733064e-07\n",
      "Steps : 559300, \t Total Gen Loss : 33.95798110961914, \t Total Dis Loss : 7.054838988551637e-07\n",
      "Steps : 559400, \t Total Gen Loss : 36.643402099609375, \t Total Dis Loss : 3.0231902314881154e-07\n",
      "Steps : 559500, \t Total Gen Loss : 34.253971099853516, \t Total Dis Loss : 9.526046051178128e-05\n",
      "Steps : 559600, \t Total Gen Loss : 37.407344818115234, \t Total Dis Loss : 2.0757532581683336e-07\n",
      "Steps : 559700, \t Total Gen Loss : 31.900785446166992, \t Total Dis Loss : 9.951386346074287e-07\n",
      "Steps : 559800, \t Total Gen Loss : 34.68558120727539, \t Total Dis Loss : 1.0307275033483165e-06\n",
      "Steps : 559900, \t Total Gen Loss : 36.26686477661133, \t Total Dis Loss : 1.5347081898653414e-06\n",
      "Steps : 560000, \t Total Gen Loss : 37.75469207763672, \t Total Dis Loss : 6.128811378403043e-07\n",
      "Steps : 560100, \t Total Gen Loss : 30.312355041503906, \t Total Dis Loss : 5.81866770517081e-05\n",
      "Steps : 560200, \t Total Gen Loss : 34.00811767578125, \t Total Dis Loss : 3.294357611594023e-06\n",
      "Steps : 560300, \t Total Gen Loss : 35.840946197509766, \t Total Dis Loss : 3.869135696277226e-07\n",
      "Steps : 560400, \t Total Gen Loss : 32.112884521484375, \t Total Dis Loss : 2.734149120442453e-07\n",
      "Steps : 560500, \t Total Gen Loss : 32.50550842285156, \t Total Dis Loss : 8.724455983610824e-05\n",
      "Steps : 560600, \t Total Gen Loss : 38.76012420654297, \t Total Dis Loss : 7.40957830203115e-06\n",
      "Steps : 560700, \t Total Gen Loss : 40.54744338989258, \t Total Dis Loss : 0.00013858461170457304\n",
      "Steps : 560800, \t Total Gen Loss : 38.767852783203125, \t Total Dis Loss : 9.947299304258195e-07\n",
      "Steps : 560900, \t Total Gen Loss : 43.314117431640625, \t Total Dis Loss : 2.0763228292253189e-07\n",
      "Steps : 561000, \t Total Gen Loss : 38.9083251953125, \t Total Dis Loss : 3.3952358080568956e-06\n",
      "Steps : 561100, \t Total Gen Loss : 38.65211868286133, \t Total Dis Loss : 2.3630421708276117e-07\n",
      "Steps : 561200, \t Total Gen Loss : 37.93971633911133, \t Total Dis Loss : 1.3450565461425867e-07\n",
      "Steps : 561300, \t Total Gen Loss : 39.80630874633789, \t Total Dis Loss : 6.486845904873917e-08\n",
      "Steps : 561400, \t Total Gen Loss : 39.694793701171875, \t Total Dis Loss : 3.577028735435306e-07\n",
      "Steps : 561500, \t Total Gen Loss : 38.94641876220703, \t Total Dis Loss : 1.7164698817850876e-07\n",
      "Steps : 561600, \t Total Gen Loss : 41.339786529541016, \t Total Dis Loss : 6.703558483422967e-06\n",
      "Steps : 561700, \t Total Gen Loss : 39.90312957763672, \t Total Dis Loss : 1.5146788712172565e-08\n",
      "Steps : 561800, \t Total Gen Loss : 39.43583679199219, \t Total Dis Loss : 3.524822389522342e-08\n",
      "Steps : 561900, \t Total Gen Loss : 39.06839370727539, \t Total Dis Loss : 6.703503885319151e-08\n",
      "Steps : 562000, \t Total Gen Loss : 40.08830261230469, \t Total Dis Loss : 9.842621295774734e-08\n",
      "Steps : 562100, \t Total Gen Loss : 32.74999237060547, \t Total Dis Loss : 1.3714574151890702e-06\n",
      "Steps : 562200, \t Total Gen Loss : 33.92716979980469, \t Total Dis Loss : 1.7392117115377914e-06\n",
      "Steps : 562300, \t Total Gen Loss : 34.386817932128906, \t Total Dis Loss : 1.8986751229022047e-06\n",
      "Steps : 562400, \t Total Gen Loss : 32.10498046875, \t Total Dis Loss : 0.003273341339081526\n",
      "Steps : 562500, \t Total Gen Loss : 36.313194274902344, \t Total Dis Loss : 1.8447800584908691e-06\n",
      "Time for epoch 100 is 70.37886071205139 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f7408c1dad0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러 발생 확률 : 0.448\n"
     ]
    }
   ],
   "source": [
    "detect_error = []\n",
    "\n",
    "for i in range(len(bol_test_labels)):\n",
    "    if tf.cast(bol_test_labels[i], dtype = tf.int32) != gt_labels[i]:\n",
    "        detect_error.append(i)\n",
    "        error_rate = len(detect_error)/len(bol_test_labels)\n",
    "        \n",
    "print('에러 발생 확률 : {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABYCAYAAABWO7HcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARjklEQVR4nO2cyZIkSVKGPzXzLZbMrK3phgYGRKY5wIgwZx6FMxd4Al6EJ+AAN07wFFzmMJcWQQaYmd6qqyqXWHwzMw6qFuGZXRmRVFaTOUKqiEtERni4m/+m9qvqr1YlKSWe7HZzDz2Ax25PAB2xJ4CO2BNAR+wJoCP2BNARewLoiD0agETkhYj8i4isReS/ROSvH3pMAMVDD2Bi/wD0wKfAz4F/FZFfpJR++ZCDkseQSYvIAngH/Cyl9KV99o/Ab1NKf/+QY3ssS+zPgJDBMfsF8BcPNJ6dPRaAlsDFjc8ugJMHGMs1eywArYDTG5+dAlcPMJZr9lgA+hIoROSLyWd/CTwoQcMjIWkAEflnIAF/g0axfwP+6qGj2GPxIIC/A2bAd8A/AX/70ODAI/Kgx2qPyYMepT0BdMSeADpiTwAdsSeAjtjBal5EDoY4AU5q8A7wEAUi0HXQ9x9xlD+ypZTktu/uJ3c48GcFroDASARCguhR9KId4V53eVC7F0BSOuY/O0XmicvtOWNMtBHSO1S82KAKzxoF6nfQ7gVQCon11x1SQx8TIUIaUUAyKLc5bwFUqHflI9n5jyh3PZhJH+OgnTlgjj5kjwKT7LMCrdVvetASrddb+83Wfp/DRuL/DKgfj4N2dwA6e83gHLMOVXyy92QA043XB7aPB9Bwy+e3Pehw4DePyH5c0b5DQfgdJWj4sQHKYf532O6fSbuPcpVHawc9yNeQEsTsCRNvcA1IAUUJTsAljdAOCBFisqTR/t7ZDV5K+e98jtz4G7uo3Ph+atPgkJNUb9/1N3531yBidhCgk89hDLBao2F4vb/Z7CdQv4AXz6AuYF5ACTQJ1p0eFxG2Ea5aiEHHnJLmShL1GAYIAeWraA8W0fCfwWuAegJSzsydvR8MiAFY2LnP7Lvf2LmV3aMFRu689A8CtHgG3QgrbzfNiV2CVKiXtGsYBboBKoGFg22A7QjNCTQzmM/NAdPEI/OzBvOwTOYe4gjjRkGNAVIJycM4QsrnJq57o9WDNHZkbxknwDp0Fmf2/sq+/1CAXn4G6xHerCHlWWp1gEGg38Kb1xBb6L6H2sPpDILT40//HJ6fQnWqD7gOtuQ8xEIPRI8UbLk5GAe4uoShh77bA7W6hLFDPSEnpTmHEpvAhQGQvb41MCp7rYFP7Lwv0ST2QwF6t1EPSmv2Cd1Kby4liOhDxBEYdTmuk1b1SeDNr6B/DZ88g6qBxXMdoCxhcNCX0It6IIVNtICvYFmpp8WgrynCcgthgDjoZyHofdnuxzzMdALac4hb9pxT2Lkj+2z/DkX0UYBCrq2yK6+A873XjpOvArCZuOzbNWwETl5CsYTlT8GfQrmEVmDjITr1xmSehKh8Uhc6ASLGW0knIkYYgi7XPt/UiDgl5bw+wFBD2th1MyidgTktbe4DUHsFqQO+R9d3tR9MDg6HAkILjAm+udTZ/+MtzJ/BYgvFc/CvoGhgKPcAJa/eMq70QTdRo6Q4aJy+L4I+m3cKVBTjN1HPSwKLz3Ry20+M54zEZYB+VE/s/x3SEd3qIEBjZv2VnZnYoR64vVDPlvl03YP0ENf6Wp6p54wzSA4Kzy6UJ9H0oO/1AeOg4DiBooDCIpdH04yYdBKipRTegfMwK/Q6fqkA9QMwKkDY31Idj/iHM+kcOpNenMxF7Av1TE3te36+RAPKS3u/BNwlnP8S+gV0Z1D9FJpPoXxZIbUnBEES+BIS+vTDu4FxFYiXGiwCGkXDAmIFYa5LaqiU4KPTcbukkXU0GSb0Numl5bbuKAJHvh7Z6zRcf7/wUItyzm1emmklmwdcgLDWqCUBykuollAuHYJjFNFoHMH1Cd9FNldC+xb6C13ydZZ4W4iN8tKYYPDQG6eFqIdHvcuNymEyqtcmOO4+RwHasM8TJjKEAF+cwYsKvvweLsfrPJ7tCl2dW+AM+AKlsQQsIrgeqndQCvjgofa0YUASVAl8G/CrgctfJd59BW0CSfATFKTUAEuIn4D/3OHnjk0Z6cvIhYc2qD6OZfaS1Ku6TiNuGrhfHuSLG1n/BIH5suFk5hjfbulvmYqcx+XoGrzyQrKwzQjpSnknEZAyUbqAE01X3BDxXUI6JdXt5JouQR0g9jCuwV0l3EWikgSN8hZi3pKU/MUrPxVJPTQvkA8GaHYKoYXt1XVwRODZZy95cVZz/ptf87Z/n7CztxHNe1pbDqlVV5cR0tcQX0NRtfgCZidQWumSOcIPevtz9lXFUuBZYaXQOUQSqQ0UnwInEF/AUMBYwOiMl8TEzwRVXo73AWiI18uCa9+FQD8G4pF1LMAr4CypV5SiSWYMFl2iRiufwI/gSrQccBqRSgPjFP39iPYDYoJ6AFdBWYOvBOeFMWh9V/eRwe7pcskxKu85p8exKHwUoC4XdTdBEOjDSDs4jTQHzAGfAc+BWYJCoKg0zLZBJ6GNCo53BlBUfikSNKL78J6jFUSLpmXbBLMBFhWcNTCrhbry9FHoO5j1A2NKNB58zjeMc0KlpZC7A0LHo1gmsQqN7RVQwnzhWVb++k0KdMosPciNiwaokyZlzkNT7y9/hcaCAuWFagNlsgzbXZesT9lLT2OC/w4w38CLCGd95KxPjDNhqEB8wjdQLJSD/EQJkE4JW+5Q0R8GaNqKKdkXgjWUtVB7QaaO6u17Y2af9Gel3SiNe8kmSzy5bZaFgnFQ3pj2HPP7md0mK7lvE2wHi0YOvCTikIgVpKVpebV6SpZaxC6Wpo2CDwWoeaFRos+jm7HTa9brCy7FE7PQs0Rj+R+iJPEOinMoWx1Hj5KsD3Cx1mW1RRsbudYUe/gyQHcJYh7QJgWxtWf61ObgjX32a+D1CuoWThxUBYRaC+mts+hZaA7m0QgaR/WiewHkKpMgsguYWpeAbhxpNXboVM2ABcgJu3UhK33oXHTn5KwN+mAt+9qxmZw3Rhg6LUGC8VTPvgivbHy1AbpCNShvcsrcg7TGNZ2CE5MSdMp5y8cQzAasiJyuCTvOo4fo8a6nnkH4I5CFgsorkBcQL6FbKccISmE96jUZnIlORsREyxEuL0xKiVrLrYFv7fcnBujCwHnHni4dBsJGc6CxVC4b0KWWy5SYz7sPQN7ZRoSJ94iFyMttVL4wkcvXWj8VokWgCIQGqJQnXNIHAp31fGTPyrXc1sBcRaurWrgaFYjc6r+w14KdfofVoMpZli0HD+Nck8QsqexUA+4mTR8EaF5bfTMYQF5F+tLBf/y2J16a7NDAsoGqtFBegp/BxQtoe/juNcwH+AMDpLWHzeVJzrZz8dsYUGUPVQ9fAa9RDssKaoUusQxcNkEv+N25Kp51Ba5QEW4HULAK4d4k7Y3IvEoLvtQst/CqBsYKihr8wroaFhl8oZlq1UA8hfBWI02ebTcBZSotjwZCzbXSb7dRJKdla/baV2fn1Qass/O6hCaK1mrJmnjEQjwfw4O8UU6p4FS1RojCQTvXuqo+VQ8SS8ZiUmerAzQLHVxrnYW8h6G06/eTQeZiZYN6x8iegG9G5Pf9+4QlmrEXdt1t0muemOdHMZ5KmoDeJUk8ClBRaGnQNFYWjUqcXQJq/T5VylOhN/fFZKMIfdKOxPxUualdacj+jn1X5302Apf8cF/DITt9Bp+fweVr2G5tXCVcWdIUUS6Kllnn/t29AHLOlNbSaphB2zvtqN5UWFqbnCp/KXcO7eapBLy2fXyEbq2i/rsjg4pc5xXg6JpYLOH3fh+6K9hs1etTAZve+CanK5U+i6SPwEHvLmAYYbPSqORtBorCOg5RSwcP1KVGvSJrylETM6xxGLbq9kfkl/faySvVsmsjqq+/0oJ0ajGq1jyaRv1sDmMDo7cw79gVsmmtisKtSt9dAep7veCwVSBSrZqKd8o/sO84gHV8ba0nS+cl2cDD+0W1bL40DcdOiEakzmkDcvECZsHquW9++PsQdTJD0ks0lj0PXuWO5M2Lpt2N+3pQVbGLqWKhOyeKlfXFQlIgLr5VAj+Z7UGQUqObC4fTenHwJz/XTm4YrHG4Uu5bnmjJIRUMb2C8Ul7Mz5ptvdZ04rxTnqvXWvzOnmvvLeUObmJfVN9ha85hDvIWnZwpc3mGJ16TTOsNvYXRau9VYqQtcnvUKESXZdNomzoUKnR1o6UQNbvWc0CX0PuSvHaEiy1sggInvfXXrOuaTKATa8ckf3MkHwCQL7Qil1Llh26y4dJZYy9M4nCypTf0elRBeWtWaAki6x8+2LNK91qXLcQrdnWSj9C3SrLlXMFqW1323Xu47M0W3raT5X4FZQevTq3r0ipPlZYkptywvA9AWTtO2Q0nMXfnEBa1pABK6ygk44RO/67tNyXW5CvUM11QaXVZaRTsWytUA2w7ex81my5qTTZzknczqCWu11aSOx9ePy9ksknComu8L0B508Auv8+bBKwl7JwuKdBUHosWA0rKY6su3STths7RqNLPrIwI8LyCZzP4vtNlsh0UlFUPY6/XKCs9lksNArmwPaQn+1r3MMVaJ6Me9Hp9Z3uaShjkOE8f/qcIg67b6X5nXysYpf1yGExf6c2Lcu0zA5npjPuV6sIRzZWkUN7qUd4YA7wZoHP6EAEb/KDhOPTKGatOuWwMxx8sWF622igX+UJb3H1j8z1yVE8/DtA0lTXPKQqtsXKSuN2y21ggaLRxhaYDYrvPilbBTlOAnGXMLWxbeNsrKHR2rxnqit0+ox7W9t0dkqloZc+40QmdNdp57adS5X0BKm0sYp0IX6P7d6w7iai7RksOnUU5yaKUCVSrK0hbJdfk9gAl9jVY6rm+jLN8eO2pjz/QNQvAW6WBdqmTSFYm7riv8nAUw8Ru04G8EV4wbUKyPmQygsvMmQ8DaOh1CWYw3PVTdBDTh8+l/U27S/l98/xWc6BxSlw3e+IH7DBAjfa0GWynxVt24ln5XL0nb6kL0fIe232RMsEHmFu/ObQQO20UZkCqCpoC1u/zmI9lAS3u4uQo7na/w1EsGZFZJNvtpREj5mhJYX6F3S4xQbNokmbYklR4G+P1OsqZZ8odZ/SDLD/DZOnvPj9iBwG63Ghukr5hv0nSup5bU6t2OZJYYlnuO5euUJBeem0MnoiG728nW+ay2jfdRnfMchJ8hw1iannJ5mfIO87uC9Bo7eFrO0mzpjvdPSpc4ySPRblBl6hHE7VlY149AaM3GeVayBXbH5Rb3zeWwl3VQABKdvu3E5ocXts3eMSO9uazKL8r8PLO1Gk7wu83loszbdpDf85ug2eB7qn2a/jPi/34Lrd6TM17mC21Ou9a9nH+f2sOnCWXJcqjgwl7dwX4cKmBFXUZGEvRd6q5LStsr2AI0PX6WZH21fMYVR9ej9oTmw6uavT8bmukjmk77Z7kp7vwncCisPRhOMKz1m0hWYfEiuhUsNdmjyB1mKRNR9l1VHP7x/Ybi9eNCCR9wDFojuQiFLY0JGltlZJmt+sb4buZQTPXDDoDlCL0NyVFWyfitLiNETY5+bxl/CIaiWOwf2Scl2ue8FxGfShAyRkJZx1levNBM+2Uv+tsJToVqxpgvoBypktm6ODrdyrZTm1rKl/II8l7In8wGB1HTFq1+6QNxNzb/4HZ0k/95BkyGeWdU/cN8zsSzhcfrUp2pq2IFoIpKWDe6abJCj2WlW7dDaPmRZcb9aYpy2av24Wmm1v+psMxd9lEXe2v7PMpQFOVQSwnu3YtZyXURNM6ZAcBOktaHrQ2YJegCSbG2cw0jXpIbbsoCpM36gGe1455IXSzQBHVs6oS6hPNrGNnG5miFraI9dsiu3/8Uoo1DEweSQZQXvnZCbyo954U+91jOB1PjLpZQjBaKNSRdhP2oQCV7DNzDKS8vy/aPywpgg5i5my7iXlQEaFEqESIJuZ7k0iq2kh9tN8kHXhy2qqJwTq26LUz7Tlbzh27FbQrqbL8UnmdiLw/MT9DxsEJVE7HspbjwfHp/w86Ynesaf//2hNAR+wJoCP2BNARewLoiD0BdMT+B2e31Q5ruei8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i  in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(test_data[detect_error[i]].reshape(32, 32, 3))\n",
    "    plt.title(gt_labels[detect_error[i]])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38424483 0.36597583\n",
      "0.13732661 0.13910724\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annomaly Detection 쪽에서 성능이 그다지 뛰어나지 않은것 같다. 실제 현업에서 annomaly detection을 하고 있는 박진우님께서도 GAN으로 성과를 내지 않는다고 하셨다...ㅠㅠ    \n",
    "그럼에도 GAN의 쓰임새가 정말 무궁무진하다는 것을 다시금 깨닫게 해준 노드였다ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
