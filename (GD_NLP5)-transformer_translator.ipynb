{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer모델을 이용한 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.\n",
      "Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\n"
     ]
    }
   ],
   "source": [
    "korean_path = os.getenv('HOME') + '/aiffel/transformer/korean-english-park.train/korean-english-park.train.ko'\n",
    "english_path = os.getenv('HOME') + '/aiffel/transformer/korean-english-park.train/korean-english-park.train.en'\n",
    "\n",
    "with open(korean_path, 'r') as f:\n",
    "    korean = f.read().splitlines()\n",
    "    \n",
    "print(korean[4])\n",
    "\n",
    "with open(english_path, 'r') as f:\n",
    "    english = f.read().splitlines()\n",
    "\n",
    "print(english[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(korean, english)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \"\", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(cleaned_corpus):\n",
    "    kor_corpus = []\n",
    "    eng_corpus = []\n",
    "    for sentence in cleaned_corpus:\n",
    "        sentence_ko = preprocess_sentence(sentence[0])\n",
    "        sentence_en = preprocess_sentence(sentence[1])\n",
    "        kor_corpus.append(sentence_ko)\n",
    "        eng_corpus.append(sentence_en)\n",
    "    \n",
    "    return kor_corpus, eng_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_corpus, eng_corpus = preprocessing(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentencepiece를 이용한 Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer1(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"ko\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write('{}\\n'.format(row))\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer2(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"eng\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write('{}\\n'.format(row))\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "tokenizer1 = generate_tokenizer1(kor_corpus, VOCAB_SIZE)\n",
    "tokenizer2 = generate_tokenizer2(eng_corpus, VOCAB_SIZE)\n",
    "tokenizer2.set_encode_extra_options('bos:eos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12b1605e9934c42a060bd1ae1ef11ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=78968.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68521"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm   # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for a in tqdm(range(len(cleaned_corpus))):\n",
    "\n",
    "    src_tokens = tokenizer1.encode_as_ids(kor_corpus[a])\n",
    "    tgt_tokens = tokenizer2.encode_as_ids(eng_corpus[a])\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67835 686 67835 686\n",
      "(67835, 50)\n",
      "(67835, 50)\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(len(enc_train), len(enc_val), len(dec_train), len(dec_val))\n",
    "print(dec_train.shape)\n",
    "print(enc_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 2\n",
    "D_MODEL = 512\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "DROPOUT = 0.3\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=DROPOUT,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 train step 커스터마이징하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['오바마는 대통령이다', '시민들이 도시 속에 산다.', '커피는 필요없다.', '일곱명의 사망자가 발생했다.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 학습시키기 및 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0038/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084029c3f5549ec990bb6609f73a9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obamas presidents presidents presidents presidents presidents .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: The Dow is the Dow .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Thats the films\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The death toll in the death toll in the deaths of the deaths .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40f66cf913d47ebad0477ce6f85ce4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obama is a presidential race .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: The city of the citys city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Theyre not going to theyre .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The deadly death tolls were dead in the death toll .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdbdd1e92b1492194916d0b4742e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obama is the first AfricanAmerican president in the presidential election .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: The urban urban urban city of urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban urban\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: The change is not trying to get out .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The death toll was the death toll in the death of the deaths .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e12942b7874b1f9ee94256beae5816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obama is the first of the Obamas inauguration .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: One of the mountainous regions to the mountainous region .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: No one need to be needed .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The sevenstory fatalities were fatalities in the death toll .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae0159518b84bb69c9646b21b9ceb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obama is the first of the Obamas president\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: One of the city of the city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Maybe needed .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The death toll was the deadliest death toll in the deaths .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c142820bce49d98f40c39d8cdfa2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: I think President Obama is the President\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: One of the city was business .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: No need need to need .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: Seven seven people were killed and seven others were killed .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4adc59d6c04236999eb82dfa237d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obama is the President Obamas .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: Mass city of the mountain city of the mountain city of the mountain city of hike in the mountain city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Even so that we need to plan .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: A seven died seven times in the fatality occurred .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7dc5074d1b479f89d37181cc384183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obamas president packages live in President Obama .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: One of the mountainous interior of the mountain mountain .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Lets need need a need .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: Another seven fatalities have been killed and a fourth person .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0af10a191f04917bfa2ef45ef14ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Hes his Democratic presidential nominee .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: I was living in the mountain .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Maybe its need to need a coffee .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: The seven member died seven member dice seven member of the dead bore seven killed seven astronauts\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fba70d7d1a44f15bf74879f8b38d066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Id President Obama .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: Thomas near the mountain city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Maybe its need .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: A sevence member of the fatalities blamed the death toll .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd1d70514a4fe8a46ec2fc2ff52589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Its president .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: heads near the city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Maybe you need a need\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: A fourth member of the died when seven Diego was put in the fourth death toll in the fourth fatality battle .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e483fcd91de443a29b5d98206d86b2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obamas president .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: nearby city Mayor .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Lets need to need much .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: Another seven member died when atop Aeuryed Williams died violently\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d8287b98554e7a8af68233764ecc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obamas presidents president Obama has presidents president since Obamas presidents president was president Obama\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: One city in business .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Let coffee need need\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: A seven astronaut died seven astronauts\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaea762cb0b4cbc85a41d72f007903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Im sure thats Obama has President\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: heads in the city of city where one of heads in Mu .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Evening coffee needs .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: Another seven died when seven astronauts died in sevenpish and a fourth fatality battle .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f901b94837f04d33bcf5a331e51978df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations\n",
      "Input: 오바마는 대통령이다\n",
      "Predicted translation: Obamas president since Obamas president .\n",
      "Input: 시민들이 도시 속에 산다.\n",
      "Predicted translation: heads in the capital city city city where its attempted city .\n",
      "Input: 커피는 필요없다.\n",
      "Predicted translation: Even though the needs to grow .\n",
      "Input: 일곱명의 사망자가 발생했다.\n",
      "Predicted translation: A fourth civilian of seven astronauts killed in the fatalities .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.3\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  15\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    print('Translations' )\n",
    "    for sentence in sentences:\n",
    "        translate(sentence, transformer, tokenizer1, tokenizer2)\n",
    "    print()\n",
    "    print('Hyperparameters' )\n",
    "    print('n_layers :', N_LAYERS)\n",
    "    print('d_model :', D_MODEL)\n",
    "    print('n_heads : ', N_HEADS)\n",
    "    print('d_ff :', D_FF)\n",
    "    print('dropout :', DROPOUT)\n",
    "    print()\n",
    "    print('TRAINING')\n",
    "    print('Warmup Steps: 4000')\n",
    "    print('Batch size : 64')\n",
    "    print('Epoch At : ', epoch+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obamas president since Obamas president .\n"
     ]
    }
   ],
   "source": [
    "translate('오바마는 대통령이다.',transformer, tokenizer1, tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정말 다산다난했던 Transformer 프로젝트였다...      \n",
    "이 프로젝트 하면서 데이터 파이프라인에 대해서 고민을 엄청 많이 하게 되었고 train_step에 대해서도 고민을 많이 하게 되었다.    \n",
    "제일 힘들었지만 제일 많은 것을 준 프로젝트라 더 신경이 많이 쓰인다.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
